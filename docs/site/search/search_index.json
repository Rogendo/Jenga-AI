{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Democratizing NLP","text":"<p>Making Advanced NLP Accessible to Every African Developer</p> <p>Get Started</p> <p>View Gallery</p>"},{"location":"#why-democraticise-nlp","title":"Why Democraticise NLP?","text":"<ul> <li>  50% Faster Development  Reduce NLP setup from weeks to hours with our unified framework </li> <li> African Context Aware Built-in support for Swahili, Luganda, and 10+ African languages </li> <li> Multi-Task Fusion  Combine models mathematically for efficient multi-task learning </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from democraticise_nlp import NLPFramework\n\n# Train Swahili sentiment analysis \n\nframework = NLPFramework(\"masakhane/swahili-bert\")\n\nframework.load_data(\"my_swahili_data.csv\")\nmodel = framework.train_task(\"sentiment\")\n\nresult = model.predict(\"Habari za leo?\")\nprint(result)\n</code></pre>"},{"location":"about/","title":"About Jenga-NLP","text":"<p>Jenga-NLP is a powerful and flexible open-source framework designed to democratize Natural Language Processing (NLP) for African languages. Our mission is to provide researchers, developers, and language enthusiasts with the tools they need to build state-of-the-art NLP models for languages that have been historically underserved in the digital world.</p>"},{"location":"about/#our-vision","title":"Our Vision","text":"<p>We believe that language should not be a barrier to accessing and benefiting from the latest advancements in technology. Jenga-NLP aims to bridge the gap by providing a unified framework for fine-tuning and evaluating multi-task NLP models on a wide range of tasks, including:</p> <ul> <li>Text Classification</li> <li>Named Entity Recognition (NER)</li> <li>Question Answering</li> <li>And more...</li> </ul>"},{"location":"about/#key-features","title":"Key Features","text":"<ul> <li>Multi-Task Learning: Train a single model on multiple tasks simultaneously, leading to better performance and more efficient use of resources.</li> <li>Config-Driven: Easily define and manage your experiments using simple and intuitive YAML configuration files.</li> <li>Extensible: The framework is designed to be easily extended with new tasks, models, and datasets.</li> <li>Pre-trained Models: Leverage the power of pre-trained transformer models like <code>distilbert-base-uncased</code> and adapt them to your specific needs.</li> <li>Experiment Tracking: Integrated with MLflow and TensorBoard for easy tracking and visualization of your experiment results.</li> <li>Attention Fusion: An optional attention fusion layer to learn task-specific representations from a shared encoder.</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#core-classes","title":"Core Classes","text":""},{"location":"api-reference/#nlpframework","title":"NLPFramework","text":"<p>The main class for all NLP operations.</p> <p>Parameters: - <code>base_model</code>: Pre-trained model name - <code>language_context</code>: African language focus</p> <p>Methods: - <code>train_task()</code>: Train a specific NLP task - <code>load_data()</code>: Load training data - <code>predict()</code>: Make predictions</p> <p>Documentation in progress...</p>"},{"location":"gallery/","title":"Gallery","text":""},{"location":"gallery/#examples-coming-soon","title":"Examples Coming Soon!","text":"<p>We're working on exciting African NLP examples:</p> <ul> <li>Swahili Sentiment Analysis</li> <li>Multilingual NER</li> <li>Government Document Processing</li> <li>Cybersecurity Threat Detection</li> </ul> <p>Check back soon!</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section provides step-by-step tutorials to help you get the most out of the Jenga-NLP framework.</p>"},{"location":"tutorials/#tutorial-1-training-your-first-multi-task-model","title":"Tutorial 1: Training Your First Multi-Task Model","text":"<p>This tutorial will guide you through the process of setting up and training a multi-task model for sentiment analysis and named entity recognition.</p> <p>Coming Soon!</p>"},{"location":"tutorials/#tutorial-2-adding-a-new-task-to-the-framework","title":"Tutorial 2: Adding a New Task to the Framework","text":"<p>Learn how to extend Jenga-NLP by adding a new custom task. This tutorial will cover the steps required to define a new task, create a dataset, and integrate it into the training pipeline.</p> <p>Coming Soon!</p>"},{"location":"tutorials/#tutorial-3-using-the-attention-fusion-layer","title":"Tutorial 3: Using the Attention Fusion Layer","text":"<p>Discover how to leverage the attention fusion layer to improve the performance of your multi-task models. This tutorial will explain the concept behind attention fusion and show you how to enable and configure it in your experiments.</p> <p>Coming Soon!</p>"},{"location":"developer_guide/core_components/fusion/","title":"Attention Fusion","text":"<p>The Jenga-NLP framework includes an optional <code>AttentionFusion</code> layer that can be used to learn task-specific representations from the shared encoder. This can lead to improved performance in multi-task learning scenarios, as it allows the model to learn to focus on different aspects of the shared representation for each task.</p>"},{"location":"developer_guide/core_components/fusion/#how-it-works","title":"How it Works","text":"<p>The <code>AttentionFusion</code> layer is a simple yet effective mechanism for creating task-specific representations. It works as follows:</p> <ol> <li> <p>Task Embeddings: The <code>AttentionFusion</code> layer maintains a set of task embeddings, one for each task. These embeddings are learned during training and capture the unique characteristics of each task.</p> </li> <li> <p>Concatenation: For a given task, the corresponding task embedding is concatenated with the shared representation from the encoder at each token position.</p> </li> <li> <p>Attention Mechanism: The concatenated representation is then passed through a small feed-forward neural network (the \"attention layer\") to compute an attention score for each token. These scores are then passed through a softmax function to produce a set of attention weights.</p> </li> <li> <p>Fused Representation: The shared representation is then multiplied by the attention weights to produce the final task-specific (or \"fused\") representation.</p> </li> </ol> <p>This process is illustrated in the following diagram:</p> <pre><code>+-----------------------+\n| Shared Representation |\n| (from encoder)        |\n+-----------------------+\n           |\n           v\n+-----------------------+      +----------------+\n|      Concatenate      | &lt;--- | Task Embedding |\n+-----------------------+      +----------------+\n           |\n           v\n+-----------------------+\n|    Attention Layer    |\n|   (Linear + Tanh)     |\n+-----------------------+\n           |\n           v\n+-----------------------+\n|        Softmax        |\n+-----------------------+\n           |\n           v\n+-----------------------+\n|   Attention Weights   |\n+-----------------------+\n           |\n           v\n+-----------------------+\n|      Multiplication   |\n+-----------------------+\n           |\n           v\n+-----------------------+\n| Fused Representation  |\n+-----------------------+\n</code></pre>"},{"location":"developer_guide/core_components/fusion/#attentionfusion-class","title":"<code>AttentionFusion</code> Class","text":"<p>The <code>AttentionFusion</code> class in <code>multitask_bert/core/fusion.py</code> implements this mechanism. It has the following key methods:</p> <ul> <li> <p><code>__init__(self, config, num_tasks)</code>: The constructor for the fusion layer. It takes the following arguments:</p> <ul> <li><code>config</code>: A Hugging Face <code>PretrainedConfig</code> object.</li> <li><code>num_tasks</code>: The total number of tasks.</li> </ul> </li> <li> <p><code>forward(self, shared_representation, task_id)</code>: The forward pass for the fusion layer. It takes the following arguments:</p> <ul> <li><code>shared_representation</code>: The output from the shared encoder.</li> <li><code>task_id</code>: The ID of the current task.</li> </ul> <p>It returns the fused representation.</p> </li> </ul>"},{"location":"developer_guide/core_components/fusion/#how-to-use-it","title":"How to Use It","text":"<p>To use the <code>AttentionFusion</code> layer, you need to enable it in your <code>experiment.yaml</code> file by adding a <code>fusion</code> section to the <code>model</code> configuration:</p> <pre><code>model:\n  base_model: \"distilbert-base-uncased\"\n  dropout: 0.1\n  fusion:\n    type: \"attention\"\n    hidden_size: 768\n</code></pre> <ul> <li><code>type</code>: The type of fusion to use. Currently, only <code>\"attention\"</code> is supported.</li> <li><code>hidden_size</code>: The hidden size of the fusion layer. This should match the hidden size of the base model.</li> </ul> <p>When the <code>fusion</code> section is present in the configuration, the <code>MultiTaskModel</code> will automatically instantiate the <code>AttentionFusion</code> layer and use it to create task-specific representations before passing them to the task heads.</p>"},{"location":"developer_guide/core_components/model/","title":"Multi-Task Model","text":"<p>The <code>MultiTaskModel</code> class in <code>multitask_bert/core/model.py</code> is the central component of the Jenga-NLP framework. It is responsible for processing input data, generating representations, and making predictions for multiple tasks simultaneously.</p>"},{"location":"developer_guide/core_components/model/#architecture","title":"Architecture","text":"<p>The <code>MultiTaskModel</code> is built on top of the Hugging Face <code>PreTrainedModel</code> class, which allows it to easily load pre-trained transformer models from the Hugging Face Hub.</p> <p>The model has the following key components:</p> <ol> <li> <p>Shared Encoder: A pre-trained transformer model (e.g., <code>distilbert-base-uncased</code>) that acts as a shared encoder for all tasks. This encoder is responsible for generating contextualized representations of the input text.</p> </li> <li> <p>Task-Specific Heads: Each task has one or more prediction heads that are attached to the shared encoder. These heads are responsible for making predictions for their specific task. For example, a classification task might have a simple linear layer as its head, while a named entity recognition (NER) task might have a token-level classification head.</p> </li> <li> <p>Attention Fusion Layer (Optional): The model can optionally include an <code>AttentionFusion</code> layer. This layer sits between the shared encoder and the task-specific heads and learns to create task-specific representations from the shared representation.</p> </li> </ol>"},{"location":"developer_guide/core_components/model/#multitaskmodel-class","title":"<code>MultiTaskModel</code> Class","text":"<p>The <code>MultiTaskModel</code> class has the following key methods:</p> <ul> <li> <p><code>__init__(self, config, model_config, tasks)</code>: The constructor for the model. It takes the following arguments:</p> <ul> <li><code>config</code>: A Hugging Face <code>PretrainedConfig</code> object.</li> <li><code>model_config</code>: A <code>ModelConfig</code> object from the experiment configuration.</li> <li><code>tasks</code>: A list of <code>BaseTask</code> objects.</li> </ul> </li> <li> <p><code>forward(self, input_ids, attention_mask, task_id, labels=None, token_type_ids=None, **kwargs)</code>: The forward pass for the model. It takes a batch of data for a single task and returns the output of that task.</p> <ul> <li><code>input_ids</code>: The input token IDs.</li> <li><code>attention_mask</code>: The attention mask.</li> <li><code>task_id</code>: The ID of the task to run.</li> <li><code>labels</code>: The labels for the task.</li> <li><code>token_type_ids</code>: The token type IDs (for models that use them).</li> </ul> </li> </ul>"},{"location":"developer_guide/core_components/model/#how-it-works","title":"How it Works","text":"<ol> <li> <p>Input Processing: The <code>forward</code> method receives a batch of data for a specific task. The <code>input_ids</code> and <code>attention_mask</code> are passed to the shared encoder.</p> </li> <li> <p>Shared Representation: The shared encoder processes the input and generates a <code>last_hidden_state</code>, which is a sequence of hidden states for each token in the input.</p> </li> <li> <p>Attention Fusion (Optional): If the <code>AttentionFusion</code> layer is enabled, it takes the <code>last_hidden_state</code> and the <code>task_id</code> as input and produces a task-specific representation. This is done by learning an attention mechanism that weighs the shared representation differently for each task.</p> </li> <li> <p>Task-Specific Head: The (potentially fused) representation is then passed to the appropriate task-specific head, which is determined by the <code>task_id</code>.</p> </li> <li> <p>Output: The task head produces the final output for the task, which includes the loss and the logits.</p> </li> </ol>"},{"location":"developer_guide/core_components/model/#why-this-architecture","title":"Why this Architecture?","text":"<p>This architecture was chosen for its balance of performance and efficiency.</p> <ul> <li> <p>Parameter Efficiency: By sharing the encoder across all tasks, the model can learn more general-purpose representations of the language, which can lead to better performance on all tasks. It also significantly reduces the number of parameters that need to be trained, making the model more efficient to train and deploy.</p> </li> <li> <p>Flexibility: The use of task-specific heads allows the framework to handle a wide range of NLP tasks, each with its own specific output format.</p> </li> <li> <p>Extensibility: The modular design makes it easy to add new tasks or models to the framework without having to modify the core architecture.</p> </li> </ul>"},{"location":"developer_guide/core_components/tasks/","title":"Tasks","text":"<p>In the Jenga-NLP framework, a \"task\" represents a specific NLP problem that you want to solve, such as text classification, named entity recognition (NER), or question answering. Each task is defined by a <code>BaseTask</code> subclass, which encapsulates the task-specific logic, including the prediction heads, loss functions, and forward pass.</p>"},{"location":"developer_guide/core_components/tasks/#the-basetask-class","title":"The <code>BaseTask</code> Class","text":"<p>The <code>BaseTask</code> class in <code>multitask_bert/tasks/base.py</code> is an abstract base class that all other task classes must inherit from. It defines the following key methods:</p> <ul> <li> <p><code>__init__(self, config)</code>: The constructor for the task. It takes a <code>TaskConfig</code> object from the experiment configuration.</p> </li> <li> <p><code>get_forward_output(self, feature, pooled_output, sequence_output)</code>: This method defines the forward pass for the task. It takes the following arguments:</p> <ul> <li><code>feature</code>: A dictionary containing the labels and attention mask for the task.</li> <li><code>pooled_output</code>: The pooled output from the shared encoder (usually the hidden state of the <code>[CLS]</code> token).</li> <li><code>sequence_output</code>: The sequence output from the shared encoder (the hidden states for all tokens).</li> </ul> <p>It should return a <code>TaskOutput</code> object, which is a dataclass that contains the loss and the logits for the task.</p> </li> </ul>"},{"location":"developer_guide/core_components/tasks/#supported-task-types","title":"Supported Task Types","text":"<p>The Jenga-NLP framework currently supports the following task types:</p>"},{"location":"developer_guide/core_components/tasks/#singlelabelclassificationtask","title":"<code>SingleLabelClassificationTask</code>","text":"<ul> <li>Class: <code>multitask_bert.tasks.classification.SingleLabelClassificationTask</code></li> <li>Type String: <code>\"single_label_classification\"</code></li> <li>Description: This task is used for single-label text classification problems, where each input is assigned to one of several mutually exclusive classes.</li> <li>Heads: It uses a single <code>nn.Linear</code> layer as its prediction head.</li> <li>Loss Function: <code>nn.CrossEntropyLoss</code></li> </ul>"},{"location":"developer_guide/core_components/tasks/#multilabelclassificationtask","title":"<code>MultiLabelClassificationTask</code>","text":"<ul> <li>Class: <code>multitask_bert.tasks.classification.MultiLabelClassificationTask</code></li> <li>Type String: <code>\"multi_label_classification\"</code></li> <li>Description: This task is used for multi-label text classification problems, where each input can be assigned to multiple classes simultaneously.</li> <li>Heads: It can have multiple prediction heads, one for each label. Each head is a <code>nn.Linear</code> layer.</li> <li>Loss Function: <code>nn.BCEWithLogitsLoss</code></li> </ul>"},{"location":"developer_guide/core_components/tasks/#nertask","title":"<code>NERTask</code>","text":"<ul> <li>Class: <code>multitask_bert.tasks.ner.NERTask</code></li> <li>Type String: <code>\"ner\"</code></li> <li>Description: This task is used for named entity recognition, where the goal is to identify and classify named entities in a text.</li> <li>Heads: It uses a token-level classification head, which is a <code>nn.Linear</code> layer that is applied to each token's hidden state.</li> <li>Loss Function: <code>nn.CrossEntropyLoss</code></li> </ul>"},{"location":"developer_guide/core_components/tasks/#how-to-add-a-new-task","title":"How to Add a New Task","text":"<p>To add a new task to the framework, you need to do the following:</p> <ol> <li> <p>Create a new task class: Create a new Python file in the <code>multitask_bert/tasks/</code> directory and define a new class that inherits from <code>BaseTask</code>.</p> </li> <li> <p>Implement the <code>__init__</code> method: In the constructor of your new task class, you need to define the task-specific prediction heads.</p> </li> <li> <p>Implement the <code>get_forward_output</code> method: This method should define the forward pass for your task. It should take the <code>pooled_output</code> and <code>sequence_output</code> from the shared encoder and pass them through the prediction heads to generate the logits. It should also calculate the loss and return a <code>TaskOutput</code> object.</p> </li> <li> <p>Update <code>get_task_class</code>: In <code>examples/run_experiment.py</code>, update the <code>get_task_class</code> function to map your new task's type string to your new task class.</p> </li> <li> <p>Update the configuration: In your <code>experiment.yaml</code> file, add a new task configuration with the <code>type</code> set to your new task's type string.</p> </li> </ol>"},{"location":"developer_guide/core_concepts/architecture/","title":"Framework Architecture","text":"<p>The Jenga-NLP framework is designed with modularity and extensibility in mind. It follows a config-driven approach, allowing you to define and manage complex multi-task learning experiments with ease.</p>"},{"location":"developer_guide/core_concepts/architecture/#core-principles","title":"Core Principles","text":"<ul> <li>Modularity: The framework is divided into distinct components, each responsible for a specific part of the NLP pipeline. This makes it easy to understand, maintain, and extend the codebase.</li> <li>Extensibility: Adding new tasks, models, or datasets is a straightforward process that involves implementing a few base classes and updating the configuration.</li> <li>Config-Driven: All aspects of an experiment, from the model architecture to the training process, are defined in a single YAML configuration file. This ensures reproducibility and simplifies experiment management.</li> </ul>"},{"location":"developer_guide/core_concepts/architecture/#high-level-overview","title":"High-Level Overview","text":"<p>The framework can be broken down into the following key components:</p> <ol> <li> <p>Configuration (<code>multitask_bert/core/config.py</code>): This component is responsible for parsing the <code>experiment.yaml</code> file and creating the necessary configuration objects. It uses Python's <code>dataclasses</code> to define a clear and type-safe configuration structure.</p> </li> <li> <p>Data Processing (<code>multitask_bert/data/data_processing.py</code>): The <code>DataProcessor</code> class handles the loading and preprocessing of data for all tasks. It takes the raw data paths from the configuration and converts them into tokenized datasets ready for training.</p> </li> <li> <p>Tasks (<code>multitask_bert/tasks/</code>): Each task (e.g., classification, NER) is represented by a <code>BaseTask</code> subclass. These classes define the task-specific heads, loss functions, and forward pass logic.</p> </li> <li> <p>Model (<code>multitask_bert/core/model.py</code>): The <code>MultiTaskModel</code> is the heart of the framework. It consists of a shared encoder (e.g., <code>distilbert-base-uncased</code>) and a set of task-specific heads. It can also include an optional <code>AttentionFusion</code> layer to learn task-specific representations.</p> </li> <li> <p>Trainer (<code>multitask_bert/training/trainer.py</code>): The <code>Trainer</code> class orchestrates the entire training and evaluation process. It uses a round-robin approach to sample batches from different tasks and handles optimization, scheduling, and metric calculation.</p> </li> <li> <p>Logging (<code>multitask_bert/training/trainer.py</code>): The framework is integrated with MLflow and TensorBoard for experiment tracking. The <code>Trainer</code> class initializes the logger and logs metrics during training and evaluation.</p> </li> <li> <p>Deployment (<code>multitask_bert/deployment/</code>): This component provides tools for exporting trained models for inference and deploying them in production environments.</p> </li> </ol>"},{"location":"developer_guide/core_concepts/architecture/#data-flow","title":"Data Flow","text":"<p>The following diagram illustrates the data flow within the framework:</p> <pre><code>[experiment.yaml] -&gt; [DataProcessor] -&gt; [Tokenized Datasets]\n                                              |\n                                              v\n+-------------------------------------------------------------------------+\n|                                  Trainer                                  |\n|                                                                         |\n|  +-----------------+      +-----------------+      +-----------------+  |\n|  | Task 1 Dataloader|      | Task 2 Dataloader|      | Task 3 Dataloader|  |\n|  +-----------------+      +-----------------+      +-----------------+  |\n|          |                      |                      |                 |\n|          +----------------------+----------------------+                 |\n|                                 |                                        |\n|                                 v                                        |\n|  +---------------------------------------------------------------------+  |\n|  |                             MultiTaskModel                            |  |\n|  |                                                                     |  |\n|  |  +-----------------------------------------------------------------+  |  |\n|  |  |                         Shared Encoder                          |  |  |\n|  |  +-----------------------------------------------------------------+  |  |\n|  |                                 |                                     |  |\n|  |  +-----------------------------------------------------------------+  |  |\n|  |  |                       (Optional) Fusion Layer                     |  |  |\n|  |  +-----------------------------------------------------------------+  |  |\n|  |                                 |                                     |  |\n|  |  +----------+      +----------+      +----------+                     |  |\n|  |  | Task 1 Head|      | Task 2 Head|      | Task 3 Head|                     |  |\n|  |  +----------+      +----------+      +----------+                     |  |\n|  +---------------------------------------------------------------------+  |\n|                                 |                                        |\n|                                 v                                        |\n|  +---------------------------------------------------------------------+  |\n|  |                                Metrics                                |  |\n|  +---------------------------------------------------------------------+  |\n|                                 |                                        |\n|                                 v                                        |\n|  +---------------------------------------------------------------------+  |\n|  |                                Logger                                 |  |\n|  |                        (MLflow / TensorBoard)                         |  |\n|  +---------------------------------------------------------------------+  |\n+-------------------------------------------------------------------------+\n</code></pre>"},{"location":"developer_guide/core_concepts/configuration/","title":"Configuration","text":"<p>The Jenga-NLP framework is designed to be highly configurable, allowing you to define and customize your experiments using a single YAML file. This document provides a detailed explanation of the configuration options available.</p>"},{"location":"developer_guide/core_concepts/configuration/#top-level-configuration-experimentconfig","title":"Top-Level Configuration (<code>ExperimentConfig</code>)","text":"<p>The main configuration is defined by the <code>ExperimentConfig</code> dataclass in <code>multitask_bert/core/config.py</code>. It has the following top-level fields:</p> <ul> <li><code>project_name</code>: A string that identifies your project.</li> <li><code>model</code>: A <code>ModelConfig</code> object that defines the model architecture.</li> <li><code>tokenizer</code>: A <code>TokenizerConfig</code> object that defines the tokenizer settings.</li> <li><code>training</code>: A <code>TrainingConfig</code> object that defines the training process.</li> <li><code>tasks</code>: A list of <code>TaskConfig</code> objects, each defining a specific task.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#modelconfig","title":"<code>ModelConfig</code>","text":"<p>The <code>ModelConfig</code> dataclass defines the model architecture.</p> <ul> <li><code>base_model</code>: The name of the pre-trained transformer model to use from the Hugging Face Hub (e.g., <code>\"distilbert-base-uncased\"</code>).</li> <li><code>dropout</code>: The dropout probability for the model.</li> <li><code>fusion</code>: An optional <code>FusionConfig</code> object to enable and configure the attention fusion layer.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#fusionconfig","title":"<code>FusionConfig</code>","text":"<p>The <code>FusionConfig</code> dataclass enables and configures the attention fusion layer.</p> <ul> <li><code>type</code>: The type of fusion to use. Currently, only <code>\"attention\"</code> is supported.</li> <li><code>hidden_size</code>: The hidden size of the fusion layer. This should match the hidden size of the base model.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#tokenizerconfig","title":"<code>TokenizerConfig</code>","text":"<p>The <code>TokenizerConfig</code> dataclass defines the tokenizer settings.</p> <ul> <li><code>max_length</code>: The maximum sequence length for the tokenizer.</li> <li><code>padding</code>: The padding strategy to use (e.g., <code>\"max_length\"</code>).</li> <li><code>truncation</code>: Whether to truncate sequences that are longer than <code>max_length</code>.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#trainingconfig","title":"<code>TrainingConfig</code>","text":"<p>The <code>TrainingConfig</code> dataclass defines the training process.</p> <ul> <li><code>output_dir</code>: The directory where the training results will be saved.</li> <li><code>learning_rate</code>: The learning rate for the optimizer.</li> <li><code>batch_size</code>: The batch size for training and evaluation.</li> <li><code>num_epochs</code>: The number of training epochs.</li> <li><code>weight_decay</code>: The weight decay for the optimizer.</li> <li><code>warmup_steps</code>: The number of warmup steps for the learning rate scheduler.</li> <li><code>eval_strategy</code>: The evaluation strategy to use (e.g., <code>\"epoch\"</code>).</li> <li><code>save_strategy</code>: The save strategy to use (e.g., <code>\"epoch\"</code>).</li> <li><code>load_best_model_at_end</code>: Whether to load the best model at the end of training.</li> <li><code>metric_for_best_model</code>: The metric to use for determining the best model.</li> <li><code>greater_is_better</code>: Whether a higher value of <code>metric_for_best_model</code> is better.</li> <li><code>early_stopping_patience</code>: The number of epochs to wait for improvement before stopping training.</li> <li><code>logging</code>: A <code>LoggingConfig</code> object to configure experiment tracking.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#loggingconfig","title":"<code>LoggingConfig</code>","text":"<p>The <code>LoggingConfig</code> dataclass configures experiment tracking.</p> <ul> <li><code>service</code>: The logging service to use. Can be <code>\"tensorboard\"</code> or <code>\"mlflow\"</code>.</li> <li><code>experiment_name</code>: The name of the experiment.</li> <li><code>tracking_uri</code>: The tracking URI for MLflow (optional).</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#taskconfig","title":"<code>TaskConfig</code>","text":"<p>The <code>TaskConfig</code> dataclass defines a specific task.</p> <ul> <li><code>name</code>: The name of the task.</li> <li><code>type</code>: The type of the task. Supported types are:<ul> <li><code>\"single_label_classification\"</code></li> <li><code>\"multi_label_classification\"</code></li> <li><code>\"ner\"</code></li> </ul> </li> <li><code>data_path</code>: The path to the data file for the task.</li> <li><code>heads</code>: A list of <code>HeadConfig</code> objects, each defining a prediction head for the task.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#headconfig","title":"<code>HeadConfig</code>","text":"<p>The <code>HeadConfig</code> dataclass defines a prediction head.</p> <ul> <li><code>name</code>: The name of the head.</li> <li><code>num_labels</code>: The number of labels for the head.</li> <li><code>weight</code>: The weight of the head's loss in the total loss.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#example-configuration","title":"Example Configuration","text":"<p>Here is an example of a complete <code>experiment.yaml</code> file:</p> <pre><code>project_name: \"JengaAI_Unified_Framework\"\n\nmodel:\n  base_model: \"distilbert-base-uncased\"\n  dropout: 0.1\n\ntokenizer:\n  max_length: 256\n  padding: \"max_length\"\n  truncation: true\n\ntraining:\n  output_dir: \"./unified_results\"\n  learning_rate: 2.0e-5\n  batch_size: 8\n  num_epochs: 5\n  weight_decay: 0.01\n  warmup_steps: 100\n  eval_strategy: \"epoch\"\n  save_strategy: \"epoch\"\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n  early_stopping_patience: 3\n  logging:\n    service: \"mlflow\"\n    experiment_name: \"JengaAI_MVP\"\n\ntasks:\n  - name: \"SentimentClassifier\"\n    type: \"single_label_classification\"\n    data_path: \"examples/sentiment_data.jsonl\"\n    heads:\n      - name: \"sentiment_head\"\n        num_labels: 2\n        weight: 1.0\n\n  - name: \"SwahiliNER\"\n    type: \"ner\"\n    data_path: \"examples/ner_data.jsonl\"\n    heads:\n      - name: \"ner_head\"\n        num_labels: 13\n        weight: 1.0\n</code></pre>"},{"location":"developer_guide/deployment/deployment/","title":"Deployment","text":"<p>The Jenga-NLP framework provides tools for exporting trained models for inference and deploying them in production environments. This document provides an overview of the deployment options available.</p>"},{"location":"developer_guide/deployment/deployment/#exporting-models","title":"Exporting Models","text":"<p>The <code>multitask_bert/deployment/export.py</code> module (which is not yet implemented) will provide a script for exporting trained models to a format that is suitable for inference. This will likely involve saving the model's weights and the tokenizer's vocabulary to a directory.</p> <p>The exported model can then be loaded into an inference script or a production environment for making predictions on new data.</p>"},{"location":"developer_guide/deployment/deployment/#inference","title":"Inference","text":"<p>The <code>multitask_bert/deployment/inference.py</code> module (which is not yet implemented) will provide a class for running inference with a trained model. This class will likely have the following key methods:</p> <ul> <li> <p><code>__init__(self, model_path)</code>: The constructor for the inference class. It will take the path to the exported model as input.</p> </li> <li> <p><code>predict(self, text, task_name)</code>: This method will take a piece of text and the name of the task to run as input and will return the predictions for that task.</p> </li> </ul>"},{"location":"developer_guide/deployment/deployment/#deployment-options","title":"Deployment Options","text":"<p>There are several options for deploying your trained models:</p>"},{"location":"developer_guide/deployment/deployment/#1-rest-api-with-fastapi","title":"1. REST API with FastAPI","text":"<p>You can create a REST API for your model using a web framework like FastAPI. This will allow you to serve your model over the network and integrate it with other applications.</p> <p>The API would have an endpoint that takes a piece of text and a task name as input and returns the predictions from the model.</p>"},{"location":"developer_guide/deployment/deployment/#2-batch-inference","title":"2. Batch Inference","text":"<p>If you need to make predictions on a large amount of data, you can use a batch inference script. This script would load the trained model and the data, and would then iterate through the data, making predictions for each example.</p> <p>The predictions can then be saved to a file or a database for further analysis.</p>"},{"location":"developer_guide/deployment/deployment/#3-integration-with-other-applications","title":"3. Integration with Other Applications","text":"<p>You can also integrate your trained model with other applications, such as a chatbot or a content moderation system. This would involve loading the model into the application and using it to make predictions on the application's data.</p>"},{"location":"developer_guide/deployment/deployment/#future-work","title":"Future Work","text":"<p>The deployment component of the Jenga-NLP framework is still under development. In the future, we plan to add the following features:</p> <ul> <li>ONNX Export: Support for exporting models to the ONNX format for improved performance and cross-platform compatibility.</li> <li>Inference Optimization: Tools for optimizing models for inference, such as quantization and pruning.</li> <li>Pre-built Docker Images: Pre-built Docker images for deploying models in a containerized environment.</li> </ul>"},{"location":"developer_guide/logging/logging/","title":"Logging","text":"<p>The Jenga-NLP framework is integrated with MLflow and TensorBoard for experiment tracking, allowing you to monitor your training process, compare different experiments, and visualize your results.</p>"},{"location":"developer_guide/logging/logging/#how-it-works","title":"How it Works","text":"<p>The <code>Trainer</code> class in <code>multitask_bert/training/trainer.py</code> is responsible for initializing the logger and logging the metrics during training and evaluation.</p> <p>At the beginning of the training process, the <code>Trainer</code> checks the <code>logging</code> section of the <code>TrainingConfig</code> to determine which logging service to use. It then initializes the appropriate logger (<code>SummaryWriter</code> for TensorBoard or <code>mlflow</code> for MLflow).</p> <p>During the training loop, the <code>Trainer</code> logs the training loss at each step. During the evaluation loop, it logs the evaluation metrics for each task and head.</p>"},{"location":"developer_guide/logging/logging/#how-to-configure-logging","title":"How to Configure Logging","text":"<p>To configure logging, you need to add a <code>logging</code> section to the <code>training</code> configuration in your <code>experiment.yaml</code> file.</p>"},{"location":"developer_guide/logging/logging/#tensorboard","title":"TensorBoard","text":"<p>To use TensorBoard, set the <code>service</code> to <code>\"tensorboard\"</code>:</p> <pre><code>training:\n  # ...\n  logging:\n    service: \"tensorboard\"\n    experiment_name: \"JengaAI_MVP\"\n</code></pre> <ul> <li><code>service</code>: The logging service to use.</li> <li><code>experiment_name</code>: The name of the experiment. The TensorBoard logs will be saved to <code>output_dir/logs/experiment_name</code>.</li> </ul> <p>To view the TensorBoard logs, run the following command in your terminal:</p> <pre><code>tensorboard --logdir=./unified_results/logs\n</code></pre>"},{"location":"developer_guide/logging/logging/#mlflow","title":"MLflow","text":"<p>To use MLflow, set the <code>service</code> to <code>\"mlflow\"</code>:</p> <pre><code>training:\n  # ...\n  logging:\n    service: \"mlflow\"\n    experiment_name: \"JengaAI_MVP\"\n    tracking_uri: \"http://localhost:5000\" # Optional\n</code></pre> <ul> <li><code>service</code>: The logging service to use.</li> <li><code>experiment_name</code>: The name of the experiment.</li> <li><code>tracking_uri</code>: The tracking URI for the MLflow server (optional). If not provided, MLflow will log to a local <code>mlruns</code> directory.</li> </ul> <p>To view the MLflow UI, run the following command in your terminal:</p> <pre><code>mlflow ui\n</code></pre>"},{"location":"developer_guide/logging/logging/#_init_logger-and-_log_metrics","title":"<code>_init_logger</code> and <code>_log_metrics</code>","text":"<p>The <code>Trainer</code> class has two key methods for logging:</p> <ul> <li> <p><code>_init_logger(self)</code>: This method is called in the constructor of the <code>Trainer</code> and is responsible for initializing the logger based on the configuration.</p> </li> <li> <p><code>_log_metrics(self, metrics, step, prefix)</code>: This method is called during the training and evaluation loops to log the metrics. It takes the following arguments:</p> <ul> <li><code>metrics</code>: A dictionary of metrics to log.</li> <li><code>step</code>: The current step or epoch.</li> <li><code>prefix</code>: A prefix to add to the metric names (e.g., <code>\"Train\"</code> or <code>\"Eval\"</code>).</li> </ul> </li> </ul> <p>By using these methods, the <code>Trainer</code> provides a consistent and flexible way to log your experiment results, regardless of which logging service you choose.</p>"},{"location":"developer_guide/training_evaluation/metrics/","title":"Metrics","text":"<p>The Jenga-NLP framework provides a set of metric functions for evaluating the performance of your multi-task models. These functions are located in <code>multitask_bert/utils/metrics.py</code> and are used by the <code>Trainer</code> class during the evaluation process.</p>"},{"location":"developer_guide/training_evaluation/metrics/#supported-metrics","title":"Supported Metrics","text":"<p>The framework currently supports the following metrics:</p>"},{"location":"developer_guide/training_evaluation/metrics/#classification-metrics","title":"Classification Metrics","text":"<ul> <li>Function: <code>compute_classification_metrics(preds, labels)</code></li> <li>Description: This function computes the accuracy, precision, recall, and F1 score for single-label classification tasks.</li> <li>Arguments:<ul> <li><code>preds</code>: The predictions from the model.</li> <li><code>labels</code>: The true labels.</li> </ul> </li> <li>Returns: A dictionary of metrics.</li> </ul>"},{"location":"developer_guide/training_evaluation/metrics/#multi-label-metrics","title":"Multi-Label Metrics","text":"<ul> <li>Function: <code>compute_multi_label_metrics(preds, labels)</code></li> <li>Description: This function computes the accuracy, precision, recall, and F1 score for multi-label classification tasks.</li> <li>Arguments:<ul> <li><code>preds</code>: The predictions from the model.</li> <li><code>labels</code>: The true labels.</li> </ul> </li> <li>Returns: A dictionary of metrics.</li> </ul>"},{"location":"developer_guide/training_evaluation/metrics/#ner-metrics","title":"NER Metrics","text":"<ul> <li>Function: <code>compute_ner_metrics(preds, labels, label_map)</code></li> <li>Description: This function computes the accuracy, precision, recall, and F1 score for named entity recognition (NER) tasks. It uses the <code>seqeval</code> library to compute the metrics at the entity level.</li> <li>Arguments:<ul> <li><code>preds</code>: The predictions from the model.</li> <li><code>labels</code>: The true labels.</li> <li><code>label_map</code>: A dictionary that maps label IDs to label names.</li> </ul> </li> <li>Returns: A dictionary of metrics.</li> </ul>"},{"location":"developer_guide/training_evaluation/metrics/#how-metrics-are-calculated","title":"How Metrics are Calculated","text":"<p>During the evaluation process, the <code>Trainer</code> class iterates through each task's evaluation dataloader and collects the predictions and labels for each head. It then calls the appropriate metric function based on the task type to compute the metrics for that head.</p> <p>The metrics for all heads are then aggregated into a single dictionary and logged to MLflow or TensorBoard.</p>"},{"location":"developer_guide/training_evaluation/metrics/#custom-metrics","title":"Custom Metrics","text":"<p>To use a custom metric function, you will need to modify the <code>evaluate</code> method of the <code>Trainer</code> class in <code>multitask_bert/training/trainer.py</code>.</p> <ol> <li> <p>Import your metric function: Import your custom metric function at the top of the file.</p> </li> <li> <p>Call your metric function: In the <code>evaluate</code> method, after the predictions and labels have been collected, call your custom metric function with the predictions and labels.</p> </li> <li> <p>Add the results to the <code>task_metrics</code> dictionary: Add the results of your custom metric function to the <code>task_metrics</code> dictionary.</p> </li> </ol> <p>By following these steps, you can easily extend the framework to support any metric function you need.</p>"},{"location":"developer_guide/training_evaluation/trainer/","title":"Trainer","text":"<p>The <code>Trainer</code> class in <code>multitask_bert/training/trainer.py</code> is responsible for orchestrating the entire training and evaluation process. It brings together the model, data, and configuration to train the multi-task model and evaluate its performance.</p>"},{"location":"developer_guide/training_evaluation/trainer/#key-responsibilities","title":"Key Responsibilities","text":"<p>The <code>Trainer</code> class has the following key responsibilities:</p> <ul> <li>Dataloader Creation: It creates task-specific dataloaders for training and evaluation.</li> <li>Optimizer and Scheduler: It creates the optimizer (AdamW) and the learning rate scheduler.</li> <li>Training Loop: It implements the main training loop, which iterates over the data and updates the model's weights.</li> <li>Evaluation Loop: It implements the evaluation loop, which calculates the metrics for each task.</li> <li>Logging: It logs the training and evaluation metrics to MLflow or TensorBoard.</li> <li>Early Stopping: It implements early stopping to prevent overfitting.</li> </ul>"},{"location":"developer_guide/training_evaluation/trainer/#training-process","title":"Training Process","text":"<p>The training process is designed to handle multiple tasks simultaneously using a round-robin approach.</p> <ol> <li> <p>Dataloader Iterators: At the beginning of each epoch, the <code>Trainer</code> creates iterators for each task's dataloader.</p> </li> <li> <p>Round-Robin Sampling: The <code>Trainer</code> then iterates through the tasks in a round-robin fashion, sampling one batch from each task's dataloader at a time.</p> </li> <li> <p>Forward Pass: For each batch, the <code>Trainer</code> performs a forward pass through the <code>MultiTaskModel</code>, providing the <code>input_ids</code>, <code>attention_mask</code>, <code>labels</code>, and <code>task_id</code>.</p> </li> <li> <p>Backward Pass: The loss from the forward pass is then used to perform a backward pass and update the model's weights.</p> </li> <li> <p>Logging: The training loss is logged to MLflow or TensorBoard at each step.</p> </li> </ol> <p>This process continues until all dataloaders are exhausted.</p>"},{"location":"developer_guide/training_evaluation/trainer/#evaluation-process","title":"Evaluation Process","text":"<p>The evaluation process is performed at the end of each epoch (if <code>eval_strategy</code> is set to <code>\"epoch\"</code>).</p> <ol> <li> <p>Evaluation Mode: The model is first set to evaluation mode (<code>model.eval()</code>).</p> </li> <li> <p>Iterate Through Tasks: The <code>Trainer</code> then iterates through each task's evaluation dataloader.</p> </li> <li> <p>Collect Predictions and Labels: For each batch, the <code>Trainer</code> performs a forward pass and collects the predictions and labels for each head of the task.</p> </li> <li> <p>Compute Metrics: Once all batches have been processed, the <code>Trainer</code> computes the metrics for each head using the appropriate metric function (e.g., <code>compute_classification_metrics</code>, <code>compute_ner_metrics</code>).</p> </li> <li> <p>Log Metrics: The evaluation metrics are then logged to MLflow or TensorBoard.</p> </li> </ol>"},{"location":"developer_guide/training_evaluation/trainer/#trainer-class","title":"<code>Trainer</code> Class","text":"<p>The <code>Trainer</code> class has the following key methods:</p> <ul> <li> <p><code>__init__(self, config, model, tokenizer, train_datasets, eval_datasets)</code>: The constructor for the <code>Trainer</code>. It takes the following arguments:</p> <ul> <li><code>config</code>: The <code>ExperimentConfig</code> object.</li> <li><code>model</code>: The <code>MultiTaskModel</code> object.</li> <li><code>tokenizer</code>: The tokenizer object.</li> <li><code>train_datasets</code>: A dictionary of training datasets, where the keys are task names.</li> <li><code>eval_datasets</code>: A dictionary of evaluation datasets, where the keys are task names.</li> </ul> </li> <li> <p><code>train(self)</code>: This method starts the training process.</p> </li> <li> <p><code>evaluate(self)</code>: This method performs the evaluation process.</p> </li> <li> <p><code>close(self)</code>: This method closes the logger.</p> </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#quick-install","title":"Quick Install","text":"<pre><code>pip install Jenga-nlp\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#your-first-swahili-sentiment-model","title":"Your First Swahili Sentiment Model","text":"<pre><code>from democraticise_nlp import NLPFramework\n\n# Initialize with African context\nframework = NLPFramework(\n    base_model=\"masakhane/swahili-bert\",\n    language_context=\"swahili\"\n)\n\n# Load your data\nframework.load_data(\"swahili_reviews.csv\")\n\n# Train sentiment analysis\nmodel = framework.train_task(\"sentiment\")\n\n# Make predictions\npredictions = model.predict(\"Bidii yako imenivutia sana!\")\nprint(predictions)\n</code></pre>"}]}