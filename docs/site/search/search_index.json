{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Jenga-AI: A Unified Multi-Task Framework for African NLP","text":"<p>Jenga-AI is an open-source framework designed to democratize Natural Language Processing for African languages and contexts.</p> <p>Our mission is to empower developers, researchers, and organizations across the continent to build, train, and deploy state-of-the-art NLP models that are efficient, powerful, and context-aware.</p>  -   :material-rocket-launch:{ .lg .middle } **Train a Single Model for Multiple Tasks**      ---      Save significant computational resources and time by training one model to simultaneously perform classification, NER, question answering, and more.  -   :material-cogs:{ .lg .middle } **Simple, Config-Driven Experiments**      ---      Define and run complex training workflows with a single, easy-to-understand YAML file. No boilerplate code required.  -   :material-earth:{ .lg .middle } **Built for African Contexts**      ---      Move beyond generic Western models. Jenga-AI is designed to be fine-tuned on local languages and datasets, capturing the unique nuances of the African linguistic landscape."},{"location":"#get-started-in-minutes","title":"Get Started in Minutes","text":"<p>Ready to train your first model? Dive into our Quickstart Guide and see how easy it is to get started with Jenga-AI.</p> <pre><code># Conceptual Example: Train Sentiment &amp; NER simultaneously\nfrom multitask_bert import Trainer, ExperimentConfig\n\n# 1. Load your experiment configuration from a YAML file\nconfig = ExperimentConfig.from_yaml(\"experiment.yaml\")\n\n# 2. Initialize the Trainer\ntrainer = Trainer.from_config(config)\n\n# 3. Train the model!\ntrainer.train()\n\n# 4. (Coming Soon) Deploy the model with a single command\n# trainer.deploy_as_api()\n</code></pre>"},{"location":"#our-vision","title":"Our Vision","text":"<p>We believe that the future of AI in Africa should be built by Africans, for Africans. Jenga-AI is more than a software library; it's a community-driven effort to build the tools and share the knowledge needed to solve our own challenges.</p> <p>Whether you're tackling misinformation in Swahili, analyzing public policy documents, or building a customer service bot that understands Sheng, Jenga-AI provides the building blocks you need.</p>"},{"location":"about/","title":"About Jenga-NLP","text":"<p>Jenga-NLP is a powerful and flexible open-source framework designed to democratize Natural Language Processing (NLP) for African languages. Our mission is to provide researchers, developers, and language enthusiasts with the tools they need to build state-of-the-art NLP models for languages that have been historically underserved in the digital world.</p>"},{"location":"about/#our-vision","title":"Our Vision","text":"<p>We believe that language should not be a barrier to accessing and benefiting from the latest advancements in technology. Jenga-NLP aims to bridge the gap by providing a unified framework for fine-tuning and evaluating multi-task NLP models on a wide range of tasks, including:</p> <ul> <li>Text Classification</li> <li>Named Entity Recognition (NER)</li> <li>Question Answering</li> <li>And more...</li> </ul>"},{"location":"about/#key-features","title":"Key Features","text":"<ul> <li>Multi-Task Learning: Train a single model on multiple tasks simultaneously, leading to better performance and more efficient use of resources.</li> <li>Config-Driven: Easily define and manage your experiments using simple and intuitive YAML configuration files.</li> <li>Extensible: The framework is designed to be easily extended with new tasks, models, and datasets.</li> <li>Pre-trained Models: Leverage the power of pre-trained transformer models like <code>distilbert-base-uncased</code> and adapt them to your specific needs.</li> <li>Experiment Tracking: Integrated with MLflow and TensorBoard for easy tracking and visualization of your experiment results.</li> <li>Attention Fusion: An optional attention fusion layer to learn task-specific representations from a shared encoder.</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#core-classes","title":"Core Classes","text":""},{"location":"api-reference/#nlpframework","title":"NLPFramework","text":"<p>The main class for all NLP operations.</p> <p>Parameters: - <code>base_model</code>: Pre-trained model name - <code>language_context</code>: African language focus</p> <p>Methods: - <code>train_task()</code>: Train a specific NLP task - <code>load_data()</code>: Load training data - <code>predict()</code>: Make predictions</p> <p>Documentation in progress...</p>"},{"location":"gallery/","title":"Gallery","text":""},{"location":"gallery/#examples-coming-soon","title":"Examples Coming Soon!","text":"<p>We're working on exciting African NLP examples:</p> <ul> <li>Swahili Sentiment Analysis</li> <li>Multilingual NER</li> <li>Government Document Processing</li> <li>Cybersecurity Threat Detection</li> </ul> <p>Check back soon!</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section provides step-by-step tutorials to help you get the most out of the Jenga-NLP framework.</p>"},{"location":"tutorials/#tutorial-1-training-your-first-multi-task-model","title":"Tutorial 1: Training Your First Multi-Task Model","text":"<p>This tutorial will guide you through the process of setting up and training a multi-task model for sentiment analysis and named entity recognition.</p> <p>Coming Soon!</p>"},{"location":"tutorials/#tutorial-2-adding-a-new-task-to-the-framework","title":"Tutorial 2: Adding a New Task to the Framework","text":"<p>Learn how to extend Jenga-NLP by adding a new custom task. This tutorial will cover the steps required to define a new task, create a dataset, and integrate it into the training pipeline.</p> <p>Coming Soon!</p>"},{"location":"tutorials/#tutorial-3-using-the-attention-fusion-layer","title":"Tutorial 3: Using the Attention Fusion Layer","text":"<p>Discover how to leverage the attention fusion layer to improve the performance of your multi-task models. This tutorial will explain the concept behind attention fusion and show you how to enable and configure it in your experiments.</p> <p>Coming Soon!</p>"},{"location":"api/core/","title":"API Reference: Core Components","text":"<p>This section provides a detailed reference for the core architectural components of the Jenga-AI framework.</p>"},{"location":"api/core/#experimentconfig","title":"<code>ExperimentConfig</code>","text":"<p>The <code>ExperimentConfig</code> is a dataclass that holds the entire configuration for a training run. It's loaded from your <code>experiment.yaml</code> file and serves as the single source of truth for the <code>Trainer</code>.</p> <p>See the How-to: Configure an Experiment guide for a full breakdown of the YAML structure.</p>"},{"location":"api/core/#key-dataclasses","title":"Key Dataclasses","text":"<ul> <li><code>ExperimentConfig</code>: The top-level container.<ul> <li><code>project_name: str</code></li> <li><code>model: ModelConfig</code></li> <li><code>tokenizer: TokenizerConfig</code></li> <li><code>training: TrainingConfig</code></li> <li><code>tasks: List[TaskConfig]</code></li> </ul> </li> <li><code>ModelConfig</code>: Defines the model architecture.<ul> <li><code>base_model: str</code>: The Hugging Face model to use as the encoder.</li> <li><code>dropout: float</code>: Dropout rate for the prediction heads.</li> <li><code>fusion: Optional[FusionConfig]</code>: Optional configuration for the attention fusion layer.</li> </ul> </li> <li><code>TaskConfig</code>: Defines a single task.<ul> <li><code>name: str</code>: A unique name for the task.</li> <li><code>type: str</code>: The task type (e.g., <code>\"single_label_classification\"</code>, <code>\"ner\"</code>).</li> <li><code>data_path: str</code>: Path to the data file.</li> <li><code>heads: List[HeadConfig]</code>: A list of prediction heads for this task.</li> </ul> </li> <li><code>HeadConfig</code>: Defines a single prediction head.<ul> <li><code>name: str</code>: A unique name for the head.</li> <li><code>num_labels: int</code>: The number of output labels for this head.</li> <li><code>weight: float</code>: The weight of this head's loss in the total loss calculation.</li> </ul> </li> </ul>"},{"location":"api/core/#multitaskmodel","title":"<code>MultiTaskModel</code>","text":"<p>Class Path: <code>multitask_bert.core.model.MultiTaskModel</code></p> <p>The <code>MultiTaskModel</code> is the heart of the framework. It is a PyTorch module that wraps a shared transformer encoder (e.g., BERT, RoBERTa) and manages a set of task-specific prediction heads.</p>"},{"location":"api/core/#architecture","title":"Architecture","text":"<ol> <li>Shared Encoder: A single pre-trained transformer model processes the input text and generates contextualized embeddings. This encoder is shared across all tasks, allowing the model to learn rich, general-purpose language representations.</li> <li>Task-Specific Heads: Each task defined in your configuration has its own set of prediction heads. These are lightweight neural network layers (usually <code>nn.Linear</code>) that take the encoder's output and produce the final logits for that specific task.</li> <li>Single-Task Forward Pass: The model's <code>forward</code> method is designed to process a batch of data for one specific task at a time. The <code>Trainer</code> manages the process of alternating between different tasks during training.</li> </ol>"},{"location":"api/core/#key-methods","title":"Key Methods","text":"<ul> <li> <p><code>__init__(self, config, model_config, task_configs)</code></p> <ul> <li>Purpose: Initializes the model. It loads the pre-trained encoder and dynamically creates the required task heads based on the provided configurations.</li> <li>Arguments:<ul> <li><code>config</code>: The Hugging Face <code>AutoConfig</code> for the base model.</li> <li><code>model_config (ModelConfig)</code>: The model configuration from your <code>experiment.yaml</code>.</li> <li><code>task_configs (List[TaskConfig])</code>: A list of task configurations.</li> </ul> </li> </ul> </li> <li> <p><code>forward(self, input_ids, attention_mask, task_id, labels=None, ...)</code></p> <ul> <li>Purpose: Performs a forward pass for a single batch of data.</li> <li>Arguments:<ul> <li><code>input_ids (torch.Tensor)</code>: The input token IDs for the batch.</li> <li><code>attention_mask (torch.Tensor)</code>: The attention mask for the batch.</li> <li><code>task_id (int)</code>: The integer index of the task to perform the forward pass for.</li> <li><code>labels (Any)</code>: The corresponding labels for the batch.</li> </ul> </li> <li>Returns: A dictionary containing the <code>loss</code> and <code>logits</code> for the specified task.</li> </ul> </li> </ul>"},{"location":"api/core/#attentionfusion","title":"<code>AttentionFusion</code>","text":"<p>Class Path: <code>multitask_bert.core.fusion.AttentionFusion</code></p> <p>The <code>AttentionFusion</code> layer is an optional but powerful component that can be enabled to improve multi-task learning. It creates task-specific representations by learning to re-weigh the shared encoder's output for each task.</p>"},{"location":"api/core/#how-it-works","title":"How It Works","text":"<ol> <li>Task Embeddings: The layer maintains a learnable embedding vector for each task.</li> <li>Attention Mechanism: For a given task, its embedding is combined with the shared representation from the encoder. This combination is passed through a small neural network that computes an \"attention score\" for each token in the sequence.</li> <li>Fused Representation: The attention scores are used to weigh the importance of each token's representation. The result is a new, \"fused\" representation that is tailored to the specific requirements of the current task before being passed to the prediction head.</li> </ol>"},{"location":"api/core/#how-to-use","title":"How to Use","text":"<p>To enable the <code>AttentionFusion</code> layer, add a <code>fusion</code> block to the <code>model</code> section of your <code>experiment.yaml</code>:</p> <pre><code>model:\n  base_model: \"distilbert-base-uncased\"\n  fusion:\n    type: \"attention\"\n    hidden_size: 768 # IMPORTANT: Must match the hidden size of your base_model\n</code></pre>"},{"location":"api/data/","title":"API Reference: Data Processing","text":"<p>The data processing components are responsible for loading raw data from your files, tokenizing it, and preparing it in the correct format for training with the <code>MultiTaskModel</code>.</p>"},{"location":"api/data/#dataprocessor","title":"<code>DataProcessor</code>","text":"<p>Class Path: <code>multitask_bert.data.data_processing.DataProcessor</code></p> <p>The <code>DataProcessor</code> is the main class that handles all data preparation, driven by the <code>ExperimentConfig</code>.</p>"},{"location":"api/data/#role-of-the-dataprocessor","title":"Role of the DataProcessor","text":"<ol> <li>Reads Configuration: It inspects the <code>tasks</code> list in your <code>ExperimentConfig</code>.</li> <li>Loads Data: For each task, it loads the data file specified in <code>data_path</code>. It currently supports <code>.jsonl</code> files.</li> <li>Processes and Tokenizes: It applies task-specific processing logic to tokenize the text and format the labels correctly.</li> <li>Automatic Label Discovery: For NER and classification tasks, it can automatically discover the set of unique labels from your data and update the <code>ExperimentConfig</code> with the correct number of labels and a <code>label_map</code>.</li> <li>Splits Data: It splits the processed data into training and evaluation sets.</li> <li>Formats for PyTorch: It formats the datasets for efficient loading by a PyTorch <code>DataLoader</code>.</li> </ol>"},{"location":"api/data/#key-methods","title":"Key Methods","text":"<ul> <li> <p><code>__init__(self, config, tokenizer)</code></p> <ul> <li>Purpose: Initializes the <code>DataProcessor</code>.</li> <li>Arguments:<ul> <li><code>config (ExperimentConfig)</code>: The complete experiment configuration object.</li> <li><code>tokenizer</code>: The Hugging Face tokenizer instance to use for processing.</li> </ul> </li> </ul> </li> <li> <p><code>process(self) -&gt; Tuple[Dict, Dict, ExperimentConfig]</code></p> <ul> <li>Purpose: The main method that orchestrates the entire data processing workflow.</li> <li>Process:<ol> <li>Iterates through each <code>TaskConfig</code> in <code>config.tasks</code>.</li> <li>Loads the corresponding data file into a Hugging Face <code>Dataset</code>.</li> <li>Calls the appropriate internal processing method (e.g., <code>_process_ner</code>) based on the <code>task.type</code>.</li> <li>Splits the resulting tokenized dataset into a training and evaluation set (currently an 80/20 split).</li> <li>Collects all processed datasets into dictionaries.</li> </ol> </li> <li>Returns: A tuple containing:<ul> <li><code>train_datasets (Dict[str, Dataset])</code>: A dictionary mapping task names to their training <code>Dataset</code> objects.</li> <li><code>eval_datasets (Dict[str, Dataset])</code>: A dictionary mapping task names to their evaluation <code>Dataset</code> objects.</li> <li><code>updated_config (ExperimentConfig)</code>: The experiment config, potentially updated with new label maps and counts discovered during processing.</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/data/#expected-data-formats","title":"Expected Data Formats","text":"<p>The <code>DataProcessor</code> expects your <code>.jsonl</code> data files to follow specific formats depending on the task type.</p>"},{"location":"api/data/#single_label_classification","title":"<code>single_label_classification</code>","text":"<p>Each line in the <code>.jsonl</code> file should be a JSON object with <code>text</code> and <code>label</code> keys.</p> <pre><code>{\"text\": \"This framework is wonderful and easy to use.\", \"label\": \"Positive\"}\n{\"text\": \"I am very frustrated with the results.\", \"label\": \"Negative\"}\n</code></pre>"},{"location":"api/data/#multi_label_classification","title":"<code>multi_label_classification</code>","text":"<p>Each line should have a <code>text</code> key and a <code>labels</code> key containing a list of all applicable string labels.</p> <pre><code>{\"text\": \"The agent was proactive and solved the issue.\", \"labels\": [\"proactiveness\", \"resolution\"]}\n{\"text\": \"The call started politely but the agent was not listening.\", \"labels\": [\"opening\", \"no_listening\"]}\n</code></pre>"},{"location":"api/data/#ner-named-entity-recognition","title":"<code>ner</code> (Named Entity Recognition)","text":"<p>Each line should have a <code>text</code> key and an <code>entities</code> key. <code>entities</code> should be a list of objects, where each object specifies the <code>label</code>, <code>start</code> character index, and <code>end</code> character index of an entity.</p> <pre><code>{\"text\": \"John Doe from Acme Corp is visiting Nairobi.\", \"entities\": [{\"label\": \"PERSON\", \"start\": 0, \"end\": 8}, {\"label\": \"ORG\", \"start\": 14, \"end\": 23}, {\"label\": \"LOCATION\", \"start\": 37, \"end\": 44}]}\n</code></pre>"},{"location":"api/tasks/","title":"API Reference: Tasks","text":"<p>A \"Task\" in Jenga-AI represents a specific NLP problem you want the model to solve. Each task is defined as a Python class that inherits from <code>BaseTask</code> and encapsulates the logic for its specific prediction heads and loss calculation.</p>"},{"location":"api/tasks/#basetask","title":"<code>BaseTask</code>","text":"<p>Class Path: <code>multitask_bert.tasks.base.BaseTask</code></p> <p>The <code>BaseTask</code> is an abstract class that provides the fundamental structure for all task implementations. It is a <code>torch.nn.Module</code> that defines the interface the <code>MultiTaskModel</code> uses to interact with a task.</p>"},{"location":"api/tasks/#key-components","title":"Key Components","text":"<ul> <li> <p><code>__init__(self, config, hidden_size)</code></p> <ul> <li>Purpose: The constructor for the task. Your implementation should call <code>super().__init__()</code> and then define the task-specific prediction heads (e.g., <code>nn.Linear</code> layers) and store them in the <code>self.heads</code> <code>ModuleDict</code>.</li> <li>Arguments:<ul> <li><code>config (TaskConfig)</code>: The configuration object for this specific task.</li> <li><code>hidden_size (int)</code>: The hidden size of the encoder's output, which is needed to correctly dimension the prediction heads.</li> </ul> </li> </ul> </li> <li> <p><code>get_forward_output(...)</code></p> <ul> <li>Purpose: This abstract method must be implemented by every subclass. It defines the task's forward pass, which takes the output from the shared encoder and produces the final logits and loss.</li> <li>Arguments:<ul> <li><code>encoder_outputs</code>: The output object from the Hugging Face encoder, which contains the <code>last_hidden_state</code> and <code>pooler_output</code>.</li> <li><code>labels</code>: The labels for the current batch.</li> </ul> </li> <li>Returns: A dictionary containing:<ul> <li><code>loss (torch.Tensor)</code>: The calculated loss for the batch.</li> <li><code>logits (Dict[str, torch.Tensor])</code>: A dictionary mapping each head name to its output logits.</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/tasks/#supported-task-implementations","title":"Supported Task Implementations","text":"<p>Jenga-AI comes with several pre-built task implementations.</p>"},{"location":"api/tasks/#multiheadsinglelabelclassificationtask","title":"<code>MultiHeadSingleLabelClassificationTask</code>","text":"<ul> <li>Class Path: <code>multitask_bert.tasks.classification.MultiHeadSingleLabelClassificationTask</code></li> <li>Config <code>type</code> string: <code>\"classification\"</code></li> <li>Description: A versatile task for single-label classification. It can have multiple prediction heads, where each head predicts a single class from its own set of labels. For example, one head could predict sentiment (Positive/Negative) while another predicts emotion (Happy/Sad/Angry).</li> <li>Heads: Uses the <code>pooled_output</code> (from the <code>[CLS]</code> token) and passes it through one or more <code>nn.Linear</code> layers.</li> <li>Loss Function: <code>nn.CrossEntropyLoss</code>.</li> </ul>"},{"location":"api/tasks/#multilabelclassificationtask","title":"<code>MultiLabelClassificationTask</code>","text":"<ul> <li>Class Path: <code>multitask_bert.tasks.classification.MultiLabelClassificationTask</code></li> <li>Config <code>type</code> string: <code>\"multi_label_classification\"</code></li> <li>Description: Used for multi-label problems where each input can be assigned zero or more labels from a set. For example, tagging an article with multiple topics.</li> <li>Heads: Uses the <code>pooled_output</code> and passes it through one or more <code>nn.Linear</code> layers.</li> <li>Loss Function: <code>nn.BCEWithLogitsLoss</code>.</li> </ul>"},{"location":"api/tasks/#nertask","title":"<code>NERTask</code>","text":"<ul> <li>Class Path: <code>multitask_bert.tasks.ner.NERTask</code></li> <li>Config <code>type</code> string: <code>\"ner\"</code></li> <li>Description: Used for Named Entity Recognition. This is a token-level classification task where the goal is to assign a label to each token in the input sequence (e.g., <code>B-PERSON</code>, <code>I-LOCATION</code>, <code>O</code>).</li> <li>Heads: Uses the <code>last_hidden_state</code> (the representation for every token) and passes it through a <code>nn.Linear</code> layer to get a logit for each token.</li> <li>Loss Function: <code>nn.CrossEntropyLoss</code>.</li> </ul> <p>See the How-to: Add a New Task guide for a step-by-step walkthrough of how to create your own custom task.</p>"},{"location":"api/trainer/","title":"API Reference: Trainer","text":"<p>The <code>Trainer</code> class is the main engine that orchestrates the training and evaluation of your <code>MultiTaskModel</code>. It brings together your configuration, data, and model to handle the entire experiment lifecycle.</p> <p>Class Path: <code>multitask_bert.training.trainer.Trainer</code></p>"},{"location":"api/trainer/#role-of-the-trainer","title":"Role of the Trainer","text":"<p>The <code>Trainer</code> is responsible for: - Creating DataLoaders: It builds PyTorch <code>DataLoader</code> instances for each task, using task-specific collate functions to handle batching correctly. - Setting up Optimization: It initializes the AdamW optimizer and a linear learning rate scheduler with warmup. - Executing the Training Loop: It runs the main training loop, iterating over epochs and feeding batches of data to the model. - Managing Multi-Tasking: It uses a round-robin scheduling approach to alternate between batches from different tasks, ensuring the model trains on all tasks in each epoch. - Running Evaluation: It performs evaluation at specified intervals, calculates metrics for each task head, and logs the results. - Handling Checkpointing &amp; Early Stopping: It saves the best model based on a chosen metric and can stop training early if performance stagnates. - Logging: It integrates with MLflow and TensorBoard to log metrics, parameters, and artifacts.</p>"},{"location":"api/trainer/#key-methods","title":"Key Methods","text":"<ul> <li> <p><code>__init__(self, config, model, tokenizer, train_datasets, eval_datasets)</code></p> <ul> <li>Purpose: Initializes the <code>Trainer</code> instance.</li> <li>Arguments:<ul> <li><code>config (ExperimentConfig)</code>: The complete experiment configuration object.</li> <li><code>model (MultiTaskModel)</code>: The instantiated multi-task model.</li> <li><code>tokenizer</code>: The Hugging Face tokenizer.</li> <li><code>train_datasets (Dict[str, Dataset])</code>: A dictionary mapping task names to their training <code>Dataset</code> objects.</li> <li><code>eval_datasets (Dict[str, Dataset])</code>: A dictionary mapping task names to their evaluation <code>Dataset</code> objects.</li> </ul> </li> <li>Note: In typical usage, you will use the <code>Trainer.from_config()</code> classmethod, which handles the initialization of the model, tokenizer, and data processor for you.</li> </ul> </li> <li> <p><code>train(self)</code></p> <ul> <li>Purpose: Starts the main training loop.</li> <li>Process:<ol> <li>Initializes the optimizer and scheduler.</li> <li>Iterates through the number of epochs defined in <code>training.num_epochs</code>.</li> <li>In each epoch, it creates iterators for each task's <code>DataLoader</code>.</li> <li>It then cycles through the tasks in a round-robin fashion, fetching one batch from each task, performing a forward and backward pass, and updating the model weights.</li> <li>Logs the training loss at each step.</li> <li>At the end of an epoch (or at specified step intervals), it calls <code>evaluate()</code>.</li> <li>Handles model checkpointing and early stopping logic.</li> </ol> </li> </ul> </li> <li> <p><code>evaluate(self, test: bool = False) -&gt; Dict[str, float]</code></p> <ul> <li>Purpose: Evaluates the model's performance on the evaluation (or test) dataset.</li> <li>Arguments:<ul> <li><code>test (bool)</code>: If <code>False</code> (default), uses the evaluation dataloaders. If <code>True</code>, it will use the test dataloaders (assuming they were created by the <code>DataProcessor</code>).</li> </ul> </li> <li>Process:<ol> <li>Sets the model to evaluation mode (<code>model.eval()</code>).</li> <li>Iterates through the dataloader for each task.</li> <li>Collects all predictions and labels for each task head.</li> <li>Computes the relevant metrics (e.g., F1, accuracy, precision) for each head using the utility functions in <code>multitask_bert.utils.metrics</code>.</li> <li>Aggregates all metrics into a single dictionary.</li> </ol> </li> <li>Returns: A dictionary containing all calculated metrics, prefixed with <code>eval_</code>. For example: <code>{'eval_Sentiment_sentiment_head_f1': 0.95, 'eval_SwahiliNER_ner_head_f1': 0.89}</code>.</li> </ul> </li> <li> <p><code>close(self)</code></p> <ul> <li>Purpose: Gracefully shuts down the logger (e.g., <code>mlflow.end_run()</code> or <code>SummaryWriter.close()</code>).</li> <li>Details: This method should be called at the end of your script, typically in a <code>finally</code> block, to ensure that all logs are saved correctly.</li> </ul> </li> </ul>"},{"location":"developer_guide/core_components/fusion/","title":"Attention Fusion","text":"<p>The Jenga-NLP framework includes an optional <code>AttentionFusion</code> layer that can be used to learn task-specific representations from the shared encoder. This can lead to improved performance in multi-task learning scenarios, as it allows the model to learn to focus on different aspects of the shared representation for each task.</p>"},{"location":"developer_guide/core_components/fusion/#how-it-works","title":"How it Works","text":"<p>The <code>AttentionFusion</code> layer is a simple yet effective mechanism for creating task-specific representations. It works as follows:</p> <ol> <li> <p>Task Embeddings: The <code>AttentionFusion</code> layer maintains a set of task embeddings, one for each task. These embeddings are learned during training and capture the unique characteristics of each task.</p> </li> <li> <p>Concatenation: For a given task, the corresponding task embedding is concatenated with the shared representation from the encoder at each token position.</p> </li> <li> <p>Attention Mechanism: The concatenated representation is then passed through a small feed-forward neural network (the \"attention layer\") to compute an attention score for each token. These scores are then passed through a softmax function to produce a set of attention weights.</p> </li> <li> <p>Fused Representation: The shared representation is then multiplied by the attention weights to produce the final task-specific (or \"fused\") representation.</p> </li> </ol> <p>This process is illustrated in the following diagram:</p> <pre><code>+-----------------------+\n| Shared Representation |\n| (from encoder)        |\n+-----------------------+\n           |\n           v\n+-----------------------+      +----------------+\n|      Concatenate      | &lt;--- | Task Embedding |\n+-----------------------+      +----------------+\n           |\n           v\n+-----------------------+\n|    Attention Layer    |\n|   (Linear + Tanh)     |\n+-----------------------+\n           |\n           v\n+-----------------------+\n|        Softmax        |\n+-----------------------+\n           |\n           v\n+-----------------------+\n|   Attention Weights   |\n+-----------------------+\n           |\n           v\n+-----------------------+\n|      Multiplication   |\n+-----------------------+\n           |\n           v\n+-----------------------+\n| Fused Representation  |\n+-----------------------+\n</code></pre>"},{"location":"developer_guide/core_components/fusion/#attentionfusion-class","title":"<code>AttentionFusion</code> Class","text":"<p>The <code>AttentionFusion</code> class in <code>multitask_bert/core/fusion.py</code> implements this mechanism. It has the following key methods:</p> <ul> <li> <p><code>__init__(self, config, num_tasks)</code>: The constructor for the fusion layer. It takes the following arguments:</p> <ul> <li><code>config</code>: A Hugging Face <code>PretrainedConfig</code> object.</li> <li><code>num_tasks</code>: The total number of tasks.</li> </ul> </li> <li> <p><code>forward(self, shared_representation, task_id)</code>: The forward pass for the fusion layer. It takes the following arguments:</p> <ul> <li><code>shared_representation</code>: The output from the shared encoder.</li> <li><code>task_id</code>: The ID of the current task.</li> </ul> <p>It returns the fused representation.</p> </li> </ul>"},{"location":"developer_guide/core_components/fusion/#how-to-use-it","title":"How to Use It","text":"<p>To use the <code>AttentionFusion</code> layer, you need to enable it in your <code>experiment.yaml</code> file by adding a <code>fusion</code> section to the <code>model</code> configuration:</p> <pre><code>model:\n  base_model: \"distilbert-base-uncased\"\n  dropout: 0.1\n  fusion:\n    type: \"attention\"\n    hidden_size: 768\n</code></pre> <ul> <li><code>type</code>: The type of fusion to use. Currently, only <code>\"attention\"</code> is supported.</li> <li><code>hidden_size</code>: The hidden size of the fusion layer. This should match the hidden size of the base model.</li> </ul> <p>When the <code>fusion</code> section is present in the configuration, the <code>MultiTaskModel</code> will automatically instantiate the <code>AttentionFusion</code> layer and use it to create task-specific representations before passing them to the task heads.</p>"},{"location":"developer_guide/core_components/model/","title":"Multi-Task Model","text":"<p>The <code>MultiTaskModel</code> class in <code>multitask_bert/core/model.py</code> is the central component of the Jenga-NLP framework. It is responsible for processing input data, generating representations, and making predictions for multiple tasks simultaneously.</p>"},{"location":"developer_guide/core_components/model/#architecture","title":"Architecture","text":"<p>The <code>MultiTaskModel</code> is built on top of the Hugging Face <code>PreTrainedModel</code> class, which allows it to easily load pre-trained transformer models from the Hugging Face Hub.</p> <p>The model has the following key components:</p> <ol> <li> <p>Shared Encoder: A pre-trained transformer model (e.g., <code>distilbert-base-uncased</code>) that acts as a shared encoder for all tasks. This encoder is responsible for generating contextualized representations of the input text.</p> </li> <li> <p>Task-Specific Heads: Each task has one or more prediction heads that are attached to the shared encoder. These heads are responsible for making predictions for their specific task. For example, a classification task might have a simple linear layer as its head, while a named entity recognition (NER) task might have a token-level classification head.</p> </li> <li> <p>Attention Fusion Layer (Optional): The model can optionally include an <code>AttentionFusion</code> layer. This layer sits between the shared encoder and the task-specific heads and learns to create task-specific representations from the shared representation.</p> </li> </ol>"},{"location":"developer_guide/core_components/model/#multitaskmodel-class","title":"<code>MultiTaskModel</code> Class","text":"<p>The <code>MultiTaskModel</code> class has the following key methods:</p> <ul> <li> <p><code>__init__(self, config, model_config, tasks)</code>: The constructor for the model. It takes the following arguments:</p> <ul> <li><code>config</code>: A Hugging Face <code>PretrainedConfig</code> object.</li> <li><code>model_config</code>: A <code>ModelConfig</code> object from the experiment configuration.</li> <li><code>tasks</code>: A list of <code>BaseTask</code> objects.</li> </ul> </li> <li> <p><code>forward(self, input_ids, attention_mask, task_id, labels=None, token_type_ids=None, **kwargs)</code>: The forward pass for the model. It takes a batch of data for a single task and returns the output of that task.</p> <ul> <li><code>input_ids</code>: The input token IDs.</li> <li><code>attention_mask</code>: The attention mask.</li> <li><code>task_id</code>: The ID of the task to run.</li> <li><code>labels</code>: The labels for the task.</li> <li><code>token_type_ids</code>: The token type IDs (for models that use them).</li> </ul> </li> </ul>"},{"location":"developer_guide/core_components/model/#how-it-works","title":"How it Works","text":"<ol> <li> <p>Input Processing: The <code>forward</code> method receives a batch of data for a specific task. The <code>input_ids</code> and <code>attention_mask</code> are passed to the shared encoder.</p> </li> <li> <p>Shared Representation: The shared encoder processes the input and generates a <code>last_hidden_state</code>, which is a sequence of hidden states for each token in the input.</p> </li> <li> <p>Attention Fusion (Optional): If the <code>AttentionFusion</code> layer is enabled, it takes the <code>last_hidden_state</code> and the <code>task_id</code> as input and produces a task-specific representation. This is done by learning an attention mechanism that weighs the shared representation differently for each task.</p> </li> <li> <p>Task-Specific Head: The (potentially fused) representation is then passed to the appropriate task-specific head, which is determined by the <code>task_id</code>.</p> </li> <li> <p>Output: The task head produces the final output for the task, which includes the loss and the logits.</p> </li> </ol>"},{"location":"developer_guide/core_components/model/#why-this-architecture","title":"Why this Architecture?","text":"<p>This architecture was chosen for its balance of performance and efficiency.</p> <ul> <li> <p>Parameter Efficiency: By sharing the encoder across all tasks, the model can learn more general-purpose representations of the language, which can lead to better performance on all tasks. It also significantly reduces the number of parameters that need to be trained, making the model more efficient to train and deploy.</p> </li> <li> <p>Flexibility: The use of task-specific heads allows the framework to handle a wide range of NLP tasks, each with its own specific output format.</p> </li> <li> <p>Extensibility: The modular design makes it easy to add new tasks or models to the framework without having to modify the core architecture.</p> </li> </ul>"},{"location":"developer_guide/core_components/tasks/","title":"Tasks","text":"<p>In the Jenga-NLP framework, a \"task\" represents a specific NLP problem that you want to solve, such as text classification, named entity recognition (NER), or question answering. Each task is defined by a <code>BaseTask</code> subclass, which encapsulates the task-specific logic, including the prediction heads, loss functions, and forward pass.</p>"},{"location":"developer_guide/core_components/tasks/#the-basetask-class","title":"The <code>BaseTask</code> Class","text":"<p>The <code>BaseTask</code> class in <code>multitask_bert/tasks/base.py</code> is an abstract base class that all other task classes must inherit from. It defines the following key methods:</p> <ul> <li> <p><code>__init__(self, config)</code>: The constructor for the task. It takes a <code>TaskConfig</code> object from the experiment configuration.</p> </li> <li> <p><code>get_forward_output(self, feature, pooled_output, sequence_output)</code>: This method defines the forward pass for the task. It takes the following arguments:</p> <ul> <li><code>feature</code>: A dictionary containing the labels and attention mask for the task.</li> <li><code>pooled_output</code>: The pooled output from the shared encoder (usually the hidden state of the <code>[CLS]</code> token).</li> <li><code>sequence_output</code>: The sequence output from the shared encoder (the hidden states for all tokens).</li> </ul> <p>It should return a <code>TaskOutput</code> object, which is a dataclass that contains the loss and the logits for the task.</p> </li> </ul>"},{"location":"developer_guide/core_components/tasks/#supported-task-types","title":"Supported Task Types","text":"<p>The Jenga-NLP framework currently supports the following task types:</p>"},{"location":"developer_guide/core_components/tasks/#singlelabelclassificationtask","title":"<code>SingleLabelClassificationTask</code>","text":"<ul> <li>Class: <code>multitask_bert.tasks.classification.SingleLabelClassificationTask</code></li> <li>Type String: <code>\"single_label_classification\"</code></li> <li>Description: This task is used for single-label text classification problems, where each input is assigned to one of several mutually exclusive classes.</li> <li>Heads: It uses a single <code>nn.Linear</code> layer as its prediction head.</li> <li>Loss Function: <code>nn.CrossEntropyLoss</code></li> </ul>"},{"location":"developer_guide/core_components/tasks/#multilabelclassificationtask","title":"<code>MultiLabelClassificationTask</code>","text":"<ul> <li>Class: <code>multitask_bert.tasks.classification.MultiLabelClassificationTask</code></li> <li>Type String: <code>\"multi_label_classification\"</code></li> <li>Description: This task is used for multi-label text classification problems, where each input can be assigned to multiple classes simultaneously.</li> <li>Heads: It can have multiple prediction heads, one for each label. Each head is a <code>nn.Linear</code> layer.</li> <li>Loss Function: <code>nn.BCEWithLogitsLoss</code></li> </ul>"},{"location":"developer_guide/core_components/tasks/#nertask","title":"<code>NERTask</code>","text":"<ul> <li>Class: <code>multitask_bert.tasks.ner.NERTask</code></li> <li>Type String: <code>\"ner\"</code></li> <li>Description: This task is used for named entity recognition, where the goal is to identify and classify named entities in a text.</li> <li>Heads: It uses a token-level classification head, which is a <code>nn.Linear</code> layer that is applied to each token's hidden state.</li> <li>Loss Function: <code>nn.CrossEntropyLoss</code></li> </ul>"},{"location":"developer_guide/core_components/tasks/#how-to-add-a-new-task","title":"How to Add a New Task","text":"<p>To add a new task to the framework, you need to do the following:</p> <ol> <li> <p>Create a new task class: Create a new Python file in the <code>multitask_bert/tasks/</code> directory and define a new class that inherits from <code>BaseTask</code>.</p> </li> <li> <p>Implement the <code>__init__</code> method: In the constructor of your new task class, you need to define the task-specific prediction heads.</p> </li> <li> <p>Implement the <code>get_forward_output</code> method: This method should define the forward pass for your task. It should take the <code>pooled_output</code> and <code>sequence_output</code> from the shared encoder and pass them through the prediction heads to generate the logits. It should also calculate the loss and return a <code>TaskOutput</code> object.</p> </li> <li> <p>Update <code>get_task_class</code>: In <code>examples/run_experiment.py</code>, update the <code>get_task_class</code> function to map your new task's type string to your new task class.</p> </li> <li> <p>Update the configuration: In your <code>experiment.yaml</code> file, add a new task configuration with the <code>type</code> set to your new task's type string.</p> </li> </ol>"},{"location":"developer_guide/core_concepts/architecture/","title":"Framework Architecture","text":"<p>The Jenga-NLP framework is designed with modularity and extensibility in mind. It follows a config-driven approach, allowing you to define and manage complex multi-task learning experiments with ease.</p>"},{"location":"developer_guide/core_concepts/architecture/#core-principles","title":"Core Principles","text":"<ul> <li>Modularity: The framework is divided into distinct components, each responsible for a specific part of the NLP pipeline. This makes it easy to understand, maintain, and extend the codebase.</li> <li>Extensibility: Adding new tasks, models, or datasets is a straightforward process that involves implementing a few base classes and updating the configuration.</li> <li>Config-Driven: All aspects of an experiment, from the model architecture to the training process, are defined in a single YAML configuration file. This ensures reproducibility and simplifies experiment management.</li> </ul>"},{"location":"developer_guide/core_concepts/architecture/#high-level-overview","title":"High-Level Overview","text":"<p>The framework can be broken down into the following key components:</p> <ol> <li> <p>Configuration (<code>multitask_bert/core/config.py</code>): This component is responsible for parsing the <code>experiment.yaml</code> file and creating the necessary configuration objects. It uses Python's <code>dataclasses</code> to define a clear and type-safe configuration structure.</p> </li> <li> <p>Data Processing (<code>multitask_bert/data/data_processing.py</code>): The <code>DataProcessor</code> class handles the loading and preprocessing of data for all tasks. It takes the raw data paths from the configuration and converts them into tokenized datasets ready for training.</p> </li> <li> <p>Tasks (<code>multitask_bert/tasks/</code>): Each task (e.g., classification, NER) is represented by a <code>BaseTask</code> subclass. These classes define the task-specific heads, loss functions, and forward pass logic.</p> </li> <li> <p>Model (<code>multitask_bert/core/model.py</code>): The <code>MultiTaskModel</code> is the heart of the framework. It consists of a shared encoder (e.g., <code>distilbert-base-uncased</code>) and a set of task-specific heads. It can also include an optional <code>AttentionFusion</code> layer to learn task-specific representations.</p> </li> <li> <p>Trainer (<code>multitask_bert/training/trainer.py</code>): The <code>Trainer</code> class orchestrates the entire training and evaluation process. It uses a round-robin approach to sample batches from different tasks and handles optimization, scheduling, and metric calculation.</p> </li> <li> <p>Logging (<code>multitask_bert/training/trainer.py</code>): The framework is integrated with MLflow and TensorBoard for experiment tracking. The <code>Trainer</code> class initializes the logger and logs metrics during training and evaluation.</p> </li> <li> <p>Deployment (<code>multitask_bert/deployment/</code>): This component provides tools for exporting trained models for inference and deploying them in production environments.</p> </li> </ol>"},{"location":"developer_guide/core_concepts/architecture/#data-flow","title":"Data Flow","text":"<p>The following diagram illustrates the data flow within the framework:</p> <pre><code>[experiment.yaml] -&gt; [DataProcessor] -&gt; [Tokenized Datasets]\n                                              |\n                                              v\n+-------------------------------------------------------------------------+\n|                                  Trainer                                  |\n|                                                                         |\n|  +-----------------+      +-----------------+      +-----------------+  |\n|  | Task 1 Dataloader|      | Task 2 Dataloader|      | Task 3 Dataloader|  |\n|  +-----------------+      +-----------------+      +-----------------+  |\n|          |                      |                      |                 |\n|          +----------------------+----------------------+                 |\n|                                 |                                        |\n|                                 v                                        |\n|  +---------------------------------------------------------------------+  |\n|  |                             MultiTaskModel                            |  |\n|  |                                                                     |  |\n|  |  +-----------------------------------------------------------------+  |  |\n|  |  |                         Shared Encoder                          |  |  |\n|  |  +-----------------------------------------------------------------+  |  |\n|  |                                 |                                     |  |\n|  |  +-----------------------------------------------------------------+  |  |\n|  |  |                       (Optional) Fusion Layer                     |  |  |\n|  |  +-----------------------------------------------------------------+  |  |\n|  |                                 |                                     |  |\n|  |  +----------+      +----------+      +----------+                     |  |\n|  |  | Task 1 Head|      | Task 2 Head|      | Task 3 Head|                     |  |\n|  |  +----------+      +----------+      +----------+                     |  |\n|  +---------------------------------------------------------------------+  |\n|                                 |                                        |\n|                                 v                                        |\n|  +---------------------------------------------------------------------+  |\n|  |                                Metrics                                |  |\n|  +---------------------------------------------------------------------+  |\n|                                 |                                        |\n|                                 v                                        |\n|  +---------------------------------------------------------------------+  |\n|  |                                Logger                                 |  |\n|  |                        (MLflow / TensorBoard)                         |  |\n|  +---------------------------------------------------------------------+  |\n+-------------------------------------------------------------------------+\n</code></pre>"},{"location":"developer_guide/core_concepts/configuration/","title":"Configuration","text":"<p>The Jenga-NLP framework is designed to be highly configurable, allowing you to define and customize your experiments using a single YAML file. This document provides a detailed explanation of the configuration options available.</p>"},{"location":"developer_guide/core_concepts/configuration/#top-level-configuration-experimentconfig","title":"Top-Level Configuration (<code>ExperimentConfig</code>)","text":"<p>The main configuration is defined by the <code>ExperimentConfig</code> dataclass in <code>multitask_bert/core/config.py</code>. It has the following top-level fields:</p> <ul> <li><code>project_name</code>: A string that identifies your project.</li> <li><code>model</code>: A <code>ModelConfig</code> object that defines the model architecture.</li> <li><code>tokenizer</code>: A <code>TokenizerConfig</code> object that defines the tokenizer settings.</li> <li><code>training</code>: A <code>TrainingConfig</code> object that defines the training process.</li> <li><code>tasks</code>: A list of <code>TaskConfig</code> objects, each defining a specific task.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#modelconfig","title":"<code>ModelConfig</code>","text":"<p>The <code>ModelConfig</code> dataclass defines the model architecture.</p> <ul> <li><code>base_model</code>: The name of the pre-trained transformer model to use from the Hugging Face Hub (e.g., <code>\"distilbert-base-uncased\"</code>).</li> <li><code>dropout</code>: The dropout probability for the model.</li> <li><code>fusion</code>: An optional <code>FusionConfig</code> object to enable and configure the attention fusion layer.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#fusionconfig","title":"<code>FusionConfig</code>","text":"<p>The <code>FusionConfig</code> dataclass enables and configures the attention fusion layer.</p> <ul> <li><code>type</code>: The type of fusion to use. Currently, only <code>\"attention\"</code> is supported.</li> <li><code>hidden_size</code>: The hidden size of the fusion layer. This should match the hidden size of the base model.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#tokenizerconfig","title":"<code>TokenizerConfig</code>","text":"<p>The <code>TokenizerConfig</code> dataclass defines the tokenizer settings.</p> <ul> <li><code>max_length</code>: The maximum sequence length for the tokenizer.</li> <li><code>padding</code>: The padding strategy to use (e.g., <code>\"max_length\"</code>).</li> <li><code>truncation</code>: Whether to truncate sequences that are longer than <code>max_length</code>.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#trainingconfig","title":"<code>TrainingConfig</code>","text":"<p>The <code>TrainingConfig</code> dataclass defines the training process.</p> <ul> <li><code>output_dir</code>: The directory where the training results will be saved.</li> <li><code>learning_rate</code>: The learning rate for the optimizer.</li> <li><code>batch_size</code>: The batch size for training and evaluation.</li> <li><code>num_epochs</code>: The number of training epochs.</li> <li><code>weight_decay</code>: The weight decay for the optimizer.</li> <li><code>warmup_steps</code>: The number of warmup steps for the learning rate scheduler.</li> <li><code>eval_strategy</code>: The evaluation strategy to use (e.g., <code>\"epoch\"</code>).</li> <li><code>save_strategy</code>: The save strategy to use (e.g., <code>\"epoch\"</code>).</li> <li><code>load_best_model_at_end</code>: Whether to load the best model at the end of training.</li> <li><code>metric_for_best_model</code>: The metric to use for determining the best model.</li> <li><code>greater_is_better</code>: Whether a higher value of <code>metric_for_best_model</code> is better.</li> <li><code>early_stopping_patience</code>: The number of epochs to wait for improvement before stopping training.</li> <li><code>logging</code>: A <code>LoggingConfig</code> object to configure experiment tracking.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#loggingconfig","title":"<code>LoggingConfig</code>","text":"<p>The <code>LoggingConfig</code> dataclass configures experiment tracking.</p> <ul> <li><code>service</code>: The logging service to use. Can be <code>\"tensorboard\"</code> or <code>\"mlflow\"</code>.</li> <li><code>experiment_name</code>: The name of the experiment.</li> <li><code>tracking_uri</code>: The tracking URI for MLflow (optional).</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#taskconfig","title":"<code>TaskConfig</code>","text":"<p>The <code>TaskConfig</code> dataclass defines a specific task.</p> <ul> <li><code>name</code>: The name of the task.</li> <li><code>type</code>: The type of the task. Supported types are:<ul> <li><code>\"single_label_classification\"</code></li> <li><code>\"multi_label_classification\"</code></li> <li><code>\"ner\"</code></li> </ul> </li> <li><code>data_path</code>: The path to the data file for the task.</li> <li><code>heads</code>: A list of <code>HeadConfig</code> objects, each defining a prediction head for the task.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#headconfig","title":"<code>HeadConfig</code>","text":"<p>The <code>HeadConfig</code> dataclass defines a prediction head.</p> <ul> <li><code>name</code>: The name of the head.</li> <li><code>num_labels</code>: The number of labels for the head.</li> <li><code>weight</code>: The weight of the head's loss in the total loss.</li> </ul>"},{"location":"developer_guide/core_concepts/configuration/#example-configuration","title":"Example Configuration","text":"<p>Here is an example of a complete <code>experiment.yaml</code> file:</p> <pre><code>project_name: \"JengaAI_Unified_Framework\"\n\nmodel:\n  base_model: \"distilbert-base-uncased\"\n  dropout: 0.1\n\ntokenizer:\n  max_length: 256\n  padding: \"max_length\"\n  truncation: true\n\ntraining:\n  output_dir: \"./unified_results\"\n  learning_rate: 2.0e-5\n  batch_size: 8\n  num_epochs: 5\n  weight_decay: 0.01\n  warmup_steps: 100\n  eval_strategy: \"epoch\"\n  save_strategy: \"epoch\"\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n  early_stopping_patience: 3\n  logging:\n    service: \"mlflow\"\n    experiment_name: \"JengaAI_MVP\"\n\ntasks:\n  - name: \"SentimentClassifier\"\n    type: \"single_label_classification\"\n    data_path: \"examples/sentiment_data.jsonl\"\n    heads:\n      - name: \"sentiment_head\"\n        num_labels: 2\n        weight: 1.0\n\n  - name: \"SwahiliNER\"\n    type: \"ner\"\n    data_path: \"examples/ner_data.jsonl\"\n    heads:\n      - name: \"ner_head\"\n        num_labels: 13\n        weight: 1.0\n</code></pre>"},{"location":"developer_guide/deployment/deployment/","title":"Deployment","text":"<p>The Jenga-NLP framework provides tools for exporting trained models for inference and deploying them in production environments. This document provides an overview of the deployment options available.</p>"},{"location":"developer_guide/deployment/deployment/#exporting-models","title":"Exporting Models","text":"<p>The <code>multitask_bert/deployment/export.py</code> module (which is not yet implemented) will provide a script for exporting trained models to a format that is suitable for inference. This will likely involve saving the model's weights and the tokenizer's vocabulary to a directory.</p> <p>The exported model can then be loaded into an inference script or a production environment for making predictions on new data.</p>"},{"location":"developer_guide/deployment/deployment/#inference","title":"Inference","text":"<p>The <code>multitask_bert/deployment/inference.py</code> module (which is not yet implemented) will provide a class for running inference with a trained model. This class will likely have the following key methods:</p> <ul> <li> <p><code>__init__(self, model_path)</code>: The constructor for the inference class. It will take the path to the exported model as input.</p> </li> <li> <p><code>predict(self, text, task_name)</code>: This method will take a piece of text and the name of the task to run as input and will return the predictions for that task.</p> </li> </ul>"},{"location":"developer_guide/deployment/deployment/#deployment-options","title":"Deployment Options","text":"<p>There are several options for deploying your trained models:</p>"},{"location":"developer_guide/deployment/deployment/#1-rest-api-with-fastapi","title":"1. REST API with FastAPI","text":"<p>You can create a REST API for your model using a web framework like FastAPI. This will allow you to serve your model over the network and integrate it with other applications.</p> <p>The API would have an endpoint that takes a piece of text and a task name as input and returns the predictions from the model.</p>"},{"location":"developer_guide/deployment/deployment/#2-batch-inference","title":"2. Batch Inference","text":"<p>If you need to make predictions on a large amount of data, you can use a batch inference script. This script would load the trained model and the data, and would then iterate through the data, making predictions for each example.</p> <p>The predictions can then be saved to a file or a database for further analysis.</p>"},{"location":"developer_guide/deployment/deployment/#3-integration-with-other-applications","title":"3. Integration with Other Applications","text":"<p>You can also integrate your trained model with other applications, such as a chatbot or a content moderation system. This would involve loading the model into the application and using it to make predictions on the application's data.</p>"},{"location":"developer_guide/deployment/deployment/#future-work","title":"Future Work","text":"<p>The deployment component of the Jenga-NLP framework is still under development. In the future, we plan to add the following features:</p> <ul> <li>ONNX Export: Support for exporting models to the ONNX format for improved performance and cross-platform compatibility.</li> <li>Inference Optimization: Tools for optimizing models for inference, such as quantization and pruning.</li> <li>Pre-built Docker Images: Pre-built Docker images for deploying models in a containerized environment.</li> </ul>"},{"location":"developer_guide/logging/logging/","title":"Logging","text":"<p>The Jenga-NLP framework is integrated with MLflow and TensorBoard for experiment tracking, allowing you to monitor your training process, compare different experiments, and visualize your results.</p>"},{"location":"developer_guide/logging/logging/#how-it-works","title":"How it Works","text":"<p>The <code>Trainer</code> class in <code>multitask_bert/training/trainer.py</code> is responsible for initializing the logger and logging the metrics during training and evaluation.</p> <p>At the beginning of the training process, the <code>Trainer</code> checks the <code>logging</code> section of the <code>TrainingConfig</code> to determine which logging service to use. It then initializes the appropriate logger (<code>SummaryWriter</code> for TensorBoard or <code>mlflow</code> for MLflow).</p> <p>During the training loop, the <code>Trainer</code> logs the training loss at each step. During the evaluation loop, it logs the evaluation metrics for each task and head.</p>"},{"location":"developer_guide/logging/logging/#how-to-configure-logging","title":"How to Configure Logging","text":"<p>To configure logging, you need to add a <code>logging</code> section to the <code>training</code> configuration in your <code>experiment.yaml</code> file.</p>"},{"location":"developer_guide/logging/logging/#tensorboard","title":"TensorBoard","text":"<p>To use TensorBoard, set the <code>service</code> to <code>\"tensorboard\"</code>:</p> <pre><code>training:\n  # ...\n  logging:\n    service: \"tensorboard\"\n    experiment_name: \"JengaAI_MVP\"\n</code></pre> <ul> <li><code>service</code>: The logging service to use.</li> <li><code>experiment_name</code>: The name of the experiment. The TensorBoard logs will be saved to <code>output_dir/logs/experiment_name</code>.</li> </ul> <p>To view the TensorBoard logs, run the following command in your terminal:</p> <pre><code>tensorboard --logdir=./unified_results/logs\n</code></pre>"},{"location":"developer_guide/logging/logging/#mlflow","title":"MLflow","text":"<p>To use MLflow, set the <code>service</code> to <code>\"mlflow\"</code>:</p> <pre><code>training:\n  # ...\n  logging:\n    service: \"mlflow\"\n    experiment_name: \"JengaAI_MVP\"\n    tracking_uri: \"http://localhost:5000\" # Optional\n</code></pre> <ul> <li><code>service</code>: The logging service to use.</li> <li><code>experiment_name</code>: The name of the experiment.</li> <li><code>tracking_uri</code>: The tracking URI for the MLflow server (optional). If not provided, MLflow will log to a local <code>mlruns</code> directory.</li> </ul> <p>To view the MLflow UI, run the following command in your terminal:</p> <pre><code>mlflow ui\n</code></pre>"},{"location":"developer_guide/logging/logging/#_init_logger-and-_log_metrics","title":"<code>_init_logger</code> and <code>_log_metrics</code>","text":"<p>The <code>Trainer</code> class has two key methods for logging:</p> <ul> <li> <p><code>_init_logger(self)</code>: This method is called in the constructor of the <code>Trainer</code> and is responsible for initializing the logger based on the configuration.</p> </li> <li> <p><code>_log_metrics(self, metrics, step, prefix)</code>: This method is called during the training and evaluation loops to log the metrics. It takes the following arguments:</p> <ul> <li><code>metrics</code>: A dictionary of metrics to log.</li> <li><code>step</code>: The current step or epoch.</li> <li><code>prefix</code>: A prefix to add to the metric names (e.g., <code>\"Train\"</code> or <code>\"Eval\"</code>).</li> </ul> </li> </ul> <p>By using these methods, the <code>Trainer</code> provides a consistent and flexible way to log your experiment results, regardless of which logging service you choose.</p>"},{"location":"developer_guide/training_evaluation/metrics/","title":"Metrics","text":"<p>The Jenga-NLP framework provides a set of metric functions for evaluating the performance of your multi-task models. These functions are located in <code>multitask_bert/utils/metrics.py</code> and are used by the <code>Trainer</code> class during the evaluation process.</p>"},{"location":"developer_guide/training_evaluation/metrics/#supported-metrics","title":"Supported Metrics","text":"<p>The framework currently supports the following metrics:</p>"},{"location":"developer_guide/training_evaluation/metrics/#classification-metrics","title":"Classification Metrics","text":"<ul> <li>Function: <code>compute_classification_metrics(preds, labels)</code></li> <li>Description: This function computes the accuracy, precision, recall, and F1 score for single-label classification tasks.</li> <li>Arguments:<ul> <li><code>preds</code>: The predictions from the model.</li> <li><code>labels</code>: The true labels.</li> </ul> </li> <li>Returns: A dictionary of metrics.</li> </ul>"},{"location":"developer_guide/training_evaluation/metrics/#multi-label-metrics","title":"Multi-Label Metrics","text":"<ul> <li>Function: <code>compute_multi_label_metrics(preds, labels)</code></li> <li>Description: This function computes the accuracy, precision, recall, and F1 score for multi-label classification tasks.</li> <li>Arguments:<ul> <li><code>preds</code>: The predictions from the model.</li> <li><code>labels</code>: The true labels.</li> </ul> </li> <li>Returns: A dictionary of metrics.</li> </ul>"},{"location":"developer_guide/training_evaluation/metrics/#ner-metrics","title":"NER Metrics","text":"<ul> <li>Function: <code>compute_ner_metrics(preds, labels, label_map)</code></li> <li>Description: This function computes the accuracy, precision, recall, and F1 score for named entity recognition (NER) tasks. It uses the <code>seqeval</code> library to compute the metrics at the entity level.</li> <li>Arguments:<ul> <li><code>preds</code>: The predictions from the model.</li> <li><code>labels</code>: The true labels.</li> <li><code>label_map</code>: A dictionary that maps label IDs to label names.</li> </ul> </li> <li>Returns: A dictionary of metrics.</li> </ul>"},{"location":"developer_guide/training_evaluation/metrics/#how-metrics-are-calculated","title":"How Metrics are Calculated","text":"<p>During the evaluation process, the <code>Trainer</code> class iterates through each task's evaluation dataloader and collects the predictions and labels for each head. It then calls the appropriate metric function based on the task type to compute the metrics for that head.</p> <p>The metrics for all heads are then aggregated into a single dictionary and logged to MLflow or TensorBoard.</p>"},{"location":"developer_guide/training_evaluation/metrics/#custom-metrics","title":"Custom Metrics","text":"<p>To use a custom metric function, you will need to modify the <code>evaluate</code> method of the <code>Trainer</code> class in <code>multitask_bert/training/trainer.py</code>.</p> <ol> <li> <p>Import your metric function: Import your custom metric function at the top of the file.</p> </li> <li> <p>Call your metric function: In the <code>evaluate</code> method, after the predictions and labels have been collected, call your custom metric function with the predictions and labels.</p> </li> <li> <p>Add the results to the <code>task_metrics</code> dictionary: Add the results of your custom metric function to the <code>task_metrics</code> dictionary.</p> </li> </ol> <p>By following these steps, you can easily extend the framework to support any metric function you need.</p>"},{"location":"developer_guide/training_evaluation/trainer/","title":"Trainer","text":"<p>The <code>Trainer</code> class in <code>multitask_bert/training/trainer.py</code> is responsible for orchestrating the entire training and evaluation process. It brings together the model, data, and configuration to train the multi-task model and evaluate its performance.</p>"},{"location":"developer_guide/training_evaluation/trainer/#key-responsibilities","title":"Key Responsibilities","text":"<p>The <code>Trainer</code> class has the following key responsibilities:</p> <ul> <li>Dataloader Creation: It creates task-specific dataloaders for training and evaluation.</li> <li>Optimizer and Scheduler: It creates the optimizer (AdamW) and the learning rate scheduler.</li> <li>Training Loop: It implements the main training loop, which iterates over the data and updates the model's weights.</li> <li>Evaluation Loop: It implements the evaluation loop, which calculates the metrics for each task.</li> <li>Logging: It logs the training and evaluation metrics to MLflow or TensorBoard.</li> <li>Early Stopping: It implements early stopping to prevent overfitting.</li> </ul>"},{"location":"developer_guide/training_evaluation/trainer/#training-process","title":"Training Process","text":"<p>The training process is designed to handle multiple tasks simultaneously using a round-robin approach.</p> <ol> <li> <p>Dataloader Iterators: At the beginning of each epoch, the <code>Trainer</code> creates iterators for each task's dataloader.</p> </li> <li> <p>Round-Robin Sampling: The <code>Trainer</code> then iterates through the tasks in a round-robin fashion, sampling one batch from each task's dataloader at a time.</p> </li> <li> <p>Forward Pass: For each batch, the <code>Trainer</code> performs a forward pass through the <code>MultiTaskModel</code>, providing the <code>input_ids</code>, <code>attention_mask</code>, <code>labels</code>, and <code>task_id</code>.</p> </li> <li> <p>Backward Pass: The loss from the forward pass is then used to perform a backward pass and update the model's weights.</p> </li> <li> <p>Logging: The training loss is logged to MLflow or TensorBoard at each step.</p> </li> </ol> <p>This process continues until all dataloaders are exhausted.</p>"},{"location":"developer_guide/training_evaluation/trainer/#evaluation-process","title":"Evaluation Process","text":"<p>The evaluation process is performed at the end of each epoch (if <code>eval_strategy</code> is set to <code>\"epoch\"</code>).</p> <ol> <li> <p>Evaluation Mode: The model is first set to evaluation mode (<code>model.eval()</code>).</p> </li> <li> <p>Iterate Through Tasks: The <code>Trainer</code> then iterates through each task's evaluation dataloader.</p> </li> <li> <p>Collect Predictions and Labels: For each batch, the <code>Trainer</code> performs a forward pass and collects the predictions and labels for each head of the task.</p> </li> <li> <p>Compute Metrics: Once all batches have been processed, the <code>Trainer</code> computes the metrics for each head using the appropriate metric function (e.g., <code>compute_classification_metrics</code>, <code>compute_ner_metrics</code>).</p> </li> <li> <p>Log Metrics: The evaluation metrics are then logged to MLflow or TensorBoard.</p> </li> </ol>"},{"location":"developer_guide/training_evaluation/trainer/#trainer-class","title":"<code>Trainer</code> Class","text":"<p>The <code>Trainer</code> class has the following key methods:</p> <ul> <li> <p><code>__init__(self, config, model, tokenizer, train_datasets, eval_datasets)</code>: The constructor for the <code>Trainer</code>. It takes the following arguments:</p> <ul> <li><code>config</code>: The <code>ExperimentConfig</code> object.</li> <li><code>model</code>: The <code>MultiTaskModel</code> object.</li> <li><code>tokenizer</code>: The tokenizer object.</li> <li><code>train_datasets</code>: A dictionary of training datasets, where the keys are task names.</li> <li><code>eval_datasets</code>: A dictionary of evaluation datasets, where the keys are task names.</li> </ul> </li> <li> <p><code>train(self)</code>: This method starts the training process.</p> </li> <li> <p><code>evaluate(self)</code>: This method performs the evaluation process.</p> </li> <li> <p><code>close(self)</code>: This method closes the logger.</p> </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Follow these instructions to set up Jenga-AI on your local machine. We recommend using a virtual environment to manage dependencies and avoid conflicts with other projects.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li><code>pip</code> (Python package installer)</li> </ul>"},{"location":"getting-started/installation/#1-create-a-virtual-environment","title":"1. Create a Virtual Environment","text":"<p>A virtual environment is a self-contained directory that holds a specific Python installation and its packages.</p> <p>=== \"macOS / Linux\"</p> <pre><code>```bash\n# Create a directory for your project\nmkdir my-jenga-project\ncd my-jenga-project\n\n# Create a virtual environment named 'venv'\npython3 -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate\n```\n</code></pre> <p>=== \"Windows\"</p> <pre><code>```bash\n# Create a directory for your project\nmkdir my-jenga-project\ncd my-jenga-project\n\n# Create a virtual environment named 'venv'\npython -m venv venv\n\n# Activate the virtual environment\nvenv\\Scripts\\activate\n```\n</code></pre> <p>After activation, your command prompt will be prefixed with <code>(venv)</code>, indicating that you are now working inside the virtual environment.</p>"},{"location":"getting-started/installation/#2-install-jenga-ai","title":"2. Install Jenga-AI","text":"<p>You can install Jenga-AI directly from its source code by cloning the repository.</p>"},{"location":"getting-started/installation/#clone-the-repository","title":"Clone the Repository","text":"<p>First, clone the Jenga-AI repository from GitHub:</p> <pre><code>git clone https://github.com/Rogendo/Jenga-AI.git\ncd Jenga-AI\n</code></pre>"},{"location":"getting-started/installation/#install-dependencies","title":"Install Dependencies","text":"<p>Once your virtual environment is active and you are in the <code>Jenga-AI</code> directory, install the required packages from the <code>requirements.txt</code> file:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>This command installs all the necessary libraries, including PyTorch, Hugging Face Transformers, and others.</p>"},{"location":"getting-started/installation/#install-jenga-ai-in-editable-mode","title":"Install Jenga-AI in Editable Mode","text":"<p>Finally, install the Jenga-AI framework itself in \"editable\" mode. This allows you to make changes to the source code and have them immediately reflected in your environment, which is ideal for development.</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#3-verify-installation","title":"3. Verify Installation","text":"<p>To ensure that the framework is installed correctly, you can run a quick test by importing one of the core classes in a Python interpreter:</p> <pre><code>python\n</code></pre> <p>Then, within the Python interpreter:</p> <pre><code>try:\n    from multitask_bert.core.config import ExperimentConfig\n    print(\"\u2705 Jenga-AI installation successful!\")\nexcept ImportError as e:\n    print(f\"\u274c Installation failed: {e}\")\n</code></pre> <p>If you see the success message, you are all set! You can now proceed to the Quickstart Guide to train your first model.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Let's train your first multi-task model with Jenga-AI in just a few minutes. In this guide, we will train a single model to perform two tasks simultaneously: 1.  Sentiment Classification: Classifying text as \"Positive\" or \"Negative\". 2.  Threat Detection: A simple binary classification to identify if a text contains a potential threat.</p>"},{"location":"getting-started/quickstart/#1-project-structure","title":"1. Project Structure","text":"<p>First, create a new project directory and set up the following file structure:</p> <pre><code>jenga-quickstart/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 sentiment_data.jsonl\n\u2502   \u2514\u2500\u2500 threat_data.jsonl\n\u251c\u2500\u2500 experiment.yaml\n\u2514\u2500\u2500 run.py\n</code></pre>"},{"location":"getting-started/quickstart/#2-create-your-data-files","title":"2. Create Your Data Files","text":"<p>Create the two data files inside the <code>data/</code> directory.</p> <p><code>data/sentiment_data.jsonl</code>: <pre><code>{\"text\": \"I love this framework, it's so easy to use!\", \"label\": \"Positive\"}\n{\"text\": \"The documentation is clear and helpful.\", \"label\": \"Positive\"}\n{\"text\": \"I encountered an error and it was frustrating.\", \"label\": \"Negative\"}\n{\"text\": \"This is the worst product I have ever used.\", \"label\": \"Negative\"}\n</code></pre></p> <p><code>data/threat_data.jsonl</code>: <pre><code>{\"text\": \"Remember to submit the weekly security report.\", \"label\": \"non-threat\"}\n{\"text\": \"The system is under attack, we need to respond now!\", \"label\": \"threat\"}\n{\"text\": \"I will shut down the entire network if my demands are not met.\", \"label\": \"threat\"}\n{\"text\": \"Let's have a team meeting tomorrow at 10 AM.\", \"label\": \"non-threat\"}\n</code></pre></p>"},{"location":"getting-started/quickstart/#3-define-your-experiment","title":"3. Define Your Experiment","text":"<p>Now, create the <code>experiment.yaml</code> file. This file tells Jenga-AI everything it needs to know about our model, tasks, and training process.</p> <p><code>experiment.yaml</code>: <pre><code>project_name: \"JengaAI_Quickstart\"\n\nmodel:\n  base_model: \"distilbert-base-uncased\" # A small, fast model for quick testing\n  dropout: 0.1\n\ntokenizer:\n  max_length: 128\n  padding: \"max_length\"\n  truncation: true\n\ntraining:\n  output_dir: \"./quickstart_results\"\n  learning_rate: 2.0e-5\n  batch_size: 2\n  num_epochs: 1 # Just one epoch for a quick run\n  weight_decay: 0.01\n  eval_strategy: \"epoch\"\n  logging:\n    service: \"tensorboard\" # Logs will be saved to output_dir/logs\n    experiment_name: \"Quickstart_Run\"\n\ntasks:\n  # Task 1: Sentiment Analysis\n  - name: \"Sentiment\"\n    type: \"single_label_classification\"\n    data_path: \"data/sentiment_data.jsonl\"\n    heads:\n      - name: \"sentiment_head\"\n        num_labels: 2 # Positive, Negative\n        label_map:\n          \"Positive\": 0\n          \"Negative\": 1\n        weight: 1.0\n\n  # Task 2: Threat Detection\n  - name: \"Threat\"\n    type: \"single_label_classification\"\n    data_path: \"data/threat_data.jsonl\"\n    heads:\n      - name: \"threat_head\"\n        num_labels: 2 # threat, non-threat\n        label_map:\n          \"threat\": 0\n          \"non-threat\": 1\n        weight: 1.0\n</code></pre></p>"},{"location":"getting-started/quickstart/#4-create-the-training-script","title":"4. Create the Training Script","text":"<p>Finally, create the <code>run.py</code> script. This is the code that will load your configuration and start the training process.</p> <p><code>run.py</code>: <pre><code>from multitask_bert.core.config import ExperimentConfig\nfrom multitask_bert.data.data_processing import DataProcessor\nfrom multitask_bert.training.trainer import Trainer\n\ndef main():\n    print(\"\ud83d\ude80 Starting Jenga-AI Quickstart...\")\n\n    # 1. Load experiment configuration from YAML\n    config = ExperimentConfig.from_yaml(\"experiment.yaml\")\n    print(\"\u2705 Configuration loaded.\")\n\n    # 2. Process the data for all tasks\n    data_processor = DataProcessor(config)\n    train_datasets, eval_datasets = data_processor.process_data()\n    print(f\"\u2705 Data processed for tasks: {[task.name for task in config.tasks]}\")\n\n    # 3. Initialize the Trainer\n    # The trainer will automatically build the model based on the config\n    trainer = Trainer(\n        config=config,\n        train_datasets=train_datasets,\n        eval_datasets=eval_datasets,\n    )\n    print(\"\u2705 Trainer initialized.\")\n\n    # 4. Start training!\n    print(\"\ud83d\udd25 Starting training...\")\n    trainer.train()\n    print(\"\ud83c\udf89 Training complete!\")\n    print(f\"\u2728 Your model and results are saved in: {config.training.output_dir}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"getting-started/quickstart/#5-run-the-training","title":"5. Run the Training","text":"<p>Make sure you have installed Jenga-AI and are in your activated virtual environment. Then, run the script from your terminal:</p> <pre><code>python run.py\n</code></pre> <p>That's it! You've just trained your first multi-task model. You can inspect the results and logs in the <code>quickstart_results</code> directory.</p> <p>Now that you've seen the basics, you can explore the Tutorials for more in-depth examples.</p>"},{"location":"guides/adding_a_new_task/","title":"How-to: Add a New Task","text":"<p>Jenga-AI is designed to be extensible. One of the most common ways to extend the framework is by adding a new task type. This guide will walk you through the process of creating a custom task.</p> <p>As an example, we will create a <code>RegressionTask</code>, which predicts a single continuous value (e.g., a score from 1.0 to 5.0).</p>"},{"location":"guides/adding_a_new_task/#1-understand-the-basetask-class","title":"1. Understand the <code>BaseTask</code> Class","text":"<p>Every task in Jenga-AI must inherit from the <code>BaseTask</code> class, which is found in <code>multitask_bert/tasks/base.py</code>. This class provides the fundamental structure that the framework expects.</p> <p>The two key methods you need to implement are: - <code>__init__(self, config, hidden_size)</code>: The constructor where you define the task-specific prediction heads. - <code>get_forward_output(...)</code>: The method that defines the forward pass for your task, taking the output from the shared encoder and calculating the loss and logits.</p>"},{"location":"guides/adding_a_new_task/#2-create-the-task-file","title":"2. Create the Task File","text":"<p>First, create a new file in the <code>multitask_bert/tasks/</code> directory. Let's call it <code>regression.py</code>.</p> <pre><code>touch multitask_bert/tasks/regression.py\n</code></pre>"},{"location":"guides/adding_a_new_task/#3-implement-the-custom-task-class","title":"3. Implement the Custom Task Class","text":"<p>Now, let's implement our <code>RegressionTask</code> inside <code>multitask_bert/tasks/regression.py</code>.</p> <pre><code># multitask_bert/tasks/regression.py\n\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, Any, Optional\n\nfrom .base import BaseTask\nfrom ..core.config import TaskConfig\n\nclass RegressionTask(BaseTask):\n    \"\"\"\n    A task for predicting a single continuous value for one or more heads.\n    \"\"\"\n    def __init__(self, config: TaskConfig, hidden_size: int):\n        # 1. Call the parent constructor\n        super().__init__(config, hidden_size)\n\n        # 2. Define the prediction heads\n        # For regression, each head is a linear layer that outputs a single value.\n        for head in config.heads:\n            self.heads[head.name] = nn.Linear(self.hidden_size, 1)\n\n    def get_forward_output(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        token_type_ids: Optional[torch.Tensor],\n        labels: Optional[Any],\n        encoder_outputs: Any,\n        **kwargs\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Defines the forward pass and loss calculation for the regression task.\n        \"\"\"\n        total_loss = None\n        all_logits = {}\n\n        # 3. Define the loss function\n        # Mean Squared Error is a common choice for regression.\n        loss_fct = nn.MSELoss()\n\n        # Use the pooled output (representation of the [CLS] token)\n        pooled_output = encoder_outputs.pooler_output if hasattr(encoder_outputs, 'pooler_output') and encoder_outputs.pooler_output is not None else encoder_outputs.last_hidden_state[:, 0]\n\n        for head_config in self.config.heads:\n            head_name = head_config.name\n            head_layer = self.heads[head_name]\n\n            # The output of the head is our predicted value (logit)\n            # We squeeze it to remove the last dimension (e.g., from [batch_size, 1] to [batch_size])\n            logits = head_layer(pooled_output).squeeze(-1)\n            all_logits[head_name] = logits\n\n            # 4. Calculate the loss\n            if labels is not None and head_name in labels and labels[head_name] is not None:\n                head_labels = labels[head_name].float() # Ensure labels are floats\n                loss = loss_fct(logits, head_labels)\n\n                if total_loss is None:\n                    total_loss = loss * head_config.weight\n                else:\n                    total_loss += loss * head_config.weight\n\n        # If no labels were provided, return a zero tensor for the loss\n        if total_loss is None:\n            total_loss = torch.tensor(0.0, device=pooled_output.device, requires_grad=True)\n\n        return {\"loss\": total_loss, \"logits\": all_logits}\n</code></pre>"},{"location":"guides/adding_a_new_task/#4-register-the-new-task","title":"4. Register the New Task","text":"<p>To make the framework aware of your new task, you need to add it to the <code>get_task_class</code> function in <code>examples/run_experiment.py</code> (or your main training script).</p> <pre><code># In examples/run_experiment.py\n\n# Import your new task at the top of the file\nfrom multitask_bert.tasks.regression import RegressionTask\n\ndef get_task_class(task_type: str) -&gt; BaseTask:\n    \"\"\"Maps a task type string to its corresponding class.\"\"\"\n    if task_type == \"classification\":\n        return MultiHeadSingleLabelClassificationTask\n    elif task_type == \"multi_label_classification\":\n        return MultiLabelClassificationTask\n    elif task_type == \"ner\":\n        return NERTask\n    # Add your new task here\n    elif task_type == \"regression\":\n        return RegressionTask\n    else:\n        raise ValueError(f\"Unknown task type: {task_type}\")\n</code></pre> <p>Note: In the future, this manual step will be replaced by an automatic task registry (see Issue #3.4 in <code>PROJECT_ROADMAP.md</code>).</p>"},{"location":"guides/adding_a_new_task/#5-use-the-new-task-in-a-configuration","title":"5. Use the New Task in a Configuration","text":"<p>You can now use your new task in an <code>experiment.yaml</code> file by setting the <code>type</code> to <code>\"regression\"</code>.</p> <pre><code># In your experiment.yaml\n\ntasks:\n  - name: \"ReviewScore\"\n    type: \"regression\" # Use the new task type\n    data_path: \"/path/to/your/regression_data.jsonl\"\n    heads:\n      - name: \"score_head\"\n        # num_labels is not strictly needed here as the head outputs 1 value,\n        # but it can be useful for consistency.\n        num_labels: 1\n        weight: 1.0\n</code></pre> <p>Your data file (<code>regression_data.jsonl</code>) should contain a numeric label: <pre><code>{\"text\": \"This was an amazing experience, 5 stars!\", \"label\": 5.0}\n{\"text\": \"It was okay, but not great.\", \"label\": 2.5}\n</code></pre></p> <p>That's it! You have successfully created, registered, and configured a new custom task in Jenga-AI.</p>"},{"location":"guides/configuration/","title":"How-to: Configure an Experiment","text":"<p>The <code>experiment.yaml</code> file is the single source of truth for any Jenga-AI experiment. It provides a clear and reproducible way to define the model, data, and training parameters. This guide provides a detailed breakdown of all available configuration options.</p>"},{"location":"guides/configuration/#top-level-structure","title":"Top-Level Structure","text":"<p>Your <code>experiment.yaml</code> file is organized into four main sections:</p> <pre><code>project_name: \"My_Awesome_Project\"\n\nmodel:\n  # ... model configuration ...\n\ntokenizer:\n  # ... tokenizer configuration ...\n\ntraining:\n  # ... training configuration ...\n\ntasks:\n  # ... a list of task configurations ...\n</code></pre>"},{"location":"guides/configuration/#model","title":"<code>model</code>","text":"<p>This section defines the architecture of your <code>MultiTaskModel</code>.</p> <pre><code>model:\n  base_model: \"distilbert-base-uncased\"\n  dropout: 0.1\n  fusion:\n    type: \"attention\"\n    hidden_size: 768\n</code></pre> Key Type Description Required <code>base_model</code> string The name of a pre-trained model from the Hugging Face Hub. Examples: <code>bert-base-cased</code>, <code>xlm-roberta-base</code>. Yes <code>dropout</code> float The dropout probability for the model's classification heads. No (defaults to <code>0.1</code>) <code>fusion</code> object Optional configuration for the <code>AttentionFusion</code> layer. Use this to create task-specific representations. No"},{"location":"guides/configuration/#fusion","title":"<code>fusion</code>","text":"<p>If you include the <code>fusion</code> block, you can configure the attention fusion layer.</p> Key Type Description Required <code>type</code> string The type of fusion. Currently, only <code>\"attention\"</code> is supported. Yes <code>hidden_size</code> int The hidden size of the fusion layer. Must match the hidden size of the <code>base_model</code>. Yes"},{"location":"guides/configuration/#tokenizer","title":"<code>tokenizer</code>","text":"<p>This section controls the behavior of the Hugging Face <code>AutoTokenizer</code>.</p> <pre><code>tokenizer:\n  max_length: 256\n  padding: \"max_length\"\n  truncation: true\n</code></pre> Key Type Description Required <code>max_length</code> int The maximum sequence length. Texts longer than this will be truncated. Yes <code>padding</code> string The padding strategy. <code>\"max_length\"</code> pads all sequences to <code>max_length</code>. No (defaults to <code>\"max_length\"</code>) <code>truncation</code> bool Whether to truncate sequences. Should generally be <code>true</code>. No (defaults to <code>true</code>)"},{"location":"guides/configuration/#training","title":"<code>training</code>","text":"<p>This section contains all hyperparameters and settings for the <code>Trainer</code>.</p> <pre><code>training:\n  output_dir: \"./results\"\n  learning_rate: 2.0e-5\n  batch_size: 16\n  num_epochs: 5\n  weight_decay: 0.01\n  warmup_steps: 100\n  eval_strategy: \"epoch\"\n  save_strategy: \"epoch\"\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n  early_stopping_patience: 3\n  logging:\n    service: \"mlflow\"\n    experiment_name: \"My_Experiment\"\n</code></pre> Key Type Description Required <code>output_dir</code> string Path to the directory where results, logs, and checkpoints will be saved. Yes <code>learning_rate</code> float The initial learning rate for the AdamW optimizer. Yes <code>batch_size</code> int The number of samples per batch for both training and evaluation. Yes <code>num_epochs</code> int The total number of training epochs to perform. Yes <code>weight_decay</code> float The weight decay to apply (if not zero). No (defaults to <code>0.0</code>) <code>warmup_steps</code> int Number of steps for the linear warmup of the learning rate. No (defaults to <code>0</code>) <code>eval_strategy</code> string When to run evaluation. <code>\"epoch\"</code> or <code>\"steps\"</code>. If <code>\"steps\"</code>, you must also set <code>eval_steps</code>. No (defaults to <code>\"epoch\"</code>) <code>save_strategy</code> string When to save model checkpoints. <code>\"epoch\"</code> or <code>\"steps\"</code>. If <code>\"steps\"</code>, you must also set <code>save_steps</code>. No (defaults to <code>\"epoch\"</code>) <code>load_best_model_at_end</code> bool If <code>true</code>, the trainer will load the best model (based on <code>metric_for_best_model</code>) at the end of training. No (defaults to <code>false</code>) <code>metric_for_best_model</code> string The metric used to identify the best model. Example: <code>eval_loss</code>, <code>eval_f1</code>. Required if <code>load_best_model_at_end</code> is <code>true</code>. <code>greater_is_better</code> bool Set to <code>true</code> if a higher value of <code>metric_for_best_model</code> is better (e.g., F1 score), and <code>false</code> if a lower value is better (e.g., loss). Required if <code>load_best_model_at_end</code> is <code>true</code>. <code>early_stopping_patience</code> int Number of evaluations with no improvement after which training will be stopped. No <code>logging</code> object Configuration for experiment tracking. Yes"},{"location":"guides/configuration/#logging","title":"<code>logging</code>","text":"Key Type Description Required <code>service</code> string The logging service to use. Supported: <code>\"mlflow\"</code>, <code>\"tensorboard\"</code>. Yes <code>experiment_name</code> string The name for the experiment run in MLflow or TensorBoard. Yes <code>tracking_uri</code> string Optional. The URI for a remote MLflow tracking server. No"},{"location":"guides/configuration/#tasks","title":"<code>tasks</code>","text":"<p>This is a list of one or more task objects. Each object defines a task for the model to learn.</p> <pre><code>tasks:\n  - name: \"Sentiment\"\n    type: \"single_label_classification\"\n    data_path: \"/path/to/sentiment_data.jsonl\"\n    heads:\n      - name: \"sentiment_head\"\n        num_labels: 2\n        weight: 1.0\n        label_map:\n          \"Positive\": 0\n          \"Negative\": 1\n</code></pre> Key Type Description Required <code>name</code> string A unique name for the task (e.g., \"Sentiment\", \"ThreatDetection\"). Yes <code>type</code> string The type of task. Supported types are: <code>\"single_label_classification\"</code>, <code>\"multi_label_classification\"</code>, <code>\"ner\"</code>. Yes <code>data_path</code> string The absolute path to the <code>.jsonl</code> data file for this task. Yes <code>heads</code> list A list of one or more prediction heads for this task. Yes"},{"location":"guides/configuration/#heads","title":"<code>heads</code>","text":"<p>Each task has at least one prediction head.</p> Key Type Description Required <code>name</code> string A unique name for the prediction head. Yes <code>num_labels</code> int The number of output labels for this head. For NER, this is the number of entity types. For classification, the number of classes. This value may be automatically updated by the <code>DataProcessor</code>. Yes <code>weight</code> float The weight of this head's loss in the total loss calculation for the task. Allows you to prioritize certain heads. No (defaults to <code>1.0</code>) <code>label_map</code> dict Optional. A dictionary mapping string labels from your data file to integer IDs. If not provided, the <code>DataProcessor</code> will create this map automatically. No"},{"location":"guides/deployment/","title":"How-to: Deploy a Model","text":"<p>A trained model is only useful if it can be deployed to make predictions on new data. This guide outlines the vision for Jenga-AI's deployment and inference tools.</p> <p>Under Development</p> <p>The deployment and inference tools described here are part of our future roadmap and are not yet implemented. You can track the progress of this feature in the <code>PROJECT_ROADMAP.md</code> file under Milestone 2.</p>"},{"location":"guides/deployment/#the-vision-from-training-to-api-in-minutes","title":"The Vision: From Training to API in Minutes","text":"<p>Our goal is to make the transition from a trained model to a production-ready API as seamless as possible. The planned workflow will look like this:</p>"},{"location":"guides/deployment/#1-export-the-model","title":"1. Export the Model","text":"<p>After training is complete, you will be able to run an export script that packages your <code>MultiTaskModel</code> and all its required components into a single, portable artifact.</p> <p>This artifact will contain: - The trained model weights. - The <code>ExperimentConfig</code> file, containing all metadata about the tasks and heads. - The Hugging Face <code>Tokenizer</code> files.</p> <p>Planned Usage: <pre><code>python -m multitask_bert.deployment.export \\\n    --checkpoint_dir ./tutorial_results/checkpoint-123 \\\n    --output_dir ./exported_models/my_multitask_model\n</code></pre></p>"},{"location":"guides/deployment/#2-run-inference-with-a-unified-wrapper","title":"2. Run Inference with a Unified Wrapper","text":"<p>Once exported, you will be able to load the model artifact using a simple <code>InferenceWrapper</code> class. This class will provide a clean interface to make predictions for any of the tasks the model was trained on.</p> <p>Planned Usage: <pre><code>from multitask_bert.deployment import InferenceWrapper\n\n# Load the exported model artifact\nmodel = InferenceWrapper(model_path=\"./exported_models/my_multitask_model\")\n\n# Predict sentiment for a piece of text\nsentiment_result = model.predict(\n    text=\"Jenga-AI is an incredibly useful framework!\",\n    task_name=\"Sentiment\"\n)\n# Expected output: {'sentiment_head': {'label': 'Positive', 'score': 0.98}}\n\n# Predict named entities from another piece of text\nner_result = model.predict(\n    text=\"John Doe is traveling to Nairobi.\",\n    task_name=\"SwahiliNER\"\n)\n# Expected output: {'ner_head': [{'entity': 'PERSON', 'word': 'John Doe'}, {'entity': 'LOCATION', 'word': 'Nairobi'}]}\n</code></pre></p>"},{"location":"guides/deployment/#3-deploy-as-a-rest-api","title":"3. Deploy as a REST API","text":"<p>The final step is to serve the model as a REST API. We plan to provide a pre-built FastAPI application that can be launched with a single command.</p> <p>Planned Usage: <pre><code># This command will start a Uvicorn server with the FastAPI app\npython -m api.main --model_path ./exported_models/my_multitask_model\n</code></pre></p> <p>You would then be able to send <code>POST</code> requests to the API to get predictions in real-time:</p> <pre><code>curl -X POST http://127.0.0.1:8000/predict \\\n-H \"Content-Type: application/json\" \\\n-d \n'{\n    \"text\": \"The system is under attack, we need to respond now!\",\n    \"task_name\": \"Threat\"\n}'\n</code></pre>"},{"location":"guides/deployment/#how-to-contribute","title":"How to Contribute","text":"<p>This is a critical part of our roadmap, and we welcome contributions from the community. If you are interested in helping build out these features, please check out the issues under Milestone 2 in our <code>PROJECT_ROADMAP.md</code> file on GitHub.</p>"},{"location":"tutorials/first_multitask_model/","title":"Tutorial: Training Your First Multi-Task Model","text":"<p>In this tutorial, we will take a deep dive into the Jenga-AI framework by training a single model on two distinct NLP tasks: 1.  Sentiment Analysis: A single-label classification task. 2.  Named Entity Recognition (NER): A token-level classification task.</p> <p>This guide will walk you through preparing the data, writing a comprehensive configuration file, and running the training script.</p>"},{"location":"tutorials/first_multitask_model/#1-project-setup","title":"1. Project Setup","text":"<p>First, create a new project directory with the following structure. We will use the dummy data files that come with the Jenga-AI repository.</p> <pre><code>jenga-tutorial/\n\u251c\u2500\u2500 config.yaml\n\u2514\u2500\u2500 train.py\n</code></pre> <p>For this tutorial, we will reference the dummy data files located in the <code>examples/</code> directory of the Jenga-AI repository: - <code>examples/sentiment_data.jsonl</code> - <code>examples/ner_data.jsonl</code></p> <p>Make sure you know the full path to these files on your system.</p>"},{"location":"tutorials/first_multitask_model/#2-understanding-the-data","title":"2. Understanding the Data","text":"<p>Let's look at the format of our two data files.</p> <p><code>sentiment_data.jsonl</code>: Each line is a JSON object with a <code>text</code> and a <code>label</code>. <pre><code>{\"text\": \"I love this framework, it's so easy to use!\", \"label\": \"Positive\"}\n{\"text\": \"This is the worst product I have ever used.\", \"label\": \"Negative\"}\n</code></pre></p> <p><code>ner_data.jsonl</code>: Each line contains <code>text</code> and a list of <code>entities</code>, where each entity has a <code>label</code> and its <code>start</code> and <code>end</code> character indices. <pre><code>{\"text\": \"John Doe visited Nairobi last week.\", \"entities\": [{\"label\": \"PERSON\", \"start\": 0, \"end\": 8}, {\"label\": \"LOCATION\", \"start\": 17, \"end\": 24}]}\n</code></pre></p>"},{"location":"tutorials/first_multitask_model/#3-the-experiment-configuration-file","title":"3. The Experiment Configuration File","text":"<p>The <code>config.yaml</code> file is the heart of your experiment. It defines every aspect of the training process. Let's create a detailed configuration.</p> <p>Create the <code>config.yaml</code> file: <pre><code># config.yaml\n\nproject_name: \"JengaAI_Tutorial\"\n\n# ---------------------------------\n# MODEL CONFIGURATION\n# Defines the core model architecture.\n# ---------------------------------\nmodel:\n  base_model: \"distilbert-base-uncased\"\n  dropout: 0.1\n  # We can optionally add an attention fusion layer here\n  # fusion:\n  #   type: \"attention\"\n  #   hidden_size: 768\n\n# ---------------------------------\n# TOKENIZER CONFIGURATION\n# Settings for the Hugging Face tokenizer.\n# ---------------------------------\ntokenizer:\n  max_length: 256\n  padding: \"max_length\"\n  truncation: true\n\n# ---------------------------------\n# TRAINING CONFIGURATION\n# All hyperparameters and settings for the training loop.\n# ---------------------------------\ntraining:\n  output_dir: \"./tutorial_results\"\n  learning_rate: 2.0e-5\n  batch_size: 8\n  num_epochs: 3\n  weight_decay: 0.01\n  warmup_steps: 100\n  eval_strategy: \"epoch\"      # Evaluate at the end of each epoch\n  save_strategy: \"epoch\"      # Save a checkpoint at the end of each epoch\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_loss\" # Use evaluation loss to find the best model\n  greater_is_better: false    # Lower loss is better\n  early_stopping_patience: 2  # Stop if eval_loss doesn't improve for 2 epochs\n  logging:\n    service: \"mlflow\"         # Use MLflow for experiment tracking\n    experiment_name: \"JengaAI_Tutorial_MVP\"\n    # tracking_uri: \"http://localhost:5000\" # Optional: for a remote MLflow server\n\n# ---------------------------------\n# TASK DEFINITIONS\n# A list of all tasks the model should learn.\n# ---------------------------------\ntasks:\n  # --- Task 1: Sentiment Analysis ---\n  - name: \"Sentiment\"\n    type: \"single_label_classification\"\n    # IMPORTANT: Use the full path to your data file\n    data_path: \"/path/to/Jenga-AI/examples/sentiment_data.jsonl\"\n    heads:\n      - name: \"sentiment_head\"\n        num_labels: 2 # We will map labels automatically in the DataProcessor\n        weight: 1.0   # The weight of this task's loss in the total loss\n\n  # --- Task 2: Named Entity Recognition ---\n  - name: \"SwahiliNER\"\n    type: \"ner\"\n    # IMPORTANT: Use the full path to your data file\n    data_path: \"/path/to/Jenga-AI/examples/ner_data.jsonl\"\n    heads:\n      - name: \"ner_head\"\n        num_labels: 13 # Placeholder, will be updated by the DataProcessor\n        weight: 1.5    # Give the NER task slightly more weight\n</code></pre> Important: Remember to replace <code>/path/to/Jenga-AI/</code> with the actual absolute path to the project directory on your machine.</p>"},{"location":"tutorials/first_multitask_model/#4-the-training-script","title":"4. The Training Script","text":"<p>Now, let's create the <code>train.py</code> script. This script is a more robust version of the one from the Quickstart guide and is based on the <code>run_experiment.py</code> file in the <code>examples</code> directory.</p> <p>Create the <code>train.py</code> file: <pre><code># train.py\nimport argparse\nimport dataclasses\nimport os\nimport yaml\n\nfrom multitask_bert.core.config import load_experiment_config\nfrom multitask_bert.data.data_processing import DataProcessor\nfrom multitask_bert.training.trainer import Trainer\n\ndef main(config_path: str):\n    \"\"\"\n    Main function to run a multi-task experiment from a config file.\n    \"\"\"\n    # 1. Load Config from the YAML file\n    print(\"Loading experiment configuration...\")\n    config = load_experiment_config(config_path)\n\n    # 2. Process Data for all defined tasks\n    # The DataProcessor will read the config, find the data files,\n    # and prepare them for training and evaluation. It also automatically\n    # determines the label maps for you.\n    print(\"Processing data for all tasks...\")\n    data_processor = DataProcessor(config)\n    train_datasets, eval_datasets, updated_config = data_processor.process_data()\n    config = updated_config # The config is updated with the correct number of labels\n\n    # 3. Instantiate the Trainer\n    # The Trainer class is the main orchestrator. It will automatically\n    # build the MultiTaskModel, optimizer, and scheduler based on the config.\n    print(\"Instantiating trainer...\")\n    trainer = Trainer(\n        config=config,\n        train_datasets=train_datasets,\n        eval_datasets=eval_datasets\n    )\n\n    # 4. Start Training\n    print(\"\ud83d\udd25 Starting training...\")\n    try:\n        trainer.train()\n        print(\"\ud83c\udf89 Training complete.\")\n\n        # 5. Final Evaluation on the test set\n        print(\"Running final evaluation...\")\n        final_metrics = trainer.evaluate(test=True) # Use test=True for final eval\n        print(\"Final evaluation metrics:\")\n        print(final_metrics)\n\n    finally:\n        # 6. Close the logger to ensure all data is saved\n        trainer.close()\n        print(\"Logger closed.\")\n\n    # 7. Save the final, updated config\n    output_config_path = os.path.join(config.training.output_dir, \"experiment_config.yaml\")\n    with open(output_config_path, 'w') as f:\n        yaml.dump(dataclasses.asdict(config), f, indent=2)\n    print(f\"\u2705 Final experiment config saved to: {output_config_path}\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run the Jenga-AI tutorial experiment.\")\n    parser.add_argument(\n        \"--config\",\n        type=str,\n        default=\"config.yaml\",\n        help=\"Path to the experiment YAML file.\"\n    )\n    args = parser.parse_args()\n    main(args.config)\n</code></pre></p>"},{"location":"tutorials/first_multitask_model/#5-run-the-experiment","title":"5. Run the Experiment","text":"<p>You are now ready to train your multi-task model. From your terminal, inside the <code>jenga-tutorial</code> directory, run the following command:</p> <pre><code>python train.py --config config.yaml\n</code></pre> <p>The script will: 1.  Load your <code>config.yaml</code>. 2.  Process both <code>sentiment_data.jsonl</code> and <code>ner_data.jsonl</code>. 3.  Build a <code>distilbert-base-uncased</code> model with two separate \"heads\" (one for sentiment, one for NER). 4.  Train the model, alternating between batches of sentiment and NER data. 5.  Evaluate the model's performance on each task at the end of every epoch. 6.  Save the best model, logs, and results into the <code>tutorial_results</code> directory.</p> <p>Congratulations! You have successfully trained a multi-task model using Jenga-AI. You can now explore the output directory and analyze the results in MLflow by running <code>mlflow ui</code>.</p>"},{"location":"tutorials/ner_model/","title":"Tutorial: Training and Using an NER Model","text":"<p>Named Entity Recognition (NER) is a common NLP task that involves identifying and classifying named entities in text (e.g., persons, organizations, locations).</p> <p>This tutorial will guide you through training a powerful NER model using Jenga-AI's streamlined workflow, and then using that model for inference.</p>"},{"location":"tutorials/ner_model/#1-the-ner-configuration-file","title":"1. The NER Configuration File","text":"<p>We will use the <code>experiment_ner.yaml</code> file provided in the <code>examples/</code> directory. This file is configured specifically for an NER task.</p> <p><code>examples/experiment_ner.yaml</code>: <pre><code>project_name: \"JengaAI_NER_Experiment\"\n\nmodel:\n  base_model: \"distilbert-base-uncased\"\n  dropout: 0.1\n\ntokenizer:\n  max_length: 512\n  padding: \"max_length\"\n  truncation: true\n\ntraining:\n  output_dir: \"./unified_results_ner\"\n  learning_rate: 2.0e-5\n  batch_size: 8\n  num_epochs: 5\n  weight_decay: 0.01\n  warmup_steps: 100\n  eval_strategy: \"epoch\"\n  save_strategy: \"epoch\"\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_SwahiliNER_loss\" \n  greater_is_better: false\n  logging:\n    service: \"mlflow\"\n    experiment_name: \"JengaAI_NER\"\n\ntasks:\n  - name: \"SwahiliNER\"\n    type: \"ner\"\n    # This path is relative to the project root\n    data_path: \"examples/ner_synthetic_dataset_v1.jsonl\"\n    heads:\n      - name: \"ner_head\"\n        # The DataProcessor will automatically determine the correct number of labels\n        num_labels: 13 \n        weight: 1.0\n</code></pre></p>"},{"location":"tutorials/ner_model/#key-differences-for-ner","title":"Key Differences for NER:","text":"<ul> <li><code>type: \"ner\"</code>: This tells the framework to use the specialized <code>NERTask</code> class, which applies classification at the token level.</li> <li><code>data_path</code>: The data file (<code>ner_synthetic_dataset_v1.jsonl</code>) contains text and a list of <code>entities</code> with character offsets, which the <code>DataProcessor</code> knows how to handle for NER tasks.</li> </ul>"},{"location":"tutorials/ner_model/#2-train-the-ner-model","title":"2. Train the NER Model","text":"<p>Thanks to the refactored <code>run_experiment.py</code> script, training the model is simple and clean. The script uses the <code>Trainer.from_config</code> method to handle all the setup boilerplate.</p> <p>To run the training, execute the following command from the root of the Jenga-AI project:</p> <pre><code>python examples/run_experiment.py --config examples/experiment_ner.yaml\n</code></pre> <p>This command will: 1.  Instantiate the <code>Trainer</code>, which automatically sets up the tokenizer, data processor, and <code>MultiTaskModel</code>. 2.  Process the NER data, aligning entity labels with tokens. 3.  Train the model for 5 epochs, saving the best-performing checkpoint based on evaluation loss. 4.  Save the final model, tokenizer, and configuration to the <code>./unified_results_ner/best_model/</code> directory.</p>"},{"location":"tutorials/ner_model/#3-run-inference-with-the-trained-model","title":"3. Run Inference with the Trained Model","text":"<p>After training, you can use the <code>examples/ner_iinference.py</code> script to load your trained model and make predictions on new text.</p> <p>This script is a self-contained example that shows all the steps required for inference: - Loading the saved experiment configuration. - Re-instantiating the model architecture. - Loading the saved model weights. - Tokenizing new text and running it through the model. - Post-processing the model's output to extract named entities.</p> <p><code>examples/ner_iinference.py</code>: <pre><code>import torch\nimport yaml\nimport os\n# ... other imports\n\nclass NERInference:\n    def __init__(self, model_dir: str):\n        # ... loads config, tokenizer, and model ...\n\n    def _load_model(self):\n        # ... logic to instantiate model and load weights ...\n\n    def predict_entities(self, text: str):\n        # ... preprocesses text, runs model, and post-processes logits ...\n\n    def _extract_entities(self, ...):\n        # ... logic to convert token predictions into entities ...\n\ndef main():\n    model_dir = \"./unified_results_ner\"\n    ner_inference = NERInference(model_dir)\n\n    test_texts = [\n        \"Hello, I'm Vincent from Dar es Salaam.\",\n        \"Mwangi Kennedy was taken to the Hospital in Nairobi.\",\n    ]\n\n    for text in test_texts:\n        print(f\"\\n\ud83d\udcdd Text: \\\"{text}\\\"\")\n        entities = ner_inference.predict_entities(text)\n        # ... prints entities ...\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"tutorials/ner_model/#how-it-works","title":"How it Works","text":"<p>The script defines an <code>NERInference</code> class that encapsulates all the logic needed to go from a saved model directory to predicted entities. While this approach is more verbose than the planned <code>InferenceWrapper</code>, it clearly shows all the steps involved in the process.</p> <p>To run the inference script, execute the following command from the project root:</p> <pre><code>python examples/ner_iinference.py\n</code></pre>"},{"location":"tutorials/ner_model/#expected-output","title":"Expected Output:","text":"<pre><code>Loading trained model...\nUsing device: cpu\nLoading config from: ./unified_results_ner/experiment_config.yaml\nLoading model from: ./unified_results_ner/best_model\n...\n\ud83d\udcdd Text: \"Hello, I'm Vincent from Dar es Salaam.\"\n\ud83d\udd0d Entities found:\n   \u2022 'vincent' \u2192 PERSON (chars 13-20)\n   \u2022 'dar es salaam' \u2192 LOCATION (chars 26-39)\n\n\ud83d\udcdd Text: \"Mwangi Kennedy was taken to the Hospital in Nairobi.\"\n\ud83d\udd0d Entities found:\n   \u2022 'mwangi kennedy' \u2192 PERSON (chars 0-14)\n   \u2022 'hospital' \u2192 ORG (chars 35-43)\n   \u2022 'nairobi' \u2192 LOCATION (chars 47-54)\n</code></pre> <p>This demonstrates a complete, end-to-end workflow for training and using an NER model within the Jenga-AI framework.</p>"}]}