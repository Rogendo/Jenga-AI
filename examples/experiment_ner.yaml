# Updated config.yaml
project_name: "JengaAI_NER_Experiment"

model:
  base_model: "distilbert-base-uncased"
  dropout: 0.1

tokenizer:
  max_length: 512  # Reduced from 512 for stability
  padding: "max_length"
  truncation: true

training:
  output_dir: "./unified_results_ner"
  learning_rate: 2.0e-5  # Use standard BERT learning rate
  batch_size: 8
  num_epochs: 5
  weight_decay: 0.01
  warmup_steps: 100
  eval_strategy: "epoch"
  save_strategy: "epoch"
  load_best_model_at_end: true
  metric_for_best_model: "eval_SwahiliNER_loss" 
  greater_is_better: false
  early_stopping_patience: 3
  logging:
    service: "mlflow"
    experiment_name: "JengaAI_NER"

tasks:
  - name: "SwahiliNER"
    type: "ner"
    data_path: "examples/ner_synthetic_dataset_v1.jsonl"
    # data_path: "/Users/naynek/Desktop/MultiClassifier/Jenga-AI/examples/ner_data.jsonl"
    heads:
      - name: "ner_head"
        num_labels: 13 # Will be updated during processing
        weight: 1.0