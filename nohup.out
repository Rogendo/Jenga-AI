Loading experiment configuration...
Loading tokenizer: distilbert-base-uncased
Processing data for all tasks...
Processing data for task: SwahiliNER
Map:   0%|          | 0/2000 [00:00<?, ? examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1000/2000 [00:04<00:04, 207.37 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:09<00:00, 222.42 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:09<00:00, 218.46 examples/s]
Map:   0%|          | 0/2000 [00:00<?, ? examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1000/2000 [00:01<00:01, 714.23 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 742.55 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 730.44 examples/s]
Some weights of MultiTaskModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.embeddings.LayerNorm.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.embeddings.word_embeddings.weight', 'encoder.transformer.layer.0.attention.k_lin.bias', 'encoder.transformer.layer.0.attention.k_lin.weight', 'encoder.transformer.layer.0.attention.out_lin.bias', 'encoder.transformer.layer.0.attention.out_lin.weight', 'encoder.transformer.layer.0.attention.q_lin.bias', 'encoder.transformer.layer.0.attention.q_lin.weight', 'encoder.transformer.layer.0.attention.v_lin.bias', 'encoder.transformer.layer.0.attention.v_lin.weight', 'encoder.transformer.layer.0.ffn.lin1.bias', 'encoder.transformer.layer.0.ffn.lin1.weight', 'encoder.transformer.layer.0.ffn.lin2.bias', 'encoder.transformer.layer.0.ffn.lin2.weight', 'encoder.transformer.layer.0.output_layer_norm.bias', 'encoder.transformer.layer.0.output_layer_norm.weight', 'encoder.transformer.layer.0.sa_layer_norm.bias', 'encoder.transformer.layer.0.sa_layer_norm.weight', 'encoder.transformer.layer.1.attention.k_lin.bias', 'encoder.transformer.layer.1.attention.k_lin.weight', 'encoder.transformer.layer.1.attention.out_lin.bias', 'encoder.transformer.layer.1.attention.out_lin.weight', 'encoder.transformer.layer.1.attention.q_lin.bias', 'encoder.transformer.layer.1.attention.q_lin.weight', 'encoder.transformer.layer.1.attention.v_lin.bias', 'encoder.transformer.layer.1.attention.v_lin.weight', 'encoder.transformer.layer.1.ffn.lin1.bias', 'encoder.transformer.layer.1.ffn.lin1.weight', 'encoder.transformer.layer.1.ffn.lin2.bias', 'encoder.transformer.layer.1.ffn.lin2.weight', 'encoder.transformer.layer.1.output_layer_norm.bias', 'encoder.transformer.layer.1.output_layer_norm.weight', 'encoder.transformer.layer.1.sa_layer_norm.bias', 'encoder.transformer.layer.1.sa_layer_norm.weight', 'encoder.transformer.layer.2.attention.k_lin.bias', 'encoder.transformer.layer.2.attention.k_lin.weight', 'encoder.transformer.layer.2.attention.out_lin.bias', 'encoder.transformer.layer.2.attention.out_lin.weight', 'encoder.transformer.layer.2.attention.q_lin.bias', 'encoder.transformer.layer.2.attention.q_lin.weight', 'encoder.transformer.layer.2.attention.v_lin.bias', 'encoder.transformer.layer.2.attention.v_lin.weight', 'encoder.transformer.layer.2.ffn.lin1.bias', 'encoder.transformer.layer.2.ffn.lin1.weight', 'encoder.transformer.layer.2.ffn.lin2.bias', 'encoder.transformer.layer.2.ffn.lin2.weight', 'encoder.transformer.layer.2.output_layer_norm.bias', 'encoder.transformer.layer.2.output_layer_norm.weight', 'encoder.transformer.layer.2.sa_layer_norm.bias', 'encoder.transformer.layer.2.sa_layer_norm.weight', 'encoder.transformer.layer.3.attention.k_lin.bias', 'encoder.transformer.layer.3.attention.k_lin.weight', 'encoder.transformer.layer.3.attention.out_lin.bias', 'encoder.transformer.layer.3.attention.out_lin.weight', 'encoder.transformer.layer.3.attention.q_lin.bias', 'encoder.transformer.layer.3.attention.q_lin.weight', 'encoder.transformer.layer.3.attention.v_lin.bias', 'encoder.transformer.layer.3.attention.v_lin.weight', 'encoder.transformer.layer.3.ffn.lin1.bias', 'encoder.transformer.layer.3.ffn.lin1.weight', 'encoder.transformer.layer.3.ffn.lin2.bias', 'encoder.transformer.layer.3.ffn.lin2.weight', 'encoder.transformer.layer.3.output_layer_norm.bias', 'encoder.transformer.layer.3.output_layer_norm.weight', 'encoder.transformer.layer.3.sa_layer_norm.bias', 'encoder.transformer.layer.3.sa_layer_norm.weight', 'encoder.transformer.layer.4.attention.k_lin.bias', 'encoder.transformer.layer.4.attention.k_lin.weight', 'encoder.transformer.layer.4.attention.out_lin.bias', 'encoder.transformer.layer.4.attention.out_lin.weight', 'encoder.transformer.layer.4.attention.q_lin.bias', 'encoder.transformer.layer.4.attention.q_lin.weight', 'encoder.transformer.layer.4.attention.v_lin.bias', 'encoder.transformer.layer.4.attention.v_lin.weight', 'encoder.transformer.layer.4.ffn.lin1.bias', 'encoder.transformer.layer.4.ffn.lin1.weight', 'encoder.transformer.layer.4.ffn.lin2.bias', 'encoder.transformer.layer.4.ffn.lin2.weight', 'encoder.transformer.layer.4.output_layer_norm.bias', 'encoder.transformer.layer.4.output_layer_norm.weight', 'encoder.transformer.layer.4.sa_layer_norm.bias', 'encoder.transformer.layer.4.sa_layer_norm.weight', 'encoder.transformer.layer.5.attention.k_lin.bias', 'encoder.transformer.layer.5.attention.k_lin.weight', 'encoder.transformer.layer.5.attention.out_lin.bias', 'encoder.transformer.layer.5.attention.out_lin.weight', 'encoder.transformer.layer.5.attention.q_lin.bias', 'encoder.transformer.layer.5.attention.q_lin.weight', 'encoder.transformer.layer.5.attention.v_lin.bias', 'encoder.transformer.layer.5.attention.v_lin.weight', 'encoder.transformer.layer.5.ffn.lin1.bias', 'encoder.transformer.layer.5.ffn.lin1.weight', 'encoder.transformer.layer.5.ffn.lin2.bias', 'encoder.transformer.layer.5.ffn.lin2.weight', 'encoder.transformer.layer.5.output_layer_norm.bias', 'encoder.transformer.layer.5.output_layer_norm.weight', 'encoder.transformer.layer.5.sa_layer_norm.bias', 'encoder.transformer.layer.5.sa_layer_norm.weight', 'tasks.0.heads.ner_head.bias', 'tasks.0.heads.ner_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534
  return FileStore(store_uri, store_uri)
Instantiating tasks and model...
Instantiating trainer...
MLflow logger initialized. Experiment: 'JengaAI_NER'
Starting training...
Training:   0%|          | 0/1000 [00:00<?, ?it/s]/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 108, in <module>
    main(args.config)
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 78, in main
    trainer.train()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/training/trainer.py", line 168, in train
    outputs = self.model(
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/core/model.py", line 81, in forward
    task_output = task.get_forward_output(
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/tasks/ner.py", line 44, in get_forward_output
    loss = loss_fct(active_logits, active_labels)
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
 (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:113.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
mlflow logger closed.
Traceback (most recent call last):
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 108, in <module>
    main(args.config)
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 78, in main
    trainer.train()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/training/trainer.py", line 176, in train
    loss.backward()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output.
Training:   0%|          | 0/1000 [00:08<?, ?it/s]
Loading experiment configuration...
Loading tokenizer: distilbert-base-uncased
Processing data for all tasks...
Processing data for task: SwahiliNER
Map:   0%|          | 0/2000 [00:00<?, ? examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1000/2000 [00:03<00:03, 253.30 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:07<00:00, 275.55 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:07<00:00, 270.31 examples/s]
Map:   0%|          | 0/2000 [00:00<?, ? examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1000/2000 [00:00<00:00, 1182.81 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1147.53 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1144.25 examples/s]
Some weights of MultiTaskModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.embeddings.LayerNorm.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.embeddings.word_embeddings.weight', 'encoder.transformer.layer.0.attention.k_lin.bias', 'encoder.transformer.layer.0.attention.k_lin.weight', 'encoder.transformer.layer.0.attention.out_lin.bias', 'encoder.transformer.layer.0.attention.out_lin.weight', 'encoder.transformer.layer.0.attention.q_lin.bias', 'encoder.transformer.layer.0.attention.q_lin.weight', 'encoder.transformer.layer.0.attention.v_lin.bias', 'encoder.transformer.layer.0.attention.v_lin.weight', 'encoder.transformer.layer.0.ffn.lin1.bias', 'encoder.transformer.layer.0.ffn.lin1.weight', 'encoder.transformer.layer.0.ffn.lin2.bias', 'encoder.transformer.layer.0.ffn.lin2.weight', 'encoder.transformer.layer.0.output_layer_norm.bias', 'encoder.transformer.layer.0.output_layer_norm.weight', 'encoder.transformer.layer.0.sa_layer_norm.bias', 'encoder.transformer.layer.0.sa_layer_norm.weight', 'encoder.transformer.layer.1.attention.k_lin.bias', 'encoder.transformer.layer.1.attention.k_lin.weight', 'encoder.transformer.layer.1.attention.out_lin.bias', 'encoder.transformer.layer.1.attention.out_lin.weight', 'encoder.transformer.layer.1.attention.q_lin.bias', 'encoder.transformer.layer.1.attention.q_lin.weight', 'encoder.transformer.layer.1.attention.v_lin.bias', 'encoder.transformer.layer.1.attention.v_lin.weight', 'encoder.transformer.layer.1.ffn.lin1.bias', 'encoder.transformer.layer.1.ffn.lin1.weight', 'encoder.transformer.layer.1.ffn.lin2.bias', 'encoder.transformer.layer.1.ffn.lin2.weight', 'encoder.transformer.layer.1.output_layer_norm.bias', 'encoder.transformer.layer.1.output_layer_norm.weight', 'encoder.transformer.layer.1.sa_layer_norm.bias', 'encoder.transformer.layer.1.sa_layer_norm.weight', 'encoder.transformer.layer.2.attention.k_lin.bias', 'encoder.transformer.layer.2.attention.k_lin.weight', 'encoder.transformer.layer.2.attention.out_lin.bias', 'encoder.transformer.layer.2.attention.out_lin.weight', 'encoder.transformer.layer.2.attention.q_lin.bias', 'encoder.transformer.layer.2.attention.q_lin.weight', 'encoder.transformer.layer.2.attention.v_lin.bias', 'encoder.transformer.layer.2.attention.v_lin.weight', 'encoder.transformer.layer.2.ffn.lin1.bias', 'encoder.transformer.layer.2.ffn.lin1.weight', 'encoder.transformer.layer.2.ffn.lin2.bias', 'encoder.transformer.layer.2.ffn.lin2.weight', 'encoder.transformer.layer.2.output_layer_norm.bias', 'encoder.transformer.layer.2.output_layer_norm.weight', 'encoder.transformer.layer.2.sa_layer_norm.bias', 'encoder.transformer.layer.2.sa_layer_norm.weight', 'encoder.transformer.layer.3.attention.k_lin.bias', 'encoder.transformer.layer.3.attention.k_lin.weight', 'encoder.transformer.layer.3.attention.out_lin.bias', 'encoder.transformer.layer.3.attention.out_lin.weight', 'encoder.transformer.layer.3.attention.q_lin.bias', 'encoder.transformer.layer.3.attention.q_lin.weight', 'encoder.transformer.layer.3.attention.v_lin.bias', 'encoder.transformer.layer.3.attention.v_lin.weight', 'encoder.transformer.layer.3.ffn.lin1.bias', 'encoder.transformer.layer.3.ffn.lin1.weight', 'encoder.transformer.layer.3.ffn.lin2.bias', 'encoder.transformer.layer.3.ffn.lin2.weight', 'encoder.transformer.layer.3.output_layer_norm.bias', 'encoder.transformer.layer.3.output_layer_norm.weight', 'encoder.transformer.layer.3.sa_layer_norm.bias', 'encoder.transformer.layer.3.sa_layer_norm.weight', 'encoder.transformer.layer.4.attention.k_lin.bias', 'encoder.transformer.layer.4.attention.k_lin.weight', 'encoder.transformer.layer.4.attention.out_lin.bias', 'encoder.transformer.layer.4.attention.out_lin.weight', 'encoder.transformer.layer.4.attention.q_lin.bias', 'encoder.transformer.layer.4.attention.q_lin.weight', 'encoder.transformer.layer.4.attention.v_lin.bias', 'encoder.transformer.layer.4.attention.v_lin.weight', 'encoder.transformer.layer.4.ffn.lin1.bias', 'encoder.transformer.layer.4.ffn.lin1.weight', 'encoder.transformer.layer.4.ffn.lin2.bias', 'encoder.transformer.layer.4.ffn.lin2.weight', 'encoder.transformer.layer.4.output_layer_norm.bias', 'encoder.transformer.layer.4.output_layer_norm.weight', 'encoder.transformer.layer.4.sa_layer_norm.bias', 'encoder.transformer.layer.4.sa_layer_norm.weight', 'encoder.transformer.layer.5.attention.k_lin.bias', 'encoder.transformer.layer.5.attention.k_lin.weight', 'encoder.transformer.layer.5.attention.out_lin.bias', 'encoder.transformer.layer.5.attention.out_lin.weight', 'encoder.transformer.layer.5.attention.q_lin.bias', 'encoder.transformer.layer.5.attention.q_lin.weight', 'encoder.transformer.layer.5.attention.v_lin.bias', 'encoder.transformer.layer.5.attention.v_lin.weight', 'encoder.transformer.layer.5.ffn.lin1.bias', 'encoder.transformer.layer.5.ffn.lin1.weight', 'encoder.transformer.layer.5.ffn.lin2.bias', 'encoder.transformer.layer.5.ffn.lin2.weight', 'encoder.transformer.layer.5.output_layer_norm.bias', 'encoder.transformer.layer.5.output_layer_norm.weight', 'encoder.transformer.layer.5.sa_layer_norm.bias', 'encoder.transformer.layer.5.sa_layer_norm.weight', 'tasks.0.heads.ner_head.bias', 'tasks.0.heads.ner_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534
  return FileStore(store_uri, store_uri)
Instantiating tasks and model...
Instantiating trainer...
MLflow logger initialized. Experiment: 'JengaAI_NER'
Starting training...
Training:   0%|          | 0/1000 [00:00<?, ?it/s]Training:   0%|          | 1/1000 [00:33<9:14:30, 33.30s/it]Training:   0%|          | 1/1000 [00:33<9:14:30, 33.30s/it, loss=nan]Training:   0%|          | 2/1000 [00:57<7:45:09, 27.97s/it, loss=nan]Training:   0%|          | 2/1000 [00:57<7:45:09, 27.97s/it, loss=nan]Training:   0%|          | 3/1000 [01:41<9:44:22, 35.17s/it, loss=nan]Training:   0%|          | 3/1000 [01:41<9:44:22, 35.17s/it, loss=nan]Training:   0%|          | 4/1000 [01:57<7:40:21, 27.73s/it, loss=nan]Training:   0%|          | 4/1000 [01:57<7:40:21, 27.73s/it, loss=nan]Training:   0%|          | 5/1000 [02:13<6:27:57, 23.39s/it, loss=nan]Training:   0%|          | 5/1000 [02:13<6:27:57, 23.39s/it, loss=nan]Training:   1%|          | 6/1000 [02:34<6:16:32, 22.73s/it, loss=nan]Training:   1%|          | 6/1000 [02:34<6:16:32, 22.73s/it, loss=nan]Training:   1%|          | 7/1000 [02:47<5:21:32, 19.43s/it, loss=nan]Training:   1%|          | 7/1000 [02:47<5:21:32, 19.43s/it, loss=nan]Training:   1%|          | 8/1000 [03:01<4:52:26, 17.69s/it, loss=nan]Training:   1%|          | 8/1000 [03:01<4:52:26, 17.69s/it, loss=nan]Training:   1%|          | 9/1000 [03:14<4:31:18, 16.43s/it, loss=nan]Training:   1%|          | 9/1000 [03:14<4:31:18, 16.43s/it, loss=nan]Training:   1%|          | 10/1000 [03:26<4:07:37, 15.01s/it, loss=nan]Training:   1%|          | 10/1000 [03:26<4:07:37, 15.01s/it, loss=nan]Training:   1%|          | 11/1000 [03:40<3:58:08, 14.45s/it, loss=nan]Training:   1%|          | 11/1000 [03:40<3:58:08, 14.45s/it, loss=nan]Training:   1%|          | 12/1000 [03:53<3:51:35, 14.06s/it, loss=nan]Training:   1%|          | 12/1000 [03:53<3:51:35, 14.06s/it, loss=nan]Training:   1%|â–         | 13/1000 [04:05<3:43:03, 13.56s/it, loss=nan]Training:   1%|â–         | 13/1000 [04:05<3:43:03, 13.56s/it, loss=nan]Training:   1%|â–         | 14/1000 [04:18<3:37:51, 13.26s/it, loss=nan]Training:   1%|â–         | 14/1000 [04:18<3:37:51, 13.26s/it, loss=nan]Training:   2%|â–         | 15/1000 [04:31<3:39:06, 13.35s/it, loss=nan]Training:   2%|â–         | 15/1000 [04:31<3:39:06, 13.35s/it, loss=nan]Training:   2%|â–         | 16/1000 [04:46<3:44:53, 13.71s/it, loss=nan]Training:   2%|â–         | 16/1000 [04:46<3:44:53, 13.71s/it, loss=nan]Training:   2%|â–         | 17/1000 [04:57<3:33:16, 13.02s/it, loss=nan]Training:   2%|â–         | 17/1000 [04:57<3:33:16, 13.02s/it, loss=nan]Training:   2%|â–         | 18/1000 [05:12<3:39:34, 13.42s/it, loss=nan]Training:   2%|â–         | 18/1000 [05:12<3:39:34, 13.42s/it, loss=nan]Training:   2%|â–         | 19/1000 [05:25<3:38:16, 13.35s/it, loss=nan]Training:   2%|â–         | 19/1000 [05:25<3:38:16, 13.35s/it, loss=nan]Training:   2%|â–         | 20/1000 [05:39<3:42:46, 13.64s/it, loss=nan]Training:   2%|â–         | 20/1000 [05:39<3:42:46, 13.64s/it, loss=nan]Training:   2%|â–         | 21/1000 [05:52<3:38:53, 13.42s/it, loss=nan]Training:   2%|â–         | 21/1000 [05:52<3:38:53, 13.42s/it, loss=nan]Training:   2%|â–         | 22/1000 [06:04<3:31:22, 12.97s/it, loss=nan]Training:   2%|â–         | 22/1000 [06:04<3:31:22, 12.97s/it, loss=nan]Training:   2%|â–         | 23/1000 [06:17<3:31:03, 12.96s/it, loss=nan]Training:   2%|â–         | 23/1000 [06:17<3:31:03, 12.96s/it, loss=nan]Training:   2%|â–         | 24/1000 [06:29<3:28:41, 12.83s/it, loss=nan]Training:   2%|â–         | 24/1000 [06:29<3:28:41, 12.83s/it, loss=nan]Training:   2%|â–Ž         | 25/1000 [06:42<3:27:01, 12.74s/it, loss=nan]Training:   2%|â–Ž         | 25/1000 [06:42<3:27:01, 12.74s/it, loss=nan]Training:   3%|â–Ž         | 26/1000 [06:54<3:24:18, 12.59s/it, loss=nan]Training:   3%|â–Ž         | 26/1000 [06:54<3:24:18, 12.59s/it, loss=nan]Training:   3%|â–Ž         | 27/1000 [07:06<3:19:59, 12.33s/it, loss=nan]Training:   3%|â–Ž         | 27/1000 [07:06<3:19:59, 12.33s/it, loss=nan]Training:   3%|â–Ž         | 28/1000 [07:20<3:28:41, 12.88s/it, loss=nan]Training:   3%|â–Ž         | 28/1000 [07:20<3:28:41, 12.88s/it, loss=nan]Training:   3%|â–Ž         | 29/1000 [07:33<3:28:31, 12.88s/it, loss=nan]Training:   3%|â–Ž         | 29/1000 [07:33<3:28:31, 12.88s/it, loss=nan]Training:   3%|â–Ž         | 30/1000 [07:45<3:23:15, 12.57s/it, loss=nan]Training:   3%|â–Ž         | 30/1000 [07:45<3:23:15, 12.57s/it, loss=nan]Training:   3%|â–Ž         | 31/1000 [07:56<3:17:51, 12.25s/it, loss=nan]Training:   3%|â–Ž         | 31/1000 [07:56<3:17:51, 12.25s/it, loss=nan]Training:   3%|â–Ž         | 32/1000 [08:08<3:17:48, 12.26s/it, loss=nan]Training:   3%|â–Ž         | 32/1000 [08:08<3:17:48, 12.26s/it, loss=nan]Training:   3%|â–Ž         | 33/1000 [08:22<3:23:34, 12.63s/it, loss=nan]Training:   3%|â–Ž         | 33/1000 [08:22<3:23:34, 12.63s/it, loss=nan]Training:   3%|â–Ž         | 34/1000 [08:35<3:26:40, 12.84s/it, loss=nan]Training:   3%|â–Ž         | 34/1000 [08:35<3:26:40, 12.84s/it, loss=nan]Training:   4%|â–Ž         | 35/1000 [08:47<3:21:39, 12.54s/it, loss=nan]Training:   4%|â–Ž         | 35/1000 [08:47<3:21:39, 12.54s/it, loss=nan]Training:   4%|â–Ž         | 36/1000 [09:02<3:32:28, 13.22s/it, loss=nan]Training:   4%|â–Ž         | 36/1000 [09:02<3:32:28, 13.22s/it, loss=nan]Training:   4%|â–Ž         | 37/1000 [09:19<3:50:03, 14.33s/it, loss=nan]Training:   4%|â–Ž         | 37/1000 [09:19<3:50:03, 14.33s/it, loss=nan]Training:   4%|â–         | 38/1000 [09:34<3:54:59, 14.66s/it, loss=nan]Training:   4%|â–         | 38/1000 [09:34<3:54:59, 14.66s/it, loss=nan]Training:   4%|â–         | 39/1000 [09:49<3:55:43, 14.72s/it, loss=nan]Training:   4%|â–         | 39/1000 [09:49<3:55:43, 14.72s/it, loss=nan]Training:   4%|â–         | 40/1000 [10:01<3:43:43, 13.98s/it, loss=nan]Training:   4%|â–         | 40/1000 [10:01<3:43:43, 13.98s/it, loss=nan]Training:   4%|â–         | 41/1000 [10:14<3:36:10, 13.53s/it, loss=nan]Training:   4%|â–         | 41/1000 [10:14<3:36:10, 13.53s/it, loss=nan]Training:   4%|â–         | 42/1000 [10:27<3:34:03, 13.41s/it, loss=nan]Training:   4%|â–         | 42/1000 [10:27<3:34:03, 13.41s/it, loss=nan]Training:   4%|â–         | 43/1000 [10:57<4:54:06, 18.44s/it, loss=nan]Training:   4%|â–         | 43/1000 [10:57<4:54:06, 18.44s/it, loss=nan]Training:   4%|â–         | 44/1000 [11:11<4:33:39, 17.18s/it, loss=nan]Training:   4%|â–         | 44/1000 [11:11<4:33:39, 17.18s/it, loss=nan]Training:   4%|â–         | 45/1000 [11:24<4:13:12, 15.91s/it, loss=nan]Training:   4%|â–         | 45/1000 [11:24<4:13:12, 15.91s/it, loss=nan]Training:   5%|â–         | 46/1000 [11:40<4:10:21, 15.75s/it, loss=nan]Training:   5%|â–         | 46/1000 [11:40<4:10:21, 15.75s/it, loss=nan]Training:   5%|â–         | 47/1000 [11:54<4:05:12, 15.44s/it, loss=nan]Training:   5%|â–         | 47/1000 [11:54<4:05:12, 15.44s/it, loss=nan]Training:   5%|â–         | 48/1000 [12:07<3:50:10, 14.51s/it, loss=nan]Training:   5%|â–         | 48/1000 [12:07<3:50:10, 14.51s/it, loss=nan]Training:   5%|â–         | 49/1000 [12:19<3:36:54, 13.69s/it, loss=nan]Training:   5%|â–         | 49/1000 [12:19<3:36:54, 13.69s/it, loss=nan]Training:   5%|â–Œ         | 50/1000 [12:30<3:25:04, 12.95s/it, loss=nan]Training:   5%|â–Œ         | 50/1000 [12:30<3:25:04, 12.95s/it, loss=nan]Training:   5%|â–Œ         | 51/1000 [12:40<3:13:05, 12.21s/it, loss=nan]Training:   5%|â–Œ         | 51/1000 [12:40<3:13:05, 12.21s/it, loss=nan]Training:   5%|â–Œ         | 52/1000 [12:50<3:01:41, 11.50s/it, loss=nan]Training:   5%|â–Œ         | 52/1000 [12:50<3:01:41, 11.50s/it, loss=nan]Training:   5%|â–Œ         | 53/1000 [13:01<2:56:23, 11.18s/it, loss=nan]Training:   5%|â–Œ         | 53/1000 [13:01<2:56:23, 11.18s/it, loss=nan]Training:   5%|â–Œ         | 54/1000 [13:11<2:52:27, 10.94s/it, loss=nan]Training:   5%|â–Œ         | 54/1000 [13:11<2:52:27, 10.94s/it, loss=nan]Training:   6%|â–Œ         | 55/1000 [13:20<2:44:19, 10.43s/it, loss=nan]Training:   6%|â–Œ         | 55/1000 [13:20<2:44:19, 10.43s/it, loss=nan]Training:   6%|â–Œ         | 56/1000 [13:30<2:41:13, 10.25s/it, loss=nan]Training:   6%|â–Œ         | 56/1000 [13:30<2:41:13, 10.25s/it, loss=nan]Training:   6%|â–Œ         | 57/1000 [13:42<2:48:03, 10.69s/it, loss=nan]Training:   6%|â–Œ         | 57/1000 [13:42<2:48:03, 10.69s/it, loss=nan]Training:   6%|â–Œ         | 58/1000 [13:53<2:52:48, 11.01s/it, loss=nan]Training:   6%|â–Œ         | 58/1000 [13:53<2:52:48, 11.01s/it, loss=nan]Training:   6%|â–Œ         | 59/1000 [14:04<2:51:07, 10.91s/it, loss=nan]Training:   6%|â–Œ         | 59/1000 [14:04<2:51:07, 10.91s/it, loss=nan]Training:   6%|â–Œ         | 60/1000 [14:16<2:55:17, 11.19s/it, loss=nan]Training:   6%|â–Œ         | 60/1000 [14:16<2:55:17, 11.19s/it, loss=nan]Training:   6%|â–Œ         | 61/1000 [14:27<2:55:33, 11.22s/it, loss=nan]Training:   6%|â–Œ         | 61/1000 [14:27<2:55:33, 11.22s/it, loss=nan]Training:   6%|â–Œ         | 62/1000 [14:38<2:51:58, 11.00s/it, loss=nan]Training:   6%|â–Œ         | 62/1000 [14:38<2:51:58, 11.00s/it, loss=nan]Training:   6%|â–‹         | 63/1000 [14:48<2:50:07, 10.89s/it, loss=nan]Training:   6%|â–‹         | 63/1000 [14:48<2:50:07, 10.89s/it, loss=nan]Training:   6%|â–‹         | 64/1000 [15:01<2:57:32, 11.38s/it, loss=nan]Training:   6%|â–‹         | 64/1000 [15:01<2:57:32, 11.38s/it, loss=nan]Training:   6%|â–‹         | 65/1000 [15:11<2:50:06, 10.92s/it, loss=nan]Training:   6%|â–‹         | 65/1000 [15:11<2:50:06, 10.92s/it, loss=nan]Training:   7%|â–‹         | 66/1000 [15:23<2:55:25, 11.27s/it, loss=nan]Training:   7%|â–‹         | 66/1000 [15:23<2:55:25, 11.27s/it, loss=nan]Training:   7%|â–‹         | 67/1000 [15:32<2:46:32, 10.71s/it, loss=nan]Training:   7%|â–‹         | 67/1000 [15:32<2:46:32, 10.71s/it, loss=nan]Training:   7%|â–‹         | 68/1000 [15:43<2:45:26, 10.65s/it, loss=nan]Training:   7%|â–‹         | 68/1000 [15:43<2:45:26, 10.65s/it, loss=nan]Training:   7%|â–‹         | 69/1000 [15:55<2:53:25, 11.18s/it, loss=nan]Training:   7%|â–‹         | 69/1000 [15:55<2:53:25, 11.18s/it, loss=nan]Training:   7%|â–‹         | 70/1000 [16:06<2:49:39, 10.95s/it, loss=nan]Training:   7%|â–‹         | 70/1000 [16:06<2:49:39, 10.95s/it, loss=nan]Training:   7%|â–‹         | 71/1000 [16:15<2:43:36, 10.57s/it, loss=nan]Training:   7%|â–‹         | 71/1000 [16:15<2:43:36, 10.57s/it, loss=nan]Training:   7%|â–‹         | 72/1000 [16:25<2:38:34, 10.25s/it, loss=nan]Training:   7%|â–‹         | 72/1000 [16:25<2:38:34, 10.25s/it, loss=nan]Training:   7%|â–‹         | 73/1000 [16:34<2:35:17, 10.05s/it, loss=nan]Training:   7%|â–‹         | 73/1000 [16:34<2:35:17, 10.05s/it, loss=nan]Training:   7%|â–‹         | 74/1000 [16:44<2:31:11,  9.80s/it, loss=nan]Training:   7%|â–‹         | 74/1000 [16:44<2:31:11,  9.80s/it, loss=nan]Training:   8%|â–Š         | 75/1000 [16:55<2:38:29, 10.28s/it, loss=nan]Training:   8%|â–Š         | 75/1000 [16:55<2:38:29, 10.28s/it, loss=nan]Training:   8%|â–Š         | 76/1000 [17:05<2:36:51, 10.19s/it, loss=nan]Training:   8%|â–Š         | 76/1000 [17:05<2:36:51, 10.19s/it, loss=nan]Training:   8%|â–Š         | 77/1000 [17:17<2:46:32, 10.83s/it, loss=nan]Training:   8%|â–Š         | 77/1000 [17:17<2:46:32, 10.83s/it, loss=nan]Training:   8%|â–Š         | 78/1000 [17:27<2:40:52, 10.47s/it, loss=nan]Training:   8%|â–Š         | 78/1000 [17:27<2:40:52, 10.47s/it, loss=nan]Training:   8%|â–Š         | 79/1000 [17:38<2:42:32, 10.59s/it, loss=nan]Training:   8%|â–Š         | 79/1000 [17:38<2:42:32, 10.59s/it, loss=nan]Training:   8%|â–Š         | 80/1000 [17:47<2:36:07, 10.18s/it, loss=nan]Training:   8%|â–Š         | 80/1000 [17:47<2:36:07, 10.18s/it, loss=nan]Training:   8%|â–Š         | 81/1000 [17:59<2:42:53, 10.63s/it, loss=nan]Training:   8%|â–Š         | 81/1000 [17:59<2:42:53, 10.63s/it, loss=nan]Training:   8%|â–Š         | 82/1000 [18:10<2:45:15, 10.80s/it, loss=nan]Training:   8%|â–Š         | 82/1000 [18:10<2:45:15, 10.80s/it, loss=nan]Training:   8%|â–Š         | 83/1000 [18:19<2:38:20, 10.36s/it, loss=nan]Training:   8%|â–Š         | 83/1000 [18:19<2:38:20, 10.36s/it, loss=nan]Training:   8%|â–Š         | 84/1000 [18:29<2:35:00, 10.15s/it, loss=nan]Training:   8%|â–Š         | 84/1000 [18:29<2:35:00, 10.15s/it, loss=nan]Training:   8%|â–Š         | 85/1000 [18:39<2:34:20, 10.12s/it, loss=nan]Training:   8%|â–Š         | 85/1000 [18:39<2:34:20, 10.12s/it, loss=nan]Training:   9%|â–Š         | 86/1000 [18:49<2:34:39, 10.15s/it, loss=nan]Training:   9%|â–Š         | 86/1000 [18:49<2:34:39, 10.15s/it, loss=nan]Training:   9%|â–Š         | 87/1000 [19:01<2:42:47, 10.70s/it, loss=nan]Training:   9%|â–Š         | 87/1000 [19:01<2:42:47, 10.70s/it, loss=nan]Training:   9%|â–‰         | 88/1000 [19:14<2:51:31, 11.28s/it, loss=nan]Training:   9%|â–‰         | 88/1000 [19:14<2:51:31, 11.28s/it, loss=nan]Training:   9%|â–‰         | 89/1000 [19:24<2:47:43, 11.05s/it, loss=nan]Training:   9%|â–‰         | 89/1000 [19:24<2:47:43, 11.05s/it, loss=nan]Training:   9%|â–‰         | 90/1000 [19:34<2:42:08, 10.69s/it, loss=nan]Training:   9%|â–‰         | 90/1000 [19:34<2:42:08, 10.69s/it, loss=nan]Training:   9%|â–‰         | 91/1000 [19:44<2:37:29, 10.40s/it, loss=nan]Training:   9%|â–‰         | 91/1000 [19:44<2:37:29, 10.40s/it, loss=nan]Training:   9%|â–‰         | 92/1000 [19:55<2:40:21, 10.60s/it, loss=nan]Training:   9%|â–‰         | 92/1000 [19:55<2:40:21, 10.60s/it, loss=nan]Training:   9%|â–‰         | 93/1000 [20:05<2:39:17, 10.54s/it, loss=nan]Training:   9%|â–‰         | 93/1000 [20:05<2:39:17, 10.54s/it, loss=nan]Training:   9%|â–‰         | 94/1000 [20:16<2:37:52, 10.46s/it, loss=nan]Training:   9%|â–‰         | 94/1000 [20:16<2:37:52, 10.46s/it, loss=nan]Training:  10%|â–‰         | 95/1000 [20:26<2:36:57, 10.41s/it, loss=nan]Training:  10%|â–‰         | 95/1000 [20:26<2:36:57, 10.41s/it, loss=nan]Training:  10%|â–‰         | 96/1000 [20:37<2:39:16, 10.57s/it, loss=nan]Training:  10%|â–‰         | 96/1000 [20:37<2:39:16, 10.57s/it, loss=nan]Training:  10%|â–‰         | 97/1000 [20:46<2:33:52, 10.22s/it, loss=nan]Training:  10%|â–‰         | 97/1000 [20:46<2:33:52, 10.22s/it, loss=nan]Training:  10%|â–‰         | 98/1000 [20:56<2:32:30, 10.14s/it, loss=nan]Training:  10%|â–‰         | 98/1000 [20:56<2:32:30, 10.14s/it, loss=nan]Training:  10%|â–‰         | 99/1000 [21:07<2:35:39, 10.37s/it, loss=nan]Training:  10%|â–‰         | 99/1000 [21:07<2:35:39, 10.37s/it, loss=nan]Training:  10%|â–ˆ         | 100/1000 [21:19<2:40:37, 10.71s/it, loss=nan]Training:  10%|â–ˆ         | 100/1000 [21:19<2:40:37, 10.71s/it, loss=nan]Training:  10%|â–ˆ         | 101/1000 [21:31<2:47:26, 11.17s/it, loss=nan]Training:  10%|â–ˆ         | 101/1000 [21:31<2:47:26, 11.17s/it, loss=nan]Training:  10%|â–ˆ         | 102/1000 [21:41<2:44:06, 10.97s/it, loss=nan]Training:  10%|â–ˆ         | 102/1000 [21:41<2:44:06, 10.97s/it, loss=nan]Training:  10%|â–ˆ         | 103/1000 [21:52<2:43:25, 10.93s/it, loss=nan]Training:  10%|â–ˆ         | 103/1000 [21:52<2:43:25, 10.93s/it, loss=nan]Training:  10%|â–ˆ         | 104/1000 [22:01<2:35:28, 10.41s/it, loss=nan]Training:  10%|â–ˆ         | 104/1000 [22:01<2:35:28, 10.41s/it, loss=nan]Training:  10%|â–ˆ         | 105/1000 [22:11<2:33:59, 10.32s/it, loss=nan]Training:  10%|â–ˆ         | 105/1000 [22:11<2:33:59, 10.32s/it, loss=nan]Training:  11%|â–ˆ         | 106/1000 [22:22<2:34:09, 10.35s/it, loss=nan]Training:  11%|â–ˆ         | 106/1000 [22:22<2:34:09, 10.35s/it, loss=nan]Training:  11%|â–ˆ         | 107/1000 [22:34<2:42:11, 10.90s/it, loss=nan]Training:  11%|â–ˆ         | 107/1000 [22:34<2:42:11, 10.90s/it, loss=nan]Training:  11%|â–ˆ         | 108/1000 [22:46<2:45:58, 11.16s/it, loss=nan]Training:  11%|â–ˆ         | 108/1000 [22:46<2:45:58, 11.16s/it, loss=nan]Training:  11%|â–ˆ         | 109/1000 [22:56<2:40:24, 10.80s/it, loss=nan]Training:  11%|â–ˆ         | 109/1000 [22:56<2:40:24, 10.80s/it, loss=nan]Training:  11%|â–ˆ         | 110/1000 [23:08<2:47:29, 11.29s/it, loss=nan]Training:  11%|â–ˆ         | 110/1000 [23:08<2:47:29, 11.29s/it, loss=nan]Training:  11%|â–ˆ         | 111/1000 [23:20<2:49:55, 11.47s/it, loss=nan]Training:  11%|â–ˆ         | 111/1000 [23:20<2:49:55, 11.47s/it, loss=nan]Training:  11%|â–ˆ         | 112/1000 [23:31<2:48:31, 11.39s/it, loss=nan]Training:  11%|â–ˆ         | 112/1000 [23:31<2:48:31, 11.39s/it, loss=nan]Training:  11%|â–ˆâ–        | 113/1000 [23:42<2:44:59, 11.16s/it, loss=nan]Training:  11%|â–ˆâ–        | 113/1000 [23:42<2:44:59, 11.16s/it, loss=nan]Training:  11%|â–ˆâ–        | 114/1000 [23:51<2:37:21, 10.66s/it, loss=nan]Training:  11%|â–ˆâ–        | 114/1000 [23:51<2:37:21, 10.66s/it, loss=nan]Training:  12%|â–ˆâ–        | 115/1000 [24:01<2:31:52, 10.30s/it, loss=nan]Training:  12%|â–ˆâ–        | 115/1000 [24:01<2:31:52, 10.30s/it, loss=nan]Training:  12%|â–ˆâ–        | 116/1000 [24:14<2:42:33, 11.03s/it, loss=nan]Training:  12%|â–ˆâ–        | 116/1000 [24:14<2:42:33, 11.03s/it, loss=nan]Training:  12%|â–ˆâ–        | 117/1000 [24:23<2:36:02, 10.60s/it, loss=nan]Training:  12%|â–ˆâ–        | 117/1000 [24:23<2:36:02, 10.60s/it, loss=nan]Training:  12%|â–ˆâ–        | 118/1000 [24:33<2:30:26, 10.23s/it, loss=nan]Training:  12%|â–ˆâ–        | 118/1000 [24:33<2:30:26, 10.23s/it, loss=nan]Training:  12%|â–ˆâ–        | 119/1000 [24:43<2:31:15, 10.30s/it, loss=nan]Training:  12%|â–ˆâ–        | 119/1000 [24:43<2:31:15, 10.30s/it, loss=nan]Training:  12%|â–ˆâ–        | 120/1000 [24:53<2:27:32, 10.06s/it, loss=nan]Training:  12%|â–ˆâ–        | 120/1000 [24:53<2:27:32, 10.06s/it, loss=nan]Training:  12%|â–ˆâ–        | 121/1000 [25:04<2:33:25, 10.47s/it, loss=nan]Training:  12%|â–ˆâ–        | 121/1000 [25:04<2:33:25, 10.47s/it, loss=nan]Training:  12%|â–ˆâ–        | 122/1000 [25:18<2:48:33, 11.52s/it, loss=nan]Training:  12%|â–ˆâ–        | 122/1000 [25:18<2:48:33, 11.52s/it, loss=nan]Training:  12%|â–ˆâ–        | 123/1000 [25:29<2:46:33, 11.39s/it, loss=nan]Training:  12%|â–ˆâ–        | 123/1000 [25:29<2:46:33, 11.39s/it, loss=nan]Training:  12%|â–ˆâ–        | 124/1000 [25:39<2:39:19, 10.91s/it, loss=nan]Training:  12%|â–ˆâ–        | 124/1000 [25:39<2:39:19, 10.91s/it, loss=nan]Training:  12%|â–ˆâ–Ž        | 125/1000 [25:49<2:34:54, 10.62s/it, loss=nan]Training:  12%|â–ˆâ–Ž        | 125/1000 [25:49<2:34:54, 10.62s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 126/1000 [25:58<2:30:19, 10.32s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 126/1000 [25:58<2:30:19, 10.32s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 127/1000 [26:11<2:39:52, 10.99s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 127/1000 [26:11<2:39:52, 10.99s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 128/1000 [26:23<2:42:10, 11.16s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 128/1000 [26:23<2:42:10, 11.16s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 129/1000 [26:32<2:35:16, 10.70s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 129/1000 [26:32<2:35:16, 10.70s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 130/1000 [26:43<2:35:16, 10.71s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 130/1000 [26:43<2:35:16, 10.71s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 131/1000 [26:53<2:30:24, 10.39s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 131/1000 [26:53<2:30:24, 10.39s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 132/1000 [27:03<2:28:44, 10.28s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 132/1000 [27:03<2:28:44, 10.28s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 133/1000 [27:13<2:28:03, 10.25s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 133/1000 [27:13<2:28:03, 10.25s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 134/1000 [27:24<2:30:38, 10.44s/it, loss=nan]Training:  13%|â–ˆâ–Ž        | 134/1000 [27:24<2:30:38, 10.44s/it, loss=nan]Training:  14%|â–ˆâ–Ž        | 135/1000 [27:34<2:30:06, 10.41s/it, loss=nan]Training:  14%|â–ˆâ–Ž        | 135/1000 [27:34<2:30:06, 10.41s/it, loss=nan]Training:  14%|â–ˆâ–Ž        | 136/1000 [27:44<2:28:35, 10.32s/it, loss=nan]Training:  14%|â–ˆâ–Ž        | 136/1000 [27:44<2:28:35, 10.32s/it, loss=nan]Training:  14%|â–ˆâ–Ž        | 137/1000 [27:54<2:28:33, 10.33s/it, loss=nan]Training:  14%|â–ˆâ–Ž        | 137/1000 [27:54<2:28:33, 10.33s/it, loss=nan]Training:  14%|â–ˆâ–        | 138/1000 [28:04<2:26:13, 10.18s/it, loss=nan]Training:  14%|â–ˆâ–        | 138/1000 [28:04<2:26:13, 10.18s/it, loss=nan]Training:  14%|â–ˆâ–        | 139/1000 [28:14<2:25:04, 10.11s/it, loss=nan]Training:  14%|â–ˆâ–        | 139/1000 [28:14<2:25:04, 10.11s/it, loss=nan]Training:  14%|â–ˆâ–        | 140/1000 [28:25<2:28:59, 10.40s/it, loss=nan]Training:  14%|â–ˆâ–        | 140/1000 [28:25<2:28:59, 10.40s/it, loss=nan]Training:  14%|â–ˆâ–        | 141/1000 [28:36<2:30:36, 10.52s/it, loss=nan]Training:  14%|â–ˆâ–        | 141/1000 [28:36<2:30:36, 10.52s/it, loss=nan]Training:  14%|â–ˆâ–        | 142/1000 [28:45<2:25:14, 10.16s/it, loss=nan]Training:  14%|â–ˆâ–        | 142/1000 [28:45<2:25:14, 10.16s/it, loss=nan]Training:  14%|â–ˆâ–        | 143/1000 [28:56<2:28:04, 10.37s/it, loss=nan]Training:  14%|â–ˆâ–        | 143/1000 [28:56<2:28:04, 10.37s/it, loss=nan]Training:  14%|â–ˆâ–        | 144/1000 [29:06<2:24:44, 10.15s/it, loss=nan]Training:  14%|â–ˆâ–        | 144/1000 [29:06<2:24:44, 10.15s/it, loss=nan]Training:  14%|â–ˆâ–        | 145/1000 [29:17<2:26:45, 10.30s/it, loss=nan]Training:  14%|â–ˆâ–        | 145/1000 [29:17<2:26:45, 10.30s/it, loss=nan]Training:  15%|â–ˆâ–        | 146/1000 [29:28<2:31:11, 10.62s/it, loss=nan]Training:  15%|â–ˆâ–        | 146/1000 [29:28<2:31:11, 10.62s/it, loss=nan]Training:  15%|â–ˆâ–        | 147/1000 [29:38<2:28:41, 10.46s/it, loss=nan]Training:  15%|â–ˆâ–        | 147/1000 [29:38<2:28:41, 10.46s/it, loss=nan]Training:  15%|â–ˆâ–        | 148/1000 [29:48<2:28:04, 10.43s/it, loss=nan]Training:  15%|â–ˆâ–        | 148/1000 [29:48<2:28:04, 10.43s/it, loss=nan]Training:  15%|â–ˆâ–        | 149/1000 [29:58<2:26:39, 10.34s/it, loss=nan]Training:  15%|â–ˆâ–        | 149/1000 [29:58<2:26:39, 10.34s/it, loss=nan]Training:  15%|â–ˆâ–Œ        | 150/1000 [30:11<2:37:12, 11.10s/it, loss=nan]Training:  15%|â–ˆâ–Œ        | 150/1000 [30:11<2:37:12, 11.10s/it, loss=nan]Training:  15%|â–ˆâ–Œ        | 151/1000 [30:22<2:34:02, 10.89s/it, loss=nan]Training:  15%|â–ˆâ–Œ        | 151/1000 [30:22<2:34:02, 10.89s/it, loss=nan]Training:  15%|â–ˆâ–Œ        | 152/1000 [30:33<2:33:48, 10.88s/it, loss=nan]Training:  15%|â–ˆâ–Œ        | 152/1000 [30:33<2:33:48, 10.88s/it, loss=nan]Training:  15%|â–ˆâ–Œ        | 153/1000 [30:43<2:32:47, 10.82s/it, loss=nan]Training:  15%|â–ˆâ–Œ        | 153/1000 [30:43<2:32:47, 10.82s/it, loss=nan]Training:  15%|â–ˆâ–Œ        | 154/1000 [30:53<2:26:48, 10.41s/it, loss=nan]Training:  15%|â–ˆâ–Œ        | 154/1000 [30:53<2:26:48, 10.41s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 155/1000 [31:05<2:32:48, 10.85s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 155/1000 [31:05<2:32:48, 10.85s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 156/1000 [31:14<2:26:16, 10.40s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 156/1000 [31:14<2:26:16, 10.40s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 157/1000 [31:23<2:22:20, 10.13s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 157/1000 [31:23<2:22:20, 10.13s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 158/1000 [31:36<2:33:50, 10.96s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 158/1000 [31:36<2:33:50, 10.96s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 159/1000 [31:47<2:33:07, 10.92s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 159/1000 [31:47<2:33:07, 10.92s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 160/1000 [31:59<2:35:46, 11.13s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 160/1000 [31:59<2:35:46, 11.13s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 161/1000 [32:09<2:30:05, 10.73s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 161/1000 [32:09<2:30:05, 10.73s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 162/1000 [32:19<2:26:36, 10.50s/it, loss=nan]Training:  16%|â–ˆâ–Œ        | 162/1000 [32:19<2:26:36, 10.50s/it, loss=nan]Training:  16%|â–ˆâ–‹        | 163/1000 [32:30<2:30:59, 10.82s/it, loss=nan]Training:  16%|â–ˆâ–‹        | 163/1000 [32:30<2:30:59, 10.82s/it, loss=nan]Training:  16%|â–ˆâ–‹        | 164/1000 [32:40<2:27:25, 10.58s/it, loss=nan]Training:  16%|â–ˆâ–‹        | 164/1000 [32:40<2:27:25, 10.58s/it, loss=nan]Training:  16%|â–ˆâ–‹        | 165/1000 [32:52<2:30:42, 10.83s/it, loss=nan]Training:  16%|â–ˆâ–‹        | 165/1000 [32:52<2:30:42, 10.83s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 166/1000 [33:02<2:29:40, 10.77s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 166/1000 [33:02<2:29:40, 10.77s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 167/1000 [33:12<2:25:01, 10.45s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 167/1000 [33:12<2:25:01, 10.45s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 168/1000 [33:23<2:29:30, 10.78s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 168/1000 [33:23<2:29:30, 10.78s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 169/1000 [33:37<2:40:40, 11.60s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 169/1000 [33:37<2:40:40, 11.60s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 170/1000 [33:47<2:34:49, 11.19s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 170/1000 [33:47<2:34:49, 11.19s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 171/1000 [33:58<2:33:03, 11.08s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 171/1000 [33:58<2:33:03, 11.08s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 172/1000 [34:08<2:29:01, 10.80s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 172/1000 [34:08<2:29:01, 10.80s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 173/1000 [34:18<2:25:11, 10.53s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 173/1000 [34:18<2:25:11, 10.53s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 174/1000 [34:28<2:23:15, 10.41s/it, loss=nan]Training:  17%|â–ˆâ–‹        | 174/1000 [34:28<2:23:15, 10.41s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 175/1000 [34:39<2:23:57, 10.47s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 175/1000 [34:39<2:23:57, 10.47s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 176/1000 [34:49<2:23:25, 10.44s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 176/1000 [34:49<2:23:25, 10.44s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 177/1000 [34:59<2:18:42, 10.11s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 177/1000 [34:59<2:18:42, 10.11s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 178/1000 [35:08<2:15:34,  9.90s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 178/1000 [35:08<2:15:34,  9.90s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 179/1000 [35:18<2:14:42,  9.84s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 179/1000 [35:18<2:14:42,  9.84s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 180/1000 [35:27<2:12:22,  9.69s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 180/1000 [35:27<2:12:22,  9.69s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 181/1000 [35:38<2:19:25, 10.21s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 181/1000 [35:38<2:19:25, 10.21s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 182/1000 [35:52<2:32:47, 11.21s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 182/1000 [35:52<2:32:47, 11.21s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 183/1000 [36:02<2:27:24, 10.83s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 183/1000 [36:02<2:27:24, 10.83s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 184/1000 [36:11<2:21:48, 10.43s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 184/1000 [36:11<2:21:48, 10.43s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 185/1000 [36:22<2:22:44, 10.51s/it, loss=nan]Training:  18%|â–ˆâ–Š        | 185/1000 [36:22<2:22:44, 10.51s/it, loss=nan]Training:  19%|â–ˆâ–Š        | 186/1000 [36:32<2:19:52, 10.31s/it, loss=nan]Training:  19%|â–ˆâ–Š        | 186/1000 [36:32<2:19:52, 10.31s/it, loss=nan]Training:  19%|â–ˆâ–Š        | 187/1000 [36:45<2:30:09, 11.08s/it, loss=nan]Training:  19%|â–ˆâ–Š        | 187/1000 [36:45<2:30:09, 11.08s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 188/1000 [36:57<2:33:02, 11.31s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 188/1000 [36:57<2:33:02, 11.31s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 189/1000 [37:08<2:34:40, 11.44s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 189/1000 [37:08<2:34:40, 11.44s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 190/1000 [37:19<2:30:09, 11.12s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 190/1000 [37:19<2:30:09, 11.12s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 191/1000 [37:30<2:29:44, 11.11s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 191/1000 [37:30<2:29:44, 11.11s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 192/1000 [37:39<2:23:22, 10.65s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 192/1000 [37:39<2:23:22, 10.65s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 193/1000 [37:51<2:27:59, 11.00s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 193/1000 [37:51<2:27:59, 11.00s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 194/1000 [38:01<2:21:14, 10.51s/it, loss=nan]Training:  19%|â–ˆâ–‰        | 194/1000 [38:01<2:21:14, 10.51s/it, loss=nan]Training:  20%|â–ˆâ–‰        | 195/1000 [38:11<2:19:31, 10.40s/it, loss=nan]Training:  20%|â–ˆâ–‰        | 195/1000 [38:11<2:19:31, 10.40s/it, loss=nan]Training:  20%|â–ˆâ–‰        | 196/1000 [38:20<2:15:33, 10.12s/it, loss=nan]Training:  20%|â–ˆâ–‰        | 196/1000 [38:20<2:15:33, 10.12s/it, loss=nan]Training:  20%|â–ˆâ–‰        | 197/1000 [38:30<2:13:38,  9.99s/it, loss=nan]Training:  20%|â–ˆâ–‰        | 197/1000 [38:30<2:13:38,  9.99s/it, loss=nan]Training:  20%|â–ˆâ–‰        | 198/1000 [38:42<2:23:56, 10.77s/it, loss=nan]Training:  20%|â–ˆâ–‰        | 198/1000 [38:42<2:23:56, 10.77s/it, loss=nan]Training:  20%|â–ˆâ–‰        | 199/1000 [38:54<2:28:39, 11.13s/it, loss=nan]Training:  20%|â–ˆâ–‰        | 199/1000 [38:54<2:28:39, 11.13s/it, loss=nan]Training:  20%|â–ˆâ–ˆ        | 200/1000 [39:06<2:30:11, 11.26s/it, loss=nan]Training:  20%|â–ˆâ–ˆ        | 200/1000 [39:06<2:30:11, 11.26s/it, loss=nan]
Evaluating SwahiliNER:   0%|          | 0/50 [00:00<?, ?it/s][A
Evaluating SwahiliNER:   2%|â–         | 1/50 [00:03<02:42,  3.31s/it][A
Evaluating SwahiliNER:   4%|â–         | 2/50 [00:06<02:44,  3.42s/it][A
Evaluating SwahiliNER:   6%|â–Œ         | 3/50 [00:10<02:38,  3.37s/it][A
Evaluating SwahiliNER:   8%|â–Š         | 4/50 [00:13<02:30,  3.28s/it][A
Evaluating SwahiliNER:  10%|â–ˆ         | 5/50 [00:16<02:24,  3.22s/it][A
Evaluating SwahiliNER:  12%|â–ˆâ–        | 6/50 [00:19<02:22,  3.25s/it][A
Evaluating SwahiliNER:  14%|â–ˆâ–        | 7/50 [00:23<02:20,  3.27s/it][A
Evaluating SwahiliNER:  16%|â–ˆâ–Œ        | 8/50 [00:26<02:17,  3.27s/it][A
Evaluating SwahiliNER:  18%|â–ˆâ–Š        | 9/50 [00:29<02:12,  3.23s/it][A
Evaluating SwahiliNER:  20%|â–ˆâ–ˆ        | 10/50 [00:32<02:10,  3.26s/it][A
Evaluating SwahiliNER:  22%|â–ˆâ–ˆâ–       | 11/50 [00:36<02:09,  3.33s/it][A
Evaluating SwahiliNER:  24%|â–ˆâ–ˆâ–       | 12/50 [00:39<02:06,  3.33s/it][A
Evaluating SwahiliNER:  26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:43<02:14,  3.62s/it][A
Evaluating SwahiliNER:  28%|â–ˆâ–ˆâ–Š       | 14/50 [00:46<02:04,  3.47s/it][A
Evaluating SwahiliNER:  30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:50<01:58,  3.37s/it][A
Evaluating SwahiliNER:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:53<01:56,  3.41s/it][A
Evaluating SwahiliNER:  34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:57<01:52,  3.41s/it][A
Evaluating SwahiliNER:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [01:00<01:46,  3.34s/it][A
Evaluating SwahiliNER:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [01:03<01:43,  3.32s/it][A
Evaluating SwahiliNER:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [01:06<01:41,  3.38s/it][A
Evaluating SwahiliNER:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [01:10<01:38,  3.38s/it][A
Evaluating SwahiliNER:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [01:13<01:31,  3.28s/it][A
Evaluating SwahiliNER:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [01:17<01:35,  3.53s/it][A
Evaluating SwahiliNER:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [01:20<01:28,  3.39s/it][A
Evaluating SwahiliNER:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [01:23<01:22,  3.31s/it][A
Evaluating SwahiliNER:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [01:27<01:21,  3.38s/it][A
Evaluating SwahiliNER:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [01:30<01:17,  3.37s/it][A
Evaluating SwahiliNER:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [01:34<01:14,  3.38s/it][A
Evaluating SwahiliNER:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [01:37<01:10,  3.33s/it][A
Evaluating SwahiliNER:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [01:40<01:08,  3.41s/it][A
Evaluating SwahiliNER:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [01:44<01:07,  3.54s/it][A
Evaluating SwahiliNER:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [01:48<01:03,  3.51s/it][A
Evaluating SwahiliNER:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [01:51<00:57,  3.41s/it][A
Evaluating SwahiliNER:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [01:54<00:55,  3.44s/it][A
Evaluating SwahiliNER:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [01:58<00:50,  3.38s/it][A
Evaluating SwahiliNER:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [02:01<00:46,  3.34s/it][A
Evaluating SwahiliNER:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [02:04<00:43,  3.33s/it][A
Evaluating SwahiliNER:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [02:07<00:39,  3.28s/it][A
Evaluating SwahiliNER:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [02:11<00:36,  3.36s/it][A
Evaluating SwahiliNER:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [02:14<00:34,  3.41s/it][A
Evaluating SwahiliNER:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [02:17<00:29,  3.32s/it][A
Evaluating SwahiliNER:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [02:21<00:26,  3.28s/it][A
Evaluating SwahiliNER:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [02:24<00:22,  3.21s/it][A
Evaluating SwahiliNER:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [02:27<00:19,  3.21s/it][A
Evaluating SwahiliNER:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [02:30<00:16,  3.27s/it][A
Evaluating SwahiliNER:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [02:34<00:13,  3.42s/it][A
Evaluating SwahiliNER:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [02:37<00:09,  3.33s/it][A
Evaluating SwahiliNER:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [02:40<00:06,  3.29s/it][A
Evaluating SwahiliNER:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [02:45<00:03,  3.59s/it][A
Evaluating SwahiliNER: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:49<00:00,  3.76s/it][AEvaluating SwahiliNER: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:49<00:00,  3.39s/it]
mlflow logger closed.
Traceback (most recent call last):
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 108, in <module>
    main(args.config)
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 78, in main
    trainer.train()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/training/trainer.py", line 187, in train
    eval_metrics = self.evaluate()
                   ^^^^^^^^^^^^^^^
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/training/trainer.py", line 251, in evaluate
    preds_np = np.concatenate(all_preds[head_name], axis=0)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 390 and the array at index 1 has size 431
Training:  20%|â–ˆâ–ˆ        | 200/1000 [42:04<2:48:18, 12.62s/it, loss=nan]
Loading experiment configuration...
Loading tokenizer: distilbert-base-uncased
Processing data for all tasks...
Processing data for task: QAScoring
Map:   0%|          | 0/4996 [00:00<?, ? examples/s]Map:   3%|â–Ž         | 152/4996 [00:00<00:03, 1486.40 examples/s]Map:   8%|â–Š         | 408/4996 [00:00<00:02, 2106.67 examples/s]Map:  15%|â–ˆâ–        | 725/4996 [00:00<00:02, 2107.52 examples/s]Map:  20%|â–ˆâ–ˆ        | 1000/4996 [00:01<00:06, 598.02 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 1189/4996 [00:01<00:05, 688.28 examples/s]Map:  29%|â–ˆâ–ˆâ–‰       | 1456/4996 [00:01<00:03, 944.41 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1793/4996 [00:01<00:02, 1317.98 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2137/4996 [00:02<00:03, 877.32 examples/s] Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2407/4996 [00:02<00:02, 1091.60 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2653/4996 [00:02<00:01, 1200.95 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2866/4996 [00:02<00:01, 1156.67 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3111/4996 [00:03<00:02, 721.68 examples/s] Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3326/4996 [00:03<00:01, 878.26 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3502/4996 [00:03<00:01, 981.05 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3701/4996 [00:03<00:01, 1050.10 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3917/4996 [00:03<00:00, 1240.83 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4420/4996 [00:04<00:00, 983.73 examples/s] Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4705/4996 [00:04<00:00, 983.98 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4996/4996 [00:05<00:00, 939.89 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4996/4996 [00:05<00:00, 977.34 examples/s]
Map:   0%|          | 0/4996 [00:00<?, ? examples/s]Map:  20%|â–ˆâ–ˆ        | 1000/4996 [00:01<00:04, 856.68 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2000/4996 [00:02<00:04, 702.89 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3000/4996 [00:04<00:03, 595.95 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4000/4996 [00:06<00:01, 564.51 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4996/4996 [00:08<00:00, 608.10 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4996/4996 [00:08<00:00, 616.63 examples/s]
Some weights of MultiTaskModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.embeddings.LayerNorm.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.embeddings.word_embeddings.weight', 'encoder.transformer.layer.0.attention.k_lin.bias', 'encoder.transformer.layer.0.attention.k_lin.weight', 'encoder.transformer.layer.0.attention.out_lin.bias', 'encoder.transformer.layer.0.attention.out_lin.weight', 'encoder.transformer.layer.0.attention.q_lin.bias', 'encoder.transformer.layer.0.attention.q_lin.weight', 'encoder.transformer.layer.0.attention.v_lin.bias', 'encoder.transformer.layer.0.attention.v_lin.weight', 'encoder.transformer.layer.0.ffn.lin1.bias', 'encoder.transformer.layer.0.ffn.lin1.weight', 'encoder.transformer.layer.0.ffn.lin2.bias', 'encoder.transformer.layer.0.ffn.lin2.weight', 'encoder.transformer.layer.0.output_layer_norm.bias', 'encoder.transformer.layer.0.output_layer_norm.weight', 'encoder.transformer.layer.0.sa_layer_norm.bias', 'encoder.transformer.layer.0.sa_layer_norm.weight', 'encoder.transformer.layer.1.attention.k_lin.bias', 'encoder.transformer.layer.1.attention.k_lin.weight', 'encoder.transformer.layer.1.attention.out_lin.bias', 'encoder.transformer.layer.1.attention.out_lin.weight', 'encoder.transformer.layer.1.attention.q_lin.bias', 'encoder.transformer.layer.1.attention.q_lin.weight', 'encoder.transformer.layer.1.attention.v_lin.bias', 'encoder.transformer.layer.1.attention.v_lin.weight', 'encoder.transformer.layer.1.ffn.lin1.bias', 'encoder.transformer.layer.1.ffn.lin1.weight', 'encoder.transformer.layer.1.ffn.lin2.bias', 'encoder.transformer.layer.1.ffn.lin2.weight', 'encoder.transformer.layer.1.output_layer_norm.bias', 'encoder.transformer.layer.1.output_layer_norm.weight', 'encoder.transformer.layer.1.sa_layer_norm.bias', 'encoder.transformer.layer.1.sa_layer_norm.weight', 'encoder.transformer.layer.2.attention.k_lin.bias', 'encoder.transformer.layer.2.attention.k_lin.weight', 'encoder.transformer.layer.2.attention.out_lin.bias', 'encoder.transformer.layer.2.attention.out_lin.weight', 'encoder.transformer.layer.2.attention.q_lin.bias', 'encoder.transformer.layer.2.attention.q_lin.weight', 'encoder.transformer.layer.2.attention.v_lin.bias', 'encoder.transformer.layer.2.attention.v_lin.weight', 'encoder.transformer.layer.2.ffn.lin1.bias', 'encoder.transformer.layer.2.ffn.lin1.weight', 'encoder.transformer.layer.2.ffn.lin2.bias', 'encoder.transformer.layer.2.ffn.lin2.weight', 'encoder.transformer.layer.2.output_layer_norm.bias', 'encoder.transformer.layer.2.output_layer_norm.weight', 'encoder.transformer.layer.2.sa_layer_norm.bias', 'encoder.transformer.layer.2.sa_layer_norm.weight', 'encoder.transformer.layer.3.attention.k_lin.bias', 'encoder.transformer.layer.3.attention.k_lin.weight', 'encoder.transformer.layer.3.attention.out_lin.bias', 'encoder.transformer.layer.3.attention.out_lin.weight', 'encoder.transformer.layer.3.attention.q_lin.bias', 'encoder.transformer.layer.3.attention.q_lin.weight', 'encoder.transformer.layer.3.attention.v_lin.bias', 'encoder.transformer.layer.3.attention.v_lin.weight', 'encoder.transformer.layer.3.ffn.lin1.bias', 'encoder.transformer.layer.3.ffn.lin1.weight', 'encoder.transformer.layer.3.ffn.lin2.bias', 'encoder.transformer.layer.3.ffn.lin2.weight', 'encoder.transformer.layer.3.output_layer_norm.bias', 'encoder.transformer.layer.3.output_layer_norm.weight', 'encoder.transformer.layer.3.sa_layer_norm.bias', 'encoder.transformer.layer.3.sa_layer_norm.weight', 'encoder.transformer.layer.4.attention.k_lin.bias', 'encoder.transformer.layer.4.attention.k_lin.weight', 'encoder.transformer.layer.4.attention.out_lin.bias', 'encoder.transformer.layer.4.attention.out_lin.weight', 'encoder.transformer.layer.4.attention.q_lin.bias', 'encoder.transformer.layer.4.attention.q_lin.weight', 'encoder.transformer.layer.4.attention.v_lin.bias', 'encoder.transformer.layer.4.attention.v_lin.weight', 'encoder.transformer.layer.4.ffn.lin1.bias', 'encoder.transformer.layer.4.ffn.lin1.weight', 'encoder.transformer.layer.4.ffn.lin2.bias', 'encoder.transformer.layer.4.ffn.lin2.weight', 'encoder.transformer.layer.4.output_layer_norm.bias', 'encoder.transformer.layer.4.output_layer_norm.weight', 'encoder.transformer.layer.4.sa_layer_norm.bias', 'encoder.transformer.layer.4.sa_layer_norm.weight', 'encoder.transformer.layer.5.attention.k_lin.bias', 'encoder.transformer.layer.5.attention.k_lin.weight', 'encoder.transformer.layer.5.attention.out_lin.bias', 'encoder.transformer.layer.5.attention.out_lin.weight', 'encoder.transformer.layer.5.attention.q_lin.bias', 'encoder.transformer.layer.5.attention.q_lin.weight', 'encoder.transformer.layer.5.attention.v_lin.bias', 'encoder.transformer.layer.5.attention.v_lin.weight', 'encoder.transformer.layer.5.ffn.lin1.bias', 'encoder.transformer.layer.5.ffn.lin1.weight', 'encoder.transformer.layer.5.ffn.lin2.bias', 'encoder.transformer.layer.5.ffn.lin2.weight', 'encoder.transformer.layer.5.output_layer_norm.bias', 'encoder.transformer.layer.5.output_layer_norm.weight', 'encoder.transformer.layer.5.sa_layer_norm.bias', 'encoder.transformer.layer.5.sa_layer_norm.weight', 'tasks.0.heads.closing.bias', 'tasks.0.heads.closing.weight', 'tasks.0.heads.hold.bias', 'tasks.0.heads.hold.weight', 'tasks.0.heads.listening.bias', 'tasks.0.heads.listening.weight', 'tasks.0.heads.opening.bias', 'tasks.0.heads.opening.weight', 'tasks.0.heads.proactiveness.bias', 'tasks.0.heads.proactiveness.weight', 'tasks.0.heads.resolution.bias', 'tasks.0.heads.resolution.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534
  return FileStore(store_uri, store_uri)
Instantiating tasks and model...
Instantiating trainer...
MLflow logger initialized. Experiment: 'JengaAI_MVP'
Starting training...
Training:   0%|          | 0/2500 [00:00<?, ?it/s]Training:   0%|          | 1/2500 [00:24<16:59:46, 24.48s/it]Training:   0%|          | 1/2500 [00:24<16:59:46, 24.48s/it, loss=nan]Training:   0%|          | 2/2500 [00:45<15:46:29, 22.73s/it, loss=nan]Training:   0%|          | 2/2500 [00:45<15:46:29, 22.73s/it, loss=nan]Training:   0%|          | 3/2500 [00:56<11:45:07, 16.94s/it, loss=nan]Training:   0%|          | 3/2500 [00:56<11:45:07, 16.94s/it, loss=nan]Training:   0%|          | 4/2500 [01:07<10:13:31, 14.75s/it, loss=nan]Training:   0%|          | 4/2500 [01:07<10:13:31, 14.75s/it, loss=nan]Training:   0%|          | 5/2500 [01:29<12:07:10, 17.49s/it, loss=nan]Training:   0%|          | 5/2500 [01:29<12:07:10, 17.49s/it, loss=nan]Training:   0%|          | 6/2500 [02:01<15:30:13, 22.38s/it, loss=nan]Training:   0%|          | 6/2500 [02:01<15:30:13, 22.38s/it, loss=nan]Training:   0%|          | 7/2500 [02:28<16:24:13, 23.69s/it, loss=nan]Training:   0%|          | 7/2500 [02:28<16:24:13, 23.69s/it, loss=nan]Training:   0%|          | 8/2500 [02:40<14:00:08, 20.23s/it, loss=nan]Training:   0%|          | 8/2500 [02:40<14:00:08, 20.23s/it, loss=nan]Training:   0%|          | 9/2500 [02:55<12:49:06, 18.53s/it, loss=nan]Training:   0%|          | 9/2500 [02:55<12:49:06, 18.53s/it, loss=nan]Training:   0%|          | 10/2500 [03:07<11:20:38, 16.40s/it, loss=nan]Training:   0%|          | 10/2500 [03:07<11:20:38, 16.40s/it, loss=nan]Training:   0%|          | 11/2500 [03:18<10:14:39, 14.82s/it, loss=nan]Training:   0%|          | 11/2500 [03:18<10:14:39, 14.82s/it, loss=nan]Training:   0%|          | 12/2500 [03:29<9:29:13, 13.73s/it, loss=nan] Training:   0%|          | 12/2500 [03:29<9:29:13, 13.73s/it, loss=nan]Training:   1%|          | 13/2500 [03:39<8:35:44, 12.44s/it, loss=nan]Training:   1%|          | 13/2500 [03:39<8:35:44, 12.44s/it, loss=nan]Training:   1%|          | 14/2500 [03:49<8:08:50, 11.80s/it, loss=nan]Training:   1%|          | 14/2500 [03:49<8:08:50, 11.80s/it, loss=nan]Training:   1%|          | 15/2500 [04:01<8:07:29, 11.77s/it, loss=nan]Training:   1%|          | 15/2500 [04:01<8:07:29, 11.77s/it, loss=nan]Training:   1%|          | 16/2500 [04:13<8:16:50, 12.00s/it, loss=nan]Training:   1%|          | 16/2500 [04:13<8:16:50, 12.00s/it, loss=nan]Training:   1%|          | 17/2500 [04:22<7:40:23, 11.13s/it, loss=nan]Training:   1%|          | 17/2500 [04:22<7:40:23, 11.13s/it, loss=nan]Training:   1%|          | 18/2500 [04:33<7:35:36, 11.01s/it, loss=nan]Training:   1%|          | 18/2500 [04:33<7:35:36, 11.01s/it, loss=nan]Training:   1%|          | 19/2500 [04:43<7:15:30, 10.53s/it, loss=nan]Training:   1%|          | 19/2500 [04:43<7:15:30, 10.53s/it, loss=nan]Training:   1%|          | 20/2500 [04:52<6:57:15, 10.10s/it, loss=nan]Training:   1%|          | 20/2500 [04:52<6:57:15, 10.10s/it, loss=nan]Training:   1%|          | 21/2500 [05:03<7:11:41, 10.45s/it, loss=nan]Training:   1%|          | 21/2500 [05:03<7:11:41, 10.45s/it, loss=nan]Training:   1%|          | 22/2500 [05:12<6:52:25,  9.99s/it, loss=nan]Training:   1%|          | 22/2500 [05:12<6:52:25,  9.99s/it, loss=nan]Training:   1%|          | 23/2500 [05:23<7:03:43, 10.26s/it, loss=nan]Training:   1%|          | 23/2500 [05:23<7:03:43, 10.26s/it, loss=nan]Training:   1%|          | 24/2500 [05:32<6:51:12,  9.96s/it, loss=nan]Training:   1%|          | 24/2500 [05:32<6:51:12,  9.96s/it, loss=nan]Training:   1%|          | 25/2500 [05:42<6:52:00,  9.99s/it, loss=nan]Training:   1%|          | 25/2500 [05:42<6:52:00,  9.99s/it, loss=nan]Training:   1%|          | 26/2500 [05:53<7:03:12, 10.26s/it, loss=nan]Training:   1%|          | 26/2500 [05:53<7:03:12, 10.26s/it, loss=nan]Training:   1%|          | 27/2500 [06:10<8:28:56, 12.35s/it, loss=nan]Training:   1%|          | 27/2500 [06:10<8:28:56, 12.35s/it, loss=nan]Training:   1%|          | 28/2500 [06:21<8:06:17, 11.80s/it, loss=nan]Training:   1%|          | 28/2500 [06:21<8:06:17, 11.80s/it, loss=nan]Training:   1%|          | 29/2500 [06:31<7:47:15, 11.35s/it, loss=nan]Training:   1%|          | 29/2500 [06:31<7:47:15, 11.35s/it, loss=nan]Training:   1%|          | 30/2500 [06:41<7:32:01, 10.98s/it, loss=nan]Training:   1%|          | 30/2500 [06:41<7:32:01, 10.98s/it, loss=nan]Training:   1%|          | 31/2500 [06:51<7:16:42, 10.61s/it, loss=nan]Training:   1%|          | 31/2500 [06:51<7:16:42, 10.61s/it, loss=nan]Training:   1%|â–         | 32/2500 [07:01<7:12:57, 10.53s/it, loss=nan]Training:   1%|â–         | 32/2500 [07:01<7:12:57, 10.53s/it, loss=nan]Training:   1%|â–         | 33/2500 [07:13<7:33:09, 11.02s/it, loss=nan]Training:   1%|â–         | 33/2500 [07:13<7:33:09, 11.02s/it, loss=nan]Training:   1%|â–         | 34/2500 [07:23<7:19:36, 10.70s/it, loss=nan]Training:   1%|â–         | 34/2500 [07:23<7:19:36, 10.70s/it, loss=nan]Training:   1%|â–         | 35/2500 [07:33<7:04:21, 10.33s/it, loss=nan]Training:   1%|â–         | 35/2500 [07:33<7:04:21, 10.33s/it, loss=nan]Training:   1%|â–         | 36/2500 [07:43<7:03:41, 10.32s/it, loss=nan]Training:   1%|â–         | 36/2500 [07:43<7:03:41, 10.32s/it, loss=nan]Training:   1%|â–         | 37/2500 [07:52<6:46:44,  9.91s/it, loss=nan]Training:   1%|â–         | 37/2500 [07:52<6:46:44,  9.91s/it, loss=nan]Training:   2%|â–         | 38/2500 [08:00<6:28:42,  9.47s/it, loss=nan]Training:   2%|â–         | 38/2500 [08:00<6:28:42,  9.47s/it, loss=nan]Training:   2%|â–         | 39/2500 [08:12<7:00:10, 10.24s/it, loss=nan]Training:   2%|â–         | 39/2500 [08:12<7:00:10, 10.24s/it, loss=nan]Training:   2%|â–         | 40/2500 [08:23<7:08:47, 10.46s/it, loss=nan]Training:   2%|â–         | 40/2500 [08:23<7:08:47, 10.46s/it, loss=nan]Training:   2%|â–         | 41/2500 [08:34<7:07:54, 10.44s/it, loss=nan]Training:   2%|â–         | 41/2500 [08:34<7:07:54, 10.44s/it, loss=nan]Training:   2%|â–         | 42/2500 [08:44<7:05:09, 10.38s/it, loss=nan]Training:   2%|â–         | 42/2500 [08:44<7:05:09, 10.38s/it, loss=nan]Training:   2%|â–         | 43/2500 [08:54<6:59:55, 10.25s/it, loss=nan]Training:   2%|â–         | 43/2500 [08:54<6:59:55, 10.25s/it, loss=nan]Training:   2%|â–         | 44/2500 [09:04<7:01:16, 10.29s/it, loss=nan]Training:   2%|â–         | 44/2500 [09:04<7:01:16, 10.29s/it, loss=nan]Training:   2%|â–         | 45/2500 [09:16<7:14:21, 10.62s/it, loss=nan]Training:   2%|â–         | 45/2500 [09:16<7:14:21, 10.62s/it, loss=nan]Training:   2%|â–         | 46/2500 [09:27<7:21:13, 10.79s/it, loss=nan]Training:   2%|â–         | 46/2500 [09:27<7:21:13, 10.79s/it, loss=nan]Training:   2%|â–         | 47/2500 [09:38<7:22:25, 10.82s/it, loss=nan]Training:   2%|â–         | 47/2500 [09:38<7:22:25, 10.82s/it, loss=nan]Training:   2%|â–         | 48/2500 [09:48<7:12:56, 10.59s/it, loss=nan]Training:   2%|â–         | 48/2500 [09:48<7:12:56, 10.59s/it, loss=nan]Training:   2%|â–         | 49/2500 [09:59<7:15:36, 10.66s/it, loss=nan]Training:   2%|â–         | 49/2500 [09:59<7:15:36, 10.66s/it, loss=nan]Training:   2%|â–         | 50/2500 [10:09<7:10:05, 10.53s/it, loss=nan]Training:   2%|â–         | 50/2500 [10:09<7:10:05, 10.53s/it, loss=nan]Training:   2%|â–         | 51/2500 [10:20<7:13:47, 10.63s/it, loss=nan]Training:   2%|â–         | 51/2500 [10:20<7:13:47, 10.63s/it, loss=nan]Training:   2%|â–         | 52/2500 [10:31<7:18:32, 10.75s/it, loss=nan]Training:   2%|â–         | 52/2500 [10:31<7:18:32, 10.75s/it, loss=nan]Training:   2%|â–         | 53/2500 [10:42<7:18:09, 10.74s/it, loss=nan]Training:   2%|â–         | 53/2500 [10:42<7:18:09, 10.74s/it, loss=nan]Training:   2%|â–         | 54/2500 [10:52<7:11:16, 10.58s/it, loss=nan]Training:   2%|â–         | 54/2500 [10:52<7:11:16, 10.58s/it, loss=nan]Training:   2%|â–         | 55/2500 [11:02<7:09:43, 10.55s/it, loss=nan]Training:   2%|â–         | 55/2500 [11:02<7:09:43, 10.55s/it, loss=nan]Training:   2%|â–         | 56/2500 [11:14<7:18:57, 10.78s/it, loss=nan]Training:   2%|â–         | 56/2500 [11:14<7:18:57, 10.78s/it, loss=nan]Training:   2%|â–         | 57/2500 [11:22<6:46:00,  9.97s/it, loss=nan]Training:   2%|â–         | 57/2500 [11:22<6:46:00,  9.97s/it, loss=nan]Training:   2%|â–         | 58/2500 [11:31<6:37:21,  9.76s/it, loss=nan]Training:   2%|â–         | 58/2500 [11:31<6:37:21,  9.76s/it, loss=nan]Training:   2%|â–         | 59/2500 [11:39<6:12:06,  9.15s/it, loss=nan]Training:   2%|â–         | 59/2500 [11:39<6:12:06,  9.15s/it, loss=nan]Training:   2%|â–         | 60/2500 [11:46<5:53:17,  8.69s/it, loss=nan]Training:   2%|â–         | 60/2500 [11:46<5:53:17,  8.69s/it, loss=nan]Training:   2%|â–         | 61/2500 [11:54<5:36:04,  8.27s/it, loss=nan]Training:   2%|â–         | 61/2500 [11:54<5:36:04,  8.27s/it, loss=nan]Training:   2%|â–         | 62/2500 [12:01<5:27:24,  8.06s/it, loss=nan]Training:   2%|â–         | 62/2500 [12:01<5:27:24,  8.06s/it, loss=nan]Training:   3%|â–Ž         | 63/2500 [12:09<5:23:15,  7.96s/it, loss=nan]Training:   3%|â–Ž         | 63/2500 [12:09<5:23:15,  7.96s/it, loss=nan]Training:   3%|â–Ž         | 64/2500 [12:16<5:16:57,  7.81s/it, loss=nan]Training:   3%|â–Ž         | 64/2500 [12:16<5:16:57,  7.81s/it, loss=nan]Training:   3%|â–Ž         | 65/2500 [12:24<5:14:21,  7.75s/it, loss=nan]Training:   3%|â–Ž         | 65/2500 [12:24<5:14:21,  7.75s/it, loss=nan]Training:   3%|â–Ž         | 66/2500 [12:33<5:30:42,  8.15s/it, loss=nan]Training:   3%|â–Ž         | 66/2500 [12:33<5:30:42,  8.15s/it, loss=nan]Training:   3%|â–Ž         | 67/2500 [12:42<5:42:04,  8.44s/it, loss=nan]Training:   3%|â–Ž         | 67/2500 [12:42<5:42:04,  8.44s/it, loss=nan]Training:   3%|â–Ž         | 68/2500 [12:52<6:04:20,  8.99s/it, loss=nan]Training:   3%|â–Ž         | 68/2500 [12:52<6:04:20,  8.99s/it, loss=nan]Training:   3%|â–Ž         | 69/2500 [13:01<5:57:39,  8.83s/it, loss=nan]Training:   3%|â–Ž         | 69/2500 [13:01<5:57:39,  8.83s/it, loss=nan]Training:   3%|â–Ž         | 70/2500 [13:09<5:45:34,  8.53s/it, loss=nan]Training:   3%|â–Ž         | 70/2500 [13:09<5:45:34,  8.53s/it, loss=nan]Training:   3%|â–Ž         | 71/2500 [13:17<5:38:18,  8.36s/it, loss=nan]Training:   3%|â–Ž         | 71/2500 [13:17<5:38:18,  8.36s/it, loss=nan]Training:   3%|â–Ž         | 72/2500 [13:25<5:35:10,  8.28s/it, loss=nan]Training:   3%|â–Ž         | 72/2500 [13:25<5:35:10,  8.28s/it, loss=nan]Training:   3%|â–Ž         | 73/2500 [13:34<5:47:05,  8.58s/it, loss=nan]Training:   3%|â–Ž         | 73/2500 [13:34<5:47:05,  8.58s/it, loss=nan]Training:   3%|â–Ž         | 74/2500 [13:44<6:00:55,  8.93s/it, loss=nan]Training:   3%|â–Ž         | 74/2500 [13:44<6:00:55,  8.93s/it, loss=nan]Training:   3%|â–Ž         | 75/2500 [13:56<6:40:44,  9.92s/it, loss=nan]Training:   3%|â–Ž         | 75/2500 [13:56<6:40:44,  9.92s/it, loss=nan]Training:   3%|â–Ž         | 76/2500 [14:05<6:24:33,  9.52s/it, loss=nan]Training:   3%|â–Ž         | 76/2500 [14:05<6:24:33,  9.52s/it, loss=nan]Training:   3%|â–Ž         | 77/2500 [14:12<6:01:26,  8.95s/it, loss=nan]Training:   3%|â–Ž         | 77/2500 [14:12<6:01:26,  8.95s/it, loss=nan]Training:   3%|â–Ž         | 78/2500 [14:20<5:44:11,  8.53s/it, loss=nan]Training:   3%|â–Ž         | 78/2500 [14:20<5:44:11,  8.53s/it, loss=nan]Training:   3%|â–Ž         | 79/2500 [14:27<5:32:42,  8.25s/it, loss=nan]Training:   3%|â–Ž         | 79/2500 [14:27<5:32:42,  8.25s/it, loss=nan]Training:   3%|â–Ž         | 80/2500 [14:36<5:39:01,  8.41s/it, loss=nan]Training:   3%|â–Ž         | 80/2500 [14:36<5:39:01,  8.41s/it, loss=nan]Training:   3%|â–Ž         | 81/2500 [14:44<5:34:52,  8.31s/it, loss=nan]Training:   3%|â–Ž         | 81/2500 [14:44<5:34:52,  8.31s/it, loss=nan]Training:   3%|â–Ž         | 82/2500 [14:52<5:25:50,  8.09s/it, loss=nan]Training:   3%|â–Ž         | 82/2500 [14:52<5:25:50,  8.09s/it, loss=nan]Training:   3%|â–Ž         | 83/2500 [15:04<6:13:58,  9.28s/it, loss=nan]Training:   3%|â–Ž         | 83/2500 [15:04<6:13:58,  9.28s/it, loss=nan]Training:   3%|â–Ž         | 84/2500 [15:14<6:28:54,  9.66s/it, loss=nan]Training:   3%|â–Ž         | 84/2500 [15:14<6:28:54,  9.66s/it, loss=nan]Training:   3%|â–Ž         | 85/2500 [15:25<6:42:34, 10.00s/it, loss=nan]Training:   3%|â–Ž         | 85/2500 [15:25<6:42:34, 10.00s/it, loss=nan]Training:   3%|â–Ž         | 86/2500 [15:36<6:50:19, 10.20s/it, loss=nan]Training:   3%|â–Ž         | 86/2500 [15:36<6:50:19, 10.20s/it, loss=nan]Training:   3%|â–Ž         | 87/2500 [15:48<7:08:29, 10.65s/it, loss=nan]Training:   3%|â–Ž         | 87/2500 [15:48<7:08:29, 10.65s/it, loss=nan]Training:   4%|â–Ž         | 88/2500 [15:58<7:04:05, 10.55s/it, loss=nan]Training:   4%|â–Ž         | 88/2500 [15:58<7:04:05, 10.55s/it, loss=nan]Training:   4%|â–Ž         | 89/2500 [16:08<6:53:14, 10.28s/it, loss=nan]Training:   4%|â–Ž         | 89/2500 [16:08<6:53:14, 10.28s/it, loss=nan]Training:   4%|â–Ž         | 90/2500 [16:18<6:50:04, 10.21s/it, loss=nan]Training:   4%|â–Ž         | 90/2500 [16:18<6:50:04, 10.21s/it, loss=nan]Training:   4%|â–Ž         | 91/2500 [16:28<6:48:41, 10.18s/it, loss=nan]Training:   4%|â–Ž         | 91/2500 [16:28<6:48:41, 10.18s/it, loss=nan]Training:   4%|â–Ž         | 92/2500 [16:38<6:45:05, 10.09s/it, loss=nan]Training:   4%|â–Ž         | 92/2500 [16:38<6:45:05, 10.09s/it, loss=nan]Training:   4%|â–Ž         | 93/2500 [16:50<7:09:13, 10.70s/it, loss=nan]Training:   4%|â–Ž         | 93/2500 [16:50<7:09:13, 10.70s/it, loss=nan]Training:   4%|â–         | 94/2500 [17:04<7:51:24, 11.76s/it, loss=nan]Training:   4%|â–         | 94/2500 [17:04<7:51:24, 11.76s/it, loss=nan]Training:   4%|â–         | 95/2500 [17:17<8:07:14, 12.16s/it, loss=nan]Training:   4%|â–         | 95/2500 [17:17<8:07:14, 12.16s/it, loss=nan]Training:   4%|â–         | 96/2500 [17:30<8:19:11, 12.46s/it, loss=nan]Training:   4%|â–         | 96/2500 [17:30<8:19:11, 12.46s/it, loss=nan]Training:   4%|â–         | 97/2500 [17:52<10:12:07, 15.28s/it, loss=nan]Training:   4%|â–         | 97/2500 [17:52<10:12:07, 15.28s/it, loss=nan]Training:   4%|â–         | 98/2500 [19:20<24:48:50, 37.19s/it, loss=nan]Training:   4%|â–         | 98/2500 [19:20<24:48:50, 37.19s/it, loss=nan]Training:   4%|â–         | 99/2500 [20:05<26:20:02, 39.48s/it, loss=nan]Training:   4%|â–         | 99/2500 [20:05<26:20:02, 39.48s/it, loss=nan]Training:   4%|â–         | 100/2500 [20:42<25:51:06, 38.78s/it, loss=nan]Training:   4%|â–         | 100/2500 [20:42<25:51:06, 38.78s/it, loss=nan]Training:   4%|â–         | 101/2500 [20:58<21:12:14, 31.82s/it, loss=nan]Training:   4%|â–         | 101/2500 [20:58<21:12:14, 31.82s/it, loss=nan]Training:   4%|â–         | 102/2500 [21:16<18:29:31, 27.76s/it, loss=nan]Training:   4%|â–         | 102/2500 [21:16<18:29:31, 27.76s/it, loss=nan]Training:   4%|â–         | 103/2500 [21:47<19:07:30, 28.72s/it, loss=nan]Training:   4%|â–         | 103/2500 [21:47<19:07:30, 28.72s/it, loss=nan]Training:   4%|â–         | 104/2500 [22:03<16:35:28, 24.93s/it, loss=nan]Training:   4%|â–         | 104/2500 [22:03<16:35:28, 24.93s/it, loss=nan]Training:   4%|â–         | 105/2500 [22:25<16:02:11, 24.10s/it, loss=nan]Training:   4%|â–         | 105/2500 [22:25<16:02:11, 24.10s/it, loss=nan]Training:   4%|â–         | 106/2500 [22:44<14:51:06, 22.33s/it, loss=nan]Training:   4%|â–         | 106/2500 [22:44<14:51:06, 22.33s/it, loss=nan]Training:   4%|â–         | 107/2500 [23:06<14:55:12, 22.45s/it, loss=nan]Training:   4%|â–         | 107/2500 [23:06<14:55:12, 22.45s/it, loss=nan]Training:   4%|â–         | 108/2500 [23:32<15:30:21, 23.34s/it, loss=nan]Training:   4%|â–         | 108/2500 [23:32<15:30:21, 23.34s/it, loss=nan]Training:   4%|â–         | 109/2500 [23:49<14:12:53, 21.40s/it, loss=nan]Training:   4%|â–         | 109/2500 [23:49<14:12:53, 21.40s/it, loss=nan]Training:   4%|â–         | 110/2500 [24:01<12:26:40, 18.75s/it, loss=nan]Training:   4%|â–         | 110/2500 [24:01<12:26:40, 18.75s/it, loss=nan]Training:   4%|â–         | 111/2500 [24:21<12:35:29, 18.97s/it, loss=nan]Training:   4%|â–         | 111/2500 [24:21<12:35:29, 18.97s/it, loss=nan]Training:   4%|â–         | 112/2500 [24:41<12:45:57, 19.25s/it, loss=nan]Training:   4%|â–         | 112/2500 [24:41<12:45:57, 19.25s/it, loss=nan]Training:   5%|â–         | 113/2500 [24:57<12:08:59, 18.32s/it, loss=nan]Training:   5%|â–         | 113/2500 [24:57<12:08:59, 18.32s/it, loss=nan]Training:   5%|â–         | 114/2500 [25:17<12:34:10, 18.96s/it, loss=nan]Training:   5%|â–         | 114/2500 [25:17<12:34:10, 18.96s/it, loss=nan]Training:   5%|â–         | 115/2500 [25:40<13:16:15, 20.03s/it, loss=nan]Training:   5%|â–         | 115/2500 [25:40<13:16:15, 20.03s/it, loss=nan]Training:   5%|â–         | 116/2500 [25:53<12:01:43, 18.16s/it, loss=nan]Training:   5%|â–         | 116/2500 [25:53<12:01:43, 18.16s/it, loss=nan]Training:   5%|â–         | 117/2500 [26:08<11:15:49, 17.02s/it, loss=nan]Training:   5%|â–         | 117/2500 [26:08<11:15:49, 17.02s/it, loss=nan]Training:   5%|â–         | 118/2500 [26:22<10:46:54, 16.29s/it, loss=nan]Training:   5%|â–         | 118/2500 [26:22<10:46:54, 16.29s/it, loss=nan]Training:   5%|â–         | 119/2500 [26:42<11:20:23, 17.15s/it, loss=nan]Training:   5%|â–         | 119/2500 [26:42<11:20:23, 17.15s/it, loss=nan]Training:   5%|â–         | 120/2500 [27:06<12:52:39, 19.48s/it, loss=nan]Training:   5%|â–         | 120/2500 [27:06<12:52:39, 19.48s/it, loss=nan]Training:   5%|â–         | 121/2500 [27:28<13:22:07, 20.23s/it, loss=nan]Training:   5%|â–         | 121/2500 [27:28<13:22:07, 20.23s/it, loss=nan]Training:   5%|â–         | 122/2500 [27:42<11:57:32, 18.10s/it, loss=nan]Training:   5%|â–         | 122/2500 [27:42<11:57:32, 18.10s/it, loss=nan]Training:   5%|â–         | 123/2500 [28:01<12:07:12, 18.36s/it, loss=nan]Training:   5%|â–         | 123/2500 [28:01<12:07:12, 18.36s/it, loss=nan]Training:   5%|â–         | 124/2500 [28:24<13:04:05, 19.80s/it, loss=nan]Training:   5%|â–         | 124/2500 [28:24<13:04:05, 19.80s/it, loss=nan]Training:   5%|â–Œ         | 125/2500 [28:42<12:48:47, 19.42s/it, loss=nan]Training:   5%|â–Œ         | 125/2500 [28:42<12:48:47, 19.42s/it, loss=nan]Training:   5%|â–Œ         | 126/2500 [29:14<15:19:47, 23.25s/it, loss=nan]Training:   5%|â–Œ         | 126/2500 [29:14<15:19:47, 23.25s/it, loss=nan]Training:   5%|â–Œ         | 127/2500 [29:39<15:36:21, 23.68s/it, loss=nan]Training:   5%|â–Œ         | 127/2500 [29:39<15:36:21, 23.68s/it, loss=nan]Training:   5%|â–Œ         | 128/2500 [29:55<14:05:06, 21.38s/it, loss=nan]Training:   5%|â–Œ         | 128/2500 [29:55<14:05:06, 21.38s/it, loss=nan]Training:   5%|â–Œ         | 129/2500 [30:15<13:44:58, 20.88s/it, loss=nan]Training:   5%|â–Œ         | 129/2500 [30:15<13:44:58, 20.88s/it, loss=nan]Training:   5%|â–Œ         | 130/2500 [30:34<13:26:32, 20.42s/it, loss=nan]Training:   5%|â–Œ         | 130/2500 [30:34<13:26:32, 20.42s/it, loss=nan]Training:   5%|â–Œ         | 131/2500 [30:58<14:06:40, 21.44s/it, loss=nan]Training:   5%|â–Œ         | 131/2500 [30:58<14:06:40, 21.44s/it, loss=nan]Training:   5%|â–Œ         | 132/2500 [31:21<14:28:37, 22.01s/it, loss=nan]Training:   5%|â–Œ         | 132/2500 [31:21<14:28:37, 22.01s/it, loss=nan]Training:   5%|â–Œ         | 133/2500 [31:39<13:30:58, 20.56s/it, loss=nan]Training:   5%|â–Œ         | 133/2500 [31:39<13:30:58, 20.56s/it, loss=nan]Training:   5%|â–Œ         | 134/2500 [31:48<11:16:44, 17.16s/it, loss=nan]Training:   5%|â–Œ         | 134/2500 [31:48<11:16:44, 17.16s/it, loss=nan]Training:   5%|â–Œ         | 135/2500 [31:58<9:57:52, 15.17s/it, loss=nan] Training:   5%|â–Œ         | 135/2500 [31:58<9:57:52, 15.17s/it, loss=nan]Training:   5%|â–Œ         | 136/2500 [32:13<9:49:02, 14.95s/it, loss=nan]Training:   5%|â–Œ         | 136/2500 [32:13<9:49:02, 14.95s/it, loss=nan]Training:   5%|â–Œ         | 137/2500 [32:35<11:11:51, 17.06s/it, loss=nan]Training:   5%|â–Œ         | 137/2500 [32:35<11:11:51, 17.06s/it, loss=nan]Training:   6%|â–Œ         | 138/2500 [32:50<10:51:54, 16.56s/it, loss=nan]Training:   6%|â–Œ         | 138/2500 [32:50<10:51:54, 16.56s/it, loss=nan]Training:   6%|â–Œ         | 139/2500 [33:09<11:24:17, 17.39s/it, loss=nan]Training:   6%|â–Œ         | 139/2500 [33:09<11:24:17, 17.39s/it, loss=nan]Training:   6%|â–Œ         | 140/2500 [33:30<11:59:42, 18.30s/it, loss=nan]Training:   6%|â–Œ         | 140/2500 [33:30<11:59:42, 18.30s/it, loss=nan]Training:   6%|â–Œ         | 141/2500 [33:49<12:06:56, 18.49s/it, loss=nan]Training:   6%|â–Œ         | 141/2500 [33:49<12:06:56, 18.49s/it, loss=nan]Training:   6%|â–Œ         | 142/2500 [34:08<12:12:32, 18.64s/it, loss=nan]Training:   6%|â–Œ         | 142/2500 [34:08<12:12:32, 18.64s/it, loss=nan]Training:   6%|â–Œ         | 143/2500 [34:31<13:08:28, 20.07s/it, loss=nan]Training:   6%|â–Œ         | 143/2500 [34:31<13:08:28, 20.07s/it, loss=nan]Training:   6%|â–Œ         | 144/2500 [34:52<13:13:37, 20.21s/it, loss=nan]Training:   6%|â–Œ         | 144/2500 [34:52<13:13:37, 20.21s/it, loss=nan]Training:   6%|â–Œ         | 145/2500 [35:21<15:05:25, 23.07s/it, loss=nan]Training:   6%|â–Œ         | 145/2500 [35:21<15:05:25, 23.07s/it, loss=nan]Training:   6%|â–Œ         | 146/2500 [35:50<16:04:05, 24.57s/it, loss=nan]Training:   6%|â–Œ         | 146/2500 [35:50<16:04:05, 24.57s/it, loss=nan]Training:   6%|â–Œ         | 147/2500 [36:05<14:15:57, 21.83s/it, loss=nan]Training:   6%|â–Œ         | 147/2500 [36:05<14:15:57, 21.83s/it, loss=nan]Training:   6%|â–Œ         | 148/2500 [36:21<13:05:19, 20.03s/it, loss=nan]Training:   6%|â–Œ         | 148/2500 [36:21<13:05:19, 20.03s/it, loss=nan]Training:   6%|â–Œ         | 149/2500 [36:44<13:43:57, 21.03s/it, loss=nan]Training:   6%|â–Œ         | 149/2500 [36:44<13:43:57, 21.03s/it, loss=nan]Training:   6%|â–Œ         | 150/2500 [37:02<13:10:13, 20.18s/it, loss=nan]Training:   6%|â–Œ         | 150/2500 [37:02<13:10:13, 20.18s/it, loss=nan]Training:   6%|â–Œ         | 151/2500 [37:22<13:09:05, 20.16s/it, loss=nan]Training:   6%|â–Œ         | 151/2500 [37:22<13:09:05, 20.16s/it, loss=nan]Training:   6%|â–Œ         | 152/2500 [37:43<13:17:13, 20.37s/it, loss=nan]Training:   6%|â–Œ         | 152/2500 [37:43<13:17:13, 20.37s/it, loss=nan]Training:   6%|â–Œ         | 153/2500 [38:17<15:53:29, 24.38s/it, loss=nan]Training:   6%|â–Œ         | 153/2500 [38:17<15:53:29, 24.38s/it, loss=nan]Training:   6%|â–Œ         | 154/2500 [38:46<16:41:02, 25.60s/it, loss=nan]Training:   6%|â–Œ         | 154/2500 [38:46<16:41:02, 25.60s/it, loss=nan]Training:   6%|â–Œ         | 155/2500 [39:15<17:25:43, 26.76s/it, loss=nan]Training:   6%|â–Œ         | 155/2500 [39:15<17:25:43, 26.76s/it, loss=nan]Training:   6%|â–Œ         | 156/2500 [39:39<16:50:22, 25.86s/it, loss=nan]Training:   6%|â–Œ         | 156/2500 [39:39<16:50:22, 25.86s/it, loss=nan]Training:   6%|â–‹         | 157/2500 [39:59<15:44:06, 24.18s/it, loss=nan]Training:   6%|â–‹         | 157/2500 [39:59<15:44:06, 24.18s/it, loss=nan]Training:   6%|â–‹         | 158/2500 [40:20<15:06:48, 23.23s/it, loss=nan]Training:   6%|â–‹         | 158/2500 [40:20<15:06:48, 23.23s/it, loss=nan]Training:   6%|â–‹         | 159/2500 [40:56<17:32:06, 26.97s/it, loss=nan]Training:   6%|â–‹         | 159/2500 [40:56<17:32:06, 26.97s/it, loss=nan]Training:   6%|â–‹         | 160/2500 [41:13<15:35:13, 23.98s/it, loss=nan]Training:   6%|â–‹         | 160/2500 [41:13<15:35:13, 23.98s/it, loss=nan]Training:   6%|â–‹         | 161/2500 [41:30<14:15:28, 21.94s/it, loss=nan]Training:   6%|â–‹         | 161/2500 [41:30<14:15:28, 21.94s/it, loss=nan]Training:   6%|â–‹         | 162/2500 [41:50<13:48:28, 21.26s/it, loss=nan]Training:   6%|â–‹         | 162/2500 [41:50<13:48:28, 21.26s/it, loss=nan]Training:   7%|â–‹         | 163/2500 [42:09<13:25:26, 20.68s/it, loss=nan]Training:   7%|â–‹         | 163/2500 [42:09<13:25:26, 20.68s/it, loss=nan]Training:   7%|â–‹         | 164/2500 [42:25<12:28:16, 19.22s/it, loss=nan]Training:   7%|â–‹         | 164/2500 [42:25<12:28:16, 19.22s/it, loss=nan]Training:   7%|â–‹         | 165/2500 [42:46<12:56:00, 19.94s/it, loss=nan]Training:   7%|â–‹         | 165/2500 [42:46<12:56:00, 19.94s/it, loss=nan]Training:   7%|â–‹         | 166/2500 [43:08<13:16:24, 20.47s/it, loss=nan]Training:   7%|â–‹         | 166/2500 [43:08<13:16:24, 20.47s/it, loss=nan]Training:   7%|â–‹         | 167/2500 [43:30<13:30:39, 20.85s/it, loss=nan]Training:   7%|â–‹         | 167/2500 [43:30<13:30:39, 20.85s/it, loss=nan]Training:   7%|â–‹         | 168/2500 [43:58<14:57:20, 23.09s/it, loss=nan]Training:   7%|â–‹         | 168/2500 [43:58<14:57:20, 23.09s/it, loss=nan]Training:   7%|â–‹         | 169/2500 [44:33<17:20:47, 26.79s/it, loss=nan]Training:   7%|â–‹         | 169/2500 [44:33<17:20:47, 26.79s/it, loss=nan]Training:   7%|â–‹         | 170/2500 [44:51<15:26:44, 23.86s/it, loss=nan]Training:   7%|â–‹         | 170/2500 [44:51<15:26:44, 23.86s/it, loss=nan]Training:   7%|â–‹         | 171/2500 [45:00<12:40:02, 19.58s/it, loss=nan]Training:   7%|â–‹         | 171/2500 [45:00<12:40:02, 19.58s/it, loss=nan]Training:   7%|â–‹         | 172/2500 [45:09<10:35:30, 16.38s/it, loss=nan]Training:   7%|â–‹         | 172/2500 [45:09<10:35:30, 16.38s/it, loss=nan]Training:   7%|â–‹         | 173/2500 [45:19<9:15:27, 14.32s/it, loss=nan] Training:   7%|â–‹         | 173/2500 [45:19<9:15:27, 14.32s/it, loss=nan]mlflow logger closed.
Traceback (most recent call last):
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 108, in <module>
    main(args.config)
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 78, in main
    trainer.train()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/training/trainer.py", line 177, in train
    optimizer.step()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 187, in step
    adamw(
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 339, in adamw
    func(
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 470, in _single_tensor_adamw
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
             ^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Exception ignored in atexit callback: <bound method finalize._exitfunc of <class 'weakref.finalize'>>
Traceback (most recent call last):
  File "/usr/lib/python3.12/weakref.py", line 656, in _exitfunc
    pending = cls._select_for_exit()
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/weakref.py", line 637, in _select_for_exit
    L = [(f,i) for (f,i) in cls._registry.items() if i.atexit]
         ^^^^^
KeyboardInterrupt: 
Loading experiment configuration...
Loading tokenizer: distilbert-base-uncased
Processing data for all tasks...
Processing data for task: QAScoring
Map:   0%|          | 0/4996 [00:00<?, ? examples/s]Map:  20%|â–ˆâ–‰        | 985/4996 [00:00<00:00, 9720.32 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2000/4996 [00:00<00:01, 2383.27 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2939/4996 [00:00<00:00, 3548.87 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4000/4996 [00:01<00:00, 2694.60 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4705/4996 [00:01<00:00, 2713.21 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4996/4996 [00:01<00:00, 2708.18 examples/s]
Map:   0%|          | 0/4996 [00:00<?, ? examples/s]Map:  20%|â–ˆâ–ˆ        | 1000/4996 [00:00<00:03, 1176.47 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2000/4996 [00:01<00:02, 1418.98 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3000/4996 [00:02<00:01, 1542.66 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4000/4996 [00:02<00:00, 1534.67 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4996/4996 [00:03<00:00, 1551.11 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4996/4996 [00:03<00:00, 1498.75 examples/s]
Some weights of MultiTaskModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.embeddings.LayerNorm.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.embeddings.word_embeddings.weight', 'encoder.transformer.layer.0.attention.k_lin.bias', 'encoder.transformer.layer.0.attention.k_lin.weight', 'encoder.transformer.layer.0.attention.out_lin.bias', 'encoder.transformer.layer.0.attention.out_lin.weight', 'encoder.transformer.layer.0.attention.q_lin.bias', 'encoder.transformer.layer.0.attention.q_lin.weight', 'encoder.transformer.layer.0.attention.v_lin.bias', 'encoder.transformer.layer.0.attention.v_lin.weight', 'encoder.transformer.layer.0.ffn.lin1.bias', 'encoder.transformer.layer.0.ffn.lin1.weight', 'encoder.transformer.layer.0.ffn.lin2.bias', 'encoder.transformer.layer.0.ffn.lin2.weight', 'encoder.transformer.layer.0.output_layer_norm.bias', 'encoder.transformer.layer.0.output_layer_norm.weight', 'encoder.transformer.layer.0.sa_layer_norm.bias', 'encoder.transformer.layer.0.sa_layer_norm.weight', 'encoder.transformer.layer.1.attention.k_lin.bias', 'encoder.transformer.layer.1.attention.k_lin.weight', 'encoder.transformer.layer.1.attention.out_lin.bias', 'encoder.transformer.layer.1.attention.out_lin.weight', 'encoder.transformer.layer.1.attention.q_lin.bias', 'encoder.transformer.layer.1.attention.q_lin.weight', 'encoder.transformer.layer.1.attention.v_lin.bias', 'encoder.transformer.layer.1.attention.v_lin.weight', 'encoder.transformer.layer.1.ffn.lin1.bias', 'encoder.transformer.layer.1.ffn.lin1.weight', 'encoder.transformer.layer.1.ffn.lin2.bias', 'encoder.transformer.layer.1.ffn.lin2.weight', 'encoder.transformer.layer.1.output_layer_norm.bias', 'encoder.transformer.layer.1.output_layer_norm.weight', 'encoder.transformer.layer.1.sa_layer_norm.bias', 'encoder.transformer.layer.1.sa_layer_norm.weight', 'encoder.transformer.layer.2.attention.k_lin.bias', 'encoder.transformer.layer.2.attention.k_lin.weight', 'encoder.transformer.layer.2.attention.out_lin.bias', 'encoder.transformer.layer.2.attention.out_lin.weight', 'encoder.transformer.layer.2.attention.q_lin.bias', 'encoder.transformer.layer.2.attention.q_lin.weight', 'encoder.transformer.layer.2.attention.v_lin.bias', 'encoder.transformer.layer.2.attention.v_lin.weight', 'encoder.transformer.layer.2.ffn.lin1.bias', 'encoder.transformer.layer.2.ffn.lin1.weight', 'encoder.transformer.layer.2.ffn.lin2.bias', 'encoder.transformer.layer.2.ffn.lin2.weight', 'encoder.transformer.layer.2.output_layer_norm.bias', 'encoder.transformer.layer.2.output_layer_norm.weight', 'encoder.transformer.layer.2.sa_layer_norm.bias', 'encoder.transformer.layer.2.sa_layer_norm.weight', 'encoder.transformer.layer.3.attention.k_lin.bias', 'encoder.transformer.layer.3.attention.k_lin.weight', 'encoder.transformer.layer.3.attention.out_lin.bias', 'encoder.transformer.layer.3.attention.out_lin.weight', 'encoder.transformer.layer.3.attention.q_lin.bias', 'encoder.transformer.layer.3.attention.q_lin.weight', 'encoder.transformer.layer.3.attention.v_lin.bias', 'encoder.transformer.layer.3.attention.v_lin.weight', 'encoder.transformer.layer.3.ffn.lin1.bias', 'encoder.transformer.layer.3.ffn.lin1.weight', 'encoder.transformer.layer.3.ffn.lin2.bias', 'encoder.transformer.layer.3.ffn.lin2.weight', 'encoder.transformer.layer.3.output_layer_norm.bias', 'encoder.transformer.layer.3.output_layer_norm.weight', 'encoder.transformer.layer.3.sa_layer_norm.bias', 'encoder.transformer.layer.3.sa_layer_norm.weight', 'encoder.transformer.layer.4.attention.k_lin.bias', 'encoder.transformer.layer.4.attention.k_lin.weight', 'encoder.transformer.layer.4.attention.out_lin.bias', 'encoder.transformer.layer.4.attention.out_lin.weight', 'encoder.transformer.layer.4.attention.q_lin.bias', 'encoder.transformer.layer.4.attention.q_lin.weight', 'encoder.transformer.layer.4.attention.v_lin.bias', 'encoder.transformer.layer.4.attention.v_lin.weight', 'encoder.transformer.layer.4.ffn.lin1.bias', 'encoder.transformer.layer.4.ffn.lin1.weight', 'encoder.transformer.layer.4.ffn.lin2.bias', 'encoder.transformer.layer.4.ffn.lin2.weight', 'encoder.transformer.layer.4.output_layer_norm.bias', 'encoder.transformer.layer.4.output_layer_norm.weight', 'encoder.transformer.layer.4.sa_layer_norm.bias', 'encoder.transformer.layer.4.sa_layer_norm.weight', 'encoder.transformer.layer.5.attention.k_lin.bias', 'encoder.transformer.layer.5.attention.k_lin.weight', 'encoder.transformer.layer.5.attention.out_lin.bias', 'encoder.transformer.layer.5.attention.out_lin.weight', 'encoder.transformer.layer.5.attention.q_lin.bias', 'encoder.transformer.layer.5.attention.q_lin.weight', 'encoder.transformer.layer.5.attention.v_lin.bias', 'encoder.transformer.layer.5.attention.v_lin.weight', 'encoder.transformer.layer.5.ffn.lin1.bias', 'encoder.transformer.layer.5.ffn.lin1.weight', 'encoder.transformer.layer.5.ffn.lin2.bias', 'encoder.transformer.layer.5.ffn.lin2.weight', 'encoder.transformer.layer.5.output_layer_norm.bias', 'encoder.transformer.layer.5.output_layer_norm.weight', 'encoder.transformer.layer.5.sa_layer_norm.bias', 'encoder.transformer.layer.5.sa_layer_norm.weight', 'tasks.0.heads.closing.bias', 'tasks.0.heads.closing.weight', 'tasks.0.heads.hold.bias', 'tasks.0.heads.hold.weight', 'tasks.0.heads.listening.bias', 'tasks.0.heads.listening.weight', 'tasks.0.heads.opening.bias', 'tasks.0.heads.opening.weight', 'tasks.0.heads.proactiveness.bias', 'tasks.0.heads.proactiveness.weight', 'tasks.0.heads.resolution.bias', 'tasks.0.heads.resolution.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534
  return FileStore(store_uri, store_uri)
Instantiating tasks and model...
Instantiating trainer...
MLflow logger initialized. Experiment: 'JengaAI_MVP'
Starting training...
Training:   0%|          | 0/2500 [00:00<?, ?it/s]Training:   0%|          | 1/2500 [00:11<8:19:17, 11.99s/it]Training:   0%|          | 1/2500 [00:11<8:19:17, 11.99s/it, loss=nan]Training:   0%|          | 2/2500 [00:20<6:47:37,  9.79s/it, loss=nan]Training:   0%|          | 2/2500 [00:20<6:47:37,  9.79s/it, loss=nan]Training:   0%|          | 3/2500 [00:27<6:08:27,  8.85s/it, loss=nan]Training:   0%|          | 3/2500 [00:27<6:08:27,  8.85s/it, loss=nan]Training:   0%|          | 4/2500 [00:35<5:46:23,  8.33s/it, loss=nan]Training:   0%|          | 4/2500 [00:35<5:46:23,  8.33s/it, loss=nan]Training:   0%|          | 5/2500 [00:43<5:37:49,  8.12s/it, loss=nan]Training:   0%|          | 5/2500 [00:43<5:37:49,  8.12s/it, loss=nan]Training:   0%|          | 6/2500 [00:52<5:49:52,  8.42s/it, loss=nan]Training:   0%|          | 6/2500 [00:52<5:49:52,  8.42s/it, loss=nan]Training:   0%|          | 7/2500 [01:00<5:41:18,  8.21s/it, loss=nan]Training:   0%|          | 7/2500 [01:00<5:41:18,  8.21s/it, loss=nan]Training:   0%|          | 8/2500 [01:07<5:30:14,  7.95s/it, loss=nan]Training:   0%|          | 8/2500 [01:07<5:30:14,  7.95s/it, loss=nan]Training:   0%|          | 9/2500 [01:15<5:32:43,  8.01s/it, loss=nan]Training:   0%|          | 9/2500 [01:15<5:32:43,  8.01s/it, loss=nan]Training:   0%|          | 10/2500 [01:23<5:36:43,  8.11s/it, loss=nan]Training:   0%|          | 10/2500 [01:23<5:36:43,  8.11s/it, loss=nan]Training:   0%|          | 11/2500 [01:31<5:27:08,  7.89s/it, loss=nan]Training:   0%|          | 11/2500 [01:31<5:27:08,  7.89s/it, loss=nan]Training:   0%|          | 12/2500 [01:39<5:24:45,  7.83s/it, loss=nan]Training:   0%|          | 12/2500 [01:39<5:24:45,  7.83s/it, loss=nan]Training:   1%|          | 13/2500 [01:46<5:22:09,  7.77s/it, loss=nan]Training:   1%|          | 13/2500 [01:46<5:22:09,  7.77s/it, loss=nan]Training:   1%|          | 14/2500 [01:55<5:34:47,  8.08s/it, loss=nan]Training:   1%|          | 14/2500 [01:55<5:34:47,  8.08s/it, loss=nan]Training:   1%|          | 15/2500 [02:03<5:29:28,  7.96s/it, loss=nan]Training:   1%|          | 15/2500 [02:03<5:29:28,  7.96s/it, loss=nan]Training:   1%|          | 16/2500 [02:10<5:26:13,  7.88s/it, loss=nan]Training:   1%|          | 16/2500 [02:10<5:26:13,  7.88s/it, loss=nan]Training:   1%|          | 17/2500 [02:18<5:20:28,  7.74s/it, loss=nan]Training:   1%|          | 17/2500 [02:18<5:20:28,  7.74s/it, loss=nan]Training:   1%|          | 18/2500 [02:25<5:19:29,  7.72s/it, loss=nan]Training:   1%|          | 18/2500 [02:25<5:19:29,  7.72s/it, loss=nan]Training:   1%|          | 19/2500 [02:33<5:19:22,  7.72s/it, loss=nan]Training:   1%|          | 19/2500 [02:33<5:19:22,  7.72s/it, loss=nan]Training:   1%|          | 20/2500 [02:41<5:15:29,  7.63s/it, loss=nan]Training:   1%|          | 20/2500 [02:41<5:15:29,  7.63s/it, loss=nan]Training:   1%|          | 21/2500 [02:48<5:14:34,  7.61s/it, loss=nan]Training:   1%|          | 21/2500 [02:48<5:14:34,  7.61s/it, loss=nan]Training:   1%|          | 22/2500 [02:57<5:30:21,  8.00s/it, loss=nan]Training:   1%|          | 22/2500 [02:57<5:30:21,  8.00s/it, loss=nan]Training:   1%|          | 23/2500 [03:04<5:23:20,  7.83s/it, loss=nan]Training:   1%|          | 23/2500 [03:04<5:23:20,  7.83s/it, loss=nan]Training:   1%|          | 24/2500 [03:12<5:17:28,  7.69s/it, loss=nan]Training:   1%|          | 24/2500 [03:12<5:17:28,  7.69s/it, loss=nan]Training:   1%|          | 25/2500 [03:19<5:15:50,  7.66s/it, loss=nan]Training:   1%|          | 25/2500 [03:19<5:15:50,  7.66s/it, loss=nan]Training:   1%|          | 26/2500 [03:27<5:17:06,  7.69s/it, loss=nan]Training:   1%|          | 26/2500 [03:27<5:17:06,  7.69s/it, loss=nan]Training:   1%|          | 27/2500 [03:35<5:14:32,  7.63s/it, loss=nan]Training:   1%|          | 27/2500 [03:35<5:14:32,  7.63s/it, loss=nan]Training:   1%|          | 28/2500 [03:42<5:16:08,  7.67s/it, loss=nan]Training:   1%|          | 28/2500 [03:42<5:16:08,  7.67s/it, loss=nan]Training:   1%|          | 29/2500 [03:50<5:13:38,  7.62s/it, loss=nan]Training:   1%|          | 29/2500 [03:50<5:13:38,  7.62s/it, loss=nan]Training:   1%|          | 30/2500 [03:59<5:35:12,  8.14s/it, loss=nan]Training:   1%|          | 30/2500 [03:59<5:35:12,  8.14s/it, loss=nan]Training:   1%|          | 31/2500 [04:07<5:27:57,  7.97s/it, loss=nan]Training:   1%|          | 31/2500 [04:07<5:27:57,  7.97s/it, loss=nan]Training:   1%|â–         | 32/2500 [04:14<5:22:28,  7.84s/it, loss=nan]Training:   1%|â–         | 32/2500 [04:14<5:22:28,  7.84s/it, loss=nan]Training:   1%|â–         | 33/2500 [04:22<5:20:11,  7.79s/it, loss=nan]Training:   1%|â–         | 33/2500 [04:22<5:20:11,  7.79s/it, loss=nan]Training:   1%|â–         | 34/2500 [04:29<5:15:09,  7.67s/it, loss=nan]Training:   1%|â–         | 34/2500 [04:29<5:15:09,  7.67s/it, loss=nan]Training:   1%|â–         | 35/2500 [04:37<5:15:02,  7.67s/it, loss=nan]Training:   1%|â–         | 35/2500 [04:37<5:15:02,  7.67s/it, loss=nan]Training:   1%|â–         | 36/2500 [04:45<5:11:39,  7.59s/it, loss=nan]Training:   1%|â–         | 36/2500 [04:45<5:11:39,  7.59s/it, loss=nan]Training:   1%|â–         | 37/2500 [04:52<5:11:41,  7.59s/it, loss=nan]Training:   1%|â–         | 37/2500 [04:52<5:11:41,  7.59s/it, loss=nan]Training:   2%|â–         | 38/2500 [05:01<5:27:02,  7.97s/it, loss=nan]Training:   2%|â–         | 38/2500 [05:01<5:27:02,  7.97s/it, loss=nan]Training:   2%|â–         | 39/2500 [05:09<5:22:41,  7.87s/it, loss=nan]Training:   2%|â–         | 39/2500 [05:09<5:22:41,  7.87s/it, loss=nan]Training:   2%|â–         | 40/2500 [05:16<5:18:54,  7.78s/it, loss=nan]Training:   2%|â–         | 40/2500 [05:16<5:18:54,  7.78s/it, loss=nan]Training:   2%|â–         | 41/2500 [05:23<5:09:01,  7.54s/it, loss=nan]Training:   2%|â–         | 41/2500 [05:23<5:09:01,  7.54s/it, loss=nan]Training:   2%|â–         | 42/2500 [05:31<5:07:45,  7.51s/it, loss=nan]Training:   2%|â–         | 42/2500 [05:31<5:07:45,  7.51s/it, loss=nan]Training:   2%|â–         | 43/2500 [05:38<5:10:19,  7.58s/it, loss=nan]Training:   2%|â–         | 43/2500 [05:38<5:10:19,  7.58s/it, loss=nan]Training:   2%|â–         | 44/2500 [05:46<5:09:46,  7.57s/it, loss=nan]Training:   2%|â–         | 44/2500 [05:46<5:09:46,  7.57s/it, loss=nan]Training:   2%|â–         | 45/2500 [05:54<5:13:19,  7.66s/it, loss=nan]Training:   2%|â–         | 45/2500 [05:54<5:13:19,  7.66s/it, loss=nan]Training:   2%|â–         | 46/2500 [06:02<5:21:16,  7.86s/it, loss=nan]Training:   2%|â–         | 46/2500 [06:02<5:21:16,  7.86s/it, loss=nan]Training:   2%|â–         | 47/2500 [06:10<5:23:38,  7.92s/it, loss=nan]Training:   2%|â–         | 47/2500 [06:10<5:23:38,  7.92s/it, loss=nan]Training:   2%|â–         | 48/2500 [06:18<5:26:07,  7.98s/it, loss=nan]Training:   2%|â–         | 48/2500 [06:18<5:26:07,  7.98s/it, loss=nan]Training:   2%|â–         | 49/2500 [06:26<5:18:24,  7.79s/it, loss=nan]Training:   2%|â–         | 49/2500 [06:26<5:18:24,  7.79s/it, loss=nan]Training:   2%|â–         | 50/2500 [06:33<5:17:29,  7.78s/it, loss=nan]Training:   2%|â–         | 50/2500 [06:33<5:17:29,  7.78s/it, loss=nan]Training:   2%|â–         | 51/2500 [06:41<5:17:31,  7.78s/it, loss=nan]Training:   2%|â–         | 51/2500 [06:41<5:17:31,  7.78s/it, loss=nan]Training:   2%|â–         | 52/2500 [06:50<5:35:57,  8.23s/it, loss=nan]Training:   2%|â–         | 52/2500 [06:50<5:35:57,  8.23s/it, loss=nan]Training:   2%|â–         | 53/2500 [06:58<5:28:49,  8.06s/it, loss=nan]Training:   2%|â–         | 53/2500 [06:58<5:28:49,  8.06s/it, loss=nan]Training:   2%|â–         | 54/2500 [07:07<5:40:48,  8.36s/it, loss=nan]Training:   2%|â–         | 54/2500 [07:07<5:40:48,  8.36s/it, loss=nan]Training:   2%|â–         | 55/2500 [07:15<5:29:55,  8.10s/it, loss=nan]Training:   2%|â–         | 55/2500 [07:15<5:29:55,  8.10s/it, loss=nan]Training:   2%|â–         | 56/2500 [07:22<5:23:19,  7.94s/it, loss=nan]Training:   2%|â–         | 56/2500 [07:22<5:23:19,  7.94s/it, loss=nan]Training:   2%|â–         | 57/2500 [07:30<5:21:25,  7.89s/it, loss=nan]Training:   2%|â–         | 57/2500 [07:30<5:21:25,  7.89s/it, loss=nan]Training:   2%|â–         | 58/2500 [07:38<5:17:05,  7.79s/it, loss=nan]Training:   2%|â–         | 58/2500 [07:38<5:17:05,  7.79s/it, loss=nan]Training:   2%|â–         | 59/2500 [07:45<5:12:58,  7.69s/it, loss=nan]Training:   2%|â–         | 59/2500 [07:45<5:12:58,  7.69s/it, loss=nan]Training:   2%|â–         | 60/2500 [07:53<5:12:29,  7.68s/it, loss=nan]Training:   2%|â–         | 60/2500 [07:53<5:12:29,  7.68s/it, loss=nan]Training:   2%|â–         | 61/2500 [08:01<5:14:52,  7.75s/it, loss=nan]Training:   2%|â–         | 61/2500 [08:01<5:14:52,  7.75s/it, loss=nan]Training:   2%|â–         | 62/2500 [08:09<5:26:23,  8.03s/it, loss=nan]Training:   2%|â–         | 62/2500 [08:09<5:26:23,  8.03s/it, loss=nan]Training:   3%|â–Ž         | 63/2500 [08:17<5:20:46,  7.90s/it, loss=nan]Training:   3%|â–Ž         | 63/2500 [08:17<5:20:46,  7.90s/it, loss=nan]Training:   3%|â–Ž         | 64/2500 [08:25<5:19:14,  7.86s/it, loss=nan]Training:   3%|â–Ž         | 64/2500 [08:25<5:19:14,  7.86s/it, loss=nan]Training:   3%|â–Ž         | 65/2500 [08:32<5:14:29,  7.75s/it, loss=nan]Training:   3%|â–Ž         | 65/2500 [08:32<5:14:29,  7.75s/it, loss=nan]Training:   3%|â–Ž         | 66/2500 [08:40<5:12:24,  7.70s/it, loss=nan]Training:   3%|â–Ž         | 66/2500 [08:40<5:12:24,  7.70s/it, loss=nan]Training:   3%|â–Ž         | 67/2500 [08:47<5:12:44,  7.71s/it, loss=nan]Training:   3%|â–Ž         | 67/2500 [08:47<5:12:44,  7.71s/it, loss=nan]Training:   3%|â–Ž         | 68/2500 [08:55<5:13:45,  7.74s/it, loss=nan]Training:   3%|â–Ž         | 68/2500 [08:55<5:13:45,  7.74s/it, loss=nan]Training:   3%|â–Ž         | 69/2500 [09:03<5:09:56,  7.65s/it, loss=nan]Training:   3%|â–Ž         | 69/2500 [09:03<5:09:56,  7.65s/it, loss=nan]Training:   3%|â–Ž         | 70/2500 [09:11<5:17:40,  7.84s/it, loss=nan]Training:   3%|â–Ž         | 70/2500 [09:11<5:17:40,  7.84s/it, loss=nan]Training:   3%|â–Ž         | 71/2500 [09:19<5:23:01,  7.98s/it, loss=nan]Training:   3%|â–Ž         | 71/2500 [09:19<5:23:01,  7.98s/it, loss=nan]Training:   3%|â–Ž         | 72/2500 [09:27<5:19:58,  7.91s/it, loss=nan]Training:   3%|â–Ž         | 72/2500 [09:27<5:19:58,  7.91s/it, loss=nan]Training:   3%|â–Ž         | 73/2500 [09:34<5:14:11,  7.77s/it, loss=nan]Training:   3%|â–Ž         | 73/2500 [09:34<5:14:11,  7.77s/it, loss=nan]Training:   3%|â–Ž         | 74/2500 [09:42<5:10:37,  7.68s/it, loss=nan]Training:   3%|â–Ž         | 74/2500 [09:42<5:10:37,  7.68s/it, loss=nan]Training:   3%|â–Ž         | 75/2500 [09:50<5:11:03,  7.70s/it, loss=nan]Training:   3%|â–Ž         | 75/2500 [09:50<5:11:03,  7.70s/it, loss=nan]Training:   3%|â–Ž         | 76/2500 [09:57<5:07:40,  7.62s/it, loss=nan]Training:   3%|â–Ž         | 76/2500 [09:57<5:07:40,  7.62s/it, loss=nan]Training:   3%|â–Ž         | 77/2500 [10:05<5:09:06,  7.65s/it, loss=nan]Training:   3%|â–Ž         | 77/2500 [10:05<5:09:06,  7.65s/it, loss=nan]Training:   3%|â–Ž         | 78/2500 [10:13<5:15:07,  7.81s/it, loss=nan]Training:   3%|â–Ž         | 78/2500 [10:13<5:15:07,  7.81s/it, loss=nan]Training:   3%|â–Ž         | 79/2500 [10:22<5:24:28,  8.04s/it, loss=nan]Training:   3%|â–Ž         | 79/2500 [10:22<5:24:28,  8.04s/it, loss=nan]Training:   3%|â–Ž         | 80/2500 [10:29<5:18:18,  7.89s/it, loss=nan]Training:   3%|â–Ž         | 80/2500 [10:29<5:18:18,  7.89s/it, loss=nan]Training:   3%|â–Ž         | 81/2500 [10:37<5:12:57,  7.76s/it, loss=nan]Training:   3%|â–Ž         | 81/2500 [10:37<5:12:57,  7.76s/it, loss=nan]Training:   3%|â–Ž         | 82/2500 [10:44<5:09:43,  7.69s/it, loss=nan]Training:   3%|â–Ž         | 82/2500 [10:44<5:09:43,  7.69s/it, loss=nan]Training:   3%|â–Ž         | 83/2500 [10:52<5:10:06,  7.70s/it, loss=nan]Training:   3%|â–Ž         | 83/2500 [10:52<5:10:06,  7.70s/it, loss=nan]Training:   3%|â–Ž         | 84/2500 [10:59<5:05:22,  7.58s/it, loss=nan]Training:   3%|â–Ž         | 84/2500 [10:59<5:05:22,  7.58s/it, loss=nan]Training:   3%|â–Ž         | 85/2500 [11:07<5:08:29,  7.66s/it, loss=nan]Training:   3%|â–Ž         | 85/2500 [11:07<5:08:29,  7.66s/it, loss=nan]Training:   3%|â–Ž         | 86/2500 [11:15<5:09:44,  7.70s/it, loss=nan]Training:   3%|â–Ž         | 86/2500 [11:15<5:09:44,  7.70s/it, loss=nan]Training:   3%|â–Ž         | 87/2500 [11:23<5:21:12,  7.99s/it, loss=nan]Training:   3%|â–Ž         | 87/2500 [11:23<5:21:12,  7.99s/it, loss=nan]Training:   4%|â–Ž         | 88/2500 [11:31<5:17:15,  7.89s/it, loss=nan]Training:   4%|â–Ž         | 88/2500 [11:31<5:17:15,  7.89s/it, loss=nan]Training:   4%|â–Ž         | 89/2500 [11:39<5:13:20,  7.80s/it, loss=nan]Training:   4%|â–Ž         | 89/2500 [11:39<5:13:20,  7.80s/it, loss=nan]Training:   4%|â–Ž         | 90/2500 [11:46<5:11:57,  7.77s/it, loss=nan]Training:   4%|â–Ž         | 90/2500 [11:46<5:11:57,  7.77s/it, loss=nan]Training:   4%|â–Ž         | 91/2500 [11:54<5:07:03,  7.65s/it, loss=nan]Training:   4%|â–Ž         | 91/2500 [11:54<5:07:03,  7.65s/it, loss=nan]Training:   4%|â–Ž         | 92/2500 [12:01<5:05:42,  7.62s/it, loss=nan]Training:   4%|â–Ž         | 92/2500 [12:01<5:05:42,  7.62s/it, loss=nan]Training:   4%|â–Ž         | 93/2500 [12:09<5:05:45,  7.62s/it, loss=nan]Training:   4%|â–Ž         | 93/2500 [12:09<5:05:45,  7.62s/it, loss=nan]Training:   4%|â–         | 94/2500 [12:17<5:06:25,  7.64s/it, loss=nan]Training:   4%|â–         | 94/2500 [12:17<5:06:25,  7.64s/it, loss=nan]Training:   4%|â–         | 95/2500 [12:25<5:20:56,  8.01s/it, loss=nan]Training:   4%|â–         | 95/2500 [12:25<5:20:56,  8.01s/it, loss=nan]Training:   4%|â–         | 96/2500 [12:33<5:16:10,  7.89s/it, loss=nan]Training:   4%|â–         | 96/2500 [12:33<5:16:10,  7.89s/it, loss=nan]Training:   4%|â–         | 97/2500 [12:41<5:13:15,  7.82s/it, loss=nan]Training:   4%|â–         | 97/2500 [12:41<5:13:15,  7.82s/it, loss=nan]Training:   4%|â–         | 98/2500 [12:48<5:09:30,  7.73s/it, loss=nan]Training:   4%|â–         | 98/2500 [12:48<5:09:30,  7.73s/it, loss=nan]Training:   4%|â–         | 99/2500 [12:56<5:08:43,  7.71s/it, loss=nan]Training:   4%|â–         | 99/2500 [12:56<5:08:43,  7.71s/it, loss=nan]Training:   4%|â–         | 100/2500 [13:04<5:08:36,  7.72s/it, loss=nan]Training:   4%|â–         | 100/2500 [13:04<5:08:36,  7.72s/it, loss=nan]Training:   4%|â–         | 101/2500 [13:11<5:04:05,  7.61s/it, loss=nan]Training:   4%|â–         | 101/2500 [13:11<5:04:05,  7.61s/it, loss=nan]Training:   4%|â–         | 102/2500 [13:19<5:03:46,  7.60s/it, loss=nan]Training:   4%|â–         | 102/2500 [13:19<5:03:46,  7.60s/it, loss=nan]Training:   4%|â–         | 103/2500 [13:28<5:21:58,  8.06s/it, loss=nan]Training:   4%|â–         | 103/2500 [13:28<5:21:58,  8.06s/it, loss=nan]Training:   4%|â–         | 104/2500 [13:35<5:16:04,  7.91s/it, loss=nan]Training:   4%|â–         | 104/2500 [13:35<5:16:04,  7.91s/it, loss=nan]Training:   4%|â–         | 105/2500 [13:43<5:11:03,  7.79s/it, loss=nan]Training:   4%|â–         | 105/2500 [13:43<5:11:03,  7.79s/it, loss=nan]Training:   4%|â–         | 106/2500 [13:50<5:08:16,  7.73s/it, loss=nan]Training:   4%|â–         | 106/2500 [13:50<5:08:16,  7.73s/it, loss=nan]Training:   4%|â–         | 107/2500 [13:58<5:05:57,  7.67s/it, loss=nan]Training:   4%|â–         | 107/2500 [13:58<5:05:57,  7.67s/it, loss=nan]Training:   4%|â–         | 108/2500 [14:06<5:05:16,  7.66s/it, loss=nan]Training:   4%|â–         | 108/2500 [14:06<5:05:16,  7.66s/it, loss=nan]Training:   4%|â–         | 109/2500 [14:12<4:56:36,  7.44s/it, loss=nan]Training:   4%|â–         | 109/2500 [14:12<4:56:36,  7.44s/it, loss=nan]Training:   4%|â–         | 110/2500 [14:20<4:56:00,  7.43s/it, loss=nan]Training:   4%|â–         | 110/2500 [14:20<4:56:00,  7.43s/it, loss=nan]Training:   4%|â–         | 111/2500 [14:29<5:13:32,  7.87s/it, loss=nan]Training:   4%|â–         | 111/2500 [14:29<5:13:32,  7.87s/it, loss=nan]Training:   4%|â–         | 112/2500 [14:36<5:09:29,  7.78s/it, loss=nan]Training:   4%|â–         | 112/2500 [14:36<5:09:29,  7.78s/it, loss=nan]Training:   5%|â–         | 113/2500 [14:44<5:05:22,  7.68s/it, loss=nan]Training:   5%|â–         | 113/2500 [14:44<5:05:22,  7.68s/it, loss=nan]Training:   5%|â–         | 114/2500 [14:51<5:00:47,  7.56s/it, loss=nan]Training:   5%|â–         | 114/2500 [14:51<5:00:47,  7.56s/it, loss=nan]Training:   5%|â–         | 115/2500 [14:59<5:01:53,  7.59s/it, loss=nan]Training:   5%|â–         | 115/2500 [14:59<5:01:53,  7.59s/it, loss=nan]Training:   5%|â–         | 116/2500 [15:06<5:01:02,  7.58s/it, loss=nan]Training:   5%|â–         | 116/2500 [15:06<5:01:02,  7.58s/it, loss=nan]Training:   5%|â–         | 117/2500 [15:14<5:01:00,  7.58s/it, loss=nan]Training:   5%|â–         | 117/2500 [15:14<5:01:00,  7.58s/it, loss=nan]Training:   5%|â–         | 118/2500 [15:21<4:58:45,  7.53s/it, loss=nan]Training:   5%|â–         | 118/2500 [15:21<4:58:45,  7.53s/it, loss=nan]Training:   5%|â–         | 119/2500 [15:30<5:06:59,  7.74s/it, loss=nan]Training:   5%|â–         | 119/2500 [15:30<5:06:59,  7.74s/it, loss=nan]Training:   5%|â–         | 120/2500 [15:38<5:15:16,  7.95s/it, loss=nan]Training:   5%|â–         | 120/2500 [15:38<5:15:16,  7.95s/it, loss=nan]Training:   5%|â–         | 121/2500 [15:46<5:10:47,  7.84s/it, loss=nan]Training:   5%|â–         | 121/2500 [15:46<5:10:47,  7.84s/it, loss=nan]Training:   5%|â–         | 122/2500 [15:53<5:08:12,  7.78s/it, loss=nan]Training:   5%|â–         | 122/2500 [15:53<5:08:12,  7.78s/it, loss=nan]Training:   5%|â–         | 123/2500 [16:01<5:04:36,  7.69s/it, loss=nan]Training:   5%|â–         | 123/2500 [16:01<5:04:36,  7.69s/it, loss=nan]Training:   5%|â–         | 124/2500 [16:08<5:04:51,  7.70s/it, loss=nan]Training:   5%|â–         | 124/2500 [16:08<5:04:51,  7.70s/it, loss=nan]Training:   5%|â–Œ         | 125/2500 [16:16<5:00:00,  7.58s/it, loss=nan]Training:   5%|â–Œ         | 125/2500 [16:16<5:00:00,  7.58s/it, loss=nan]Training:   5%|â–Œ         | 126/2500 [16:23<5:02:21,  7.64s/it, loss=nan]Training:   5%|â–Œ         | 126/2500 [16:23<5:02:21,  7.64s/it, loss=nan]Training:   5%|â–Œ         | 127/2500 [16:31<5:02:59,  7.66s/it, loss=nan]Training:   5%|â–Œ         | 127/2500 [16:31<5:02:59,  7.66s/it, loss=nan]Training:   5%|â–Œ         | 128/2500 [16:40<5:16:15,  8.00s/it, loss=nan]Training:   5%|â–Œ         | 128/2500 [16:40<5:16:15,  8.00s/it, loss=nan]Training:   5%|â–Œ         | 129/2500 [16:47<5:10:17,  7.85s/it, loss=nan]Training:   5%|â–Œ         | 129/2500 [16:47<5:10:17,  7.85s/it, loss=nan]Training:   5%|â–Œ         | 130/2500 [16:55<5:03:04,  7.67s/it, loss=nan]Training:   5%|â–Œ         | 130/2500 [16:55<5:03:04,  7.67s/it, loss=nan]Training:   5%|â–Œ         | 131/2500 [17:02<5:00:33,  7.61s/it, loss=nan]Training:   5%|â–Œ         | 131/2500 [17:02<5:00:33,  7.61s/it, loss=nan]Training:   5%|â–Œ         | 132/2500 [17:10<5:01:57,  7.65s/it, loss=nan]Training:   5%|â–Œ         | 132/2500 [17:10<5:01:57,  7.65s/it, loss=nan]Training:   5%|â–Œ         | 133/2500 [17:17<5:00:16,  7.61s/it, loss=nan]Training:   5%|â–Œ         | 133/2500 [17:17<5:00:16,  7.61s/it, loss=nan]Training:   5%|â–Œ         | 134/2500 [17:25<4:56:52,  7.53s/it, loss=nan]Training:   5%|â–Œ         | 134/2500 [17:25<4:56:52,  7.53s/it, loss=nan]Training:   5%|â–Œ         | 135/2500 [17:32<4:55:16,  7.49s/it, loss=nan]Training:   5%|â–Œ         | 135/2500 [17:32<4:55:16,  7.49s/it, loss=nan]Training:   5%|â–Œ         | 136/2500 [17:41<5:12:33,  7.93s/it, loss=nan]Training:   5%|â–Œ         | 136/2500 [17:41<5:12:33,  7.93s/it, loss=nan]Training:   5%|â–Œ         | 137/2500 [17:49<5:12:08,  7.93s/it, loss=nan]Training:   5%|â–Œ         | 137/2500 [17:49<5:12:08,  7.93s/it, loss=nan]Training:   6%|â–Œ         | 138/2500 [17:56<5:06:04,  7.77s/it, loss=nan]Training:   6%|â–Œ         | 138/2500 [17:56<5:06:04,  7.77s/it, loss=nan]Training:   6%|â–Œ         | 139/2500 [18:04<5:03:10,  7.70s/it, loss=nan]Training:   6%|â–Œ         | 139/2500 [18:04<5:03:10,  7.70s/it, loss=nan]Training:   6%|â–Œ         | 140/2500 [18:12<5:04:11,  7.73s/it, loss=nan]Training:   6%|â–Œ         | 140/2500 [18:12<5:04:11,  7.73s/it, loss=nan]Training:   6%|â–Œ         | 141/2500 [18:20<5:06:05,  7.79s/it, loss=nan]Training:   6%|â–Œ         | 141/2500 [18:20<5:06:05,  7.79s/it, loss=nan]mlflow logger closed.
Traceback (most recent call last):
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 108, in <module>
    main(args.config)
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 78, in main
    trainer.train()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/training/trainer.py", line 176, in train
    loss.backward()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Training:   6%|â–Œ         | 141/2500 [18:27<5:08:48,  7.85s/it, loss=nan]
Loading experiment configuration...
Loading tokenizer: distilbert-base-uncased
Processing data for all tasks...
Processing data for task: SwahiliNER
Map:   0%|          | 0/2000 [00:00<?, ? examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1000/2000 [00:02<00:02, 334.31 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:05<00:00, 372.29 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:05<00:00, 363.30 examples/s]
Map:   0%|          | 0/2000 [00:00<?, ? examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1000/2000 [00:00<00:00, 1506.49 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1401.99 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1407.08 examples/s]
Some weights of MultiTaskModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.embeddings.LayerNorm.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.embeddings.word_embeddings.weight', 'encoder.transformer.layer.0.attention.k_lin.bias', 'encoder.transformer.layer.0.attention.k_lin.weight', 'encoder.transformer.layer.0.attention.out_lin.bias', 'encoder.transformer.layer.0.attention.out_lin.weight', 'encoder.transformer.layer.0.attention.q_lin.bias', 'encoder.transformer.layer.0.attention.q_lin.weight', 'encoder.transformer.layer.0.attention.v_lin.bias', 'encoder.transformer.layer.0.attention.v_lin.weight', 'encoder.transformer.layer.0.ffn.lin1.bias', 'encoder.transformer.layer.0.ffn.lin1.weight', 'encoder.transformer.layer.0.ffn.lin2.bias', 'encoder.transformer.layer.0.ffn.lin2.weight', 'encoder.transformer.layer.0.output_layer_norm.bias', 'encoder.transformer.layer.0.output_layer_norm.weight', 'encoder.transformer.layer.0.sa_layer_norm.bias', 'encoder.transformer.layer.0.sa_layer_norm.weight', 'encoder.transformer.layer.1.attention.k_lin.bias', 'encoder.transformer.layer.1.attention.k_lin.weight', 'encoder.transformer.layer.1.attention.out_lin.bias', 'encoder.transformer.layer.1.attention.out_lin.weight', 'encoder.transformer.layer.1.attention.q_lin.bias', 'encoder.transformer.layer.1.attention.q_lin.weight', 'encoder.transformer.layer.1.attention.v_lin.bias', 'encoder.transformer.layer.1.attention.v_lin.weight', 'encoder.transformer.layer.1.ffn.lin1.bias', 'encoder.transformer.layer.1.ffn.lin1.weight', 'encoder.transformer.layer.1.ffn.lin2.bias', 'encoder.transformer.layer.1.ffn.lin2.weight', 'encoder.transformer.layer.1.output_layer_norm.bias', 'encoder.transformer.layer.1.output_layer_norm.weight', 'encoder.transformer.layer.1.sa_layer_norm.bias', 'encoder.transformer.layer.1.sa_layer_norm.weight', 'encoder.transformer.layer.2.attention.k_lin.bias', 'encoder.transformer.layer.2.attention.k_lin.weight', 'encoder.transformer.layer.2.attention.out_lin.bias', 'encoder.transformer.layer.2.attention.out_lin.weight', 'encoder.transformer.layer.2.attention.q_lin.bias', 'encoder.transformer.layer.2.attention.q_lin.weight', 'encoder.transformer.layer.2.attention.v_lin.bias', 'encoder.transformer.layer.2.attention.v_lin.weight', 'encoder.transformer.layer.2.ffn.lin1.bias', 'encoder.transformer.layer.2.ffn.lin1.weight', 'encoder.transformer.layer.2.ffn.lin2.bias', 'encoder.transformer.layer.2.ffn.lin2.weight', 'encoder.transformer.layer.2.output_layer_norm.bias', 'encoder.transformer.layer.2.output_layer_norm.weight', 'encoder.transformer.layer.2.sa_layer_norm.bias', 'encoder.transformer.layer.2.sa_layer_norm.weight', 'encoder.transformer.layer.3.attention.k_lin.bias', 'encoder.transformer.layer.3.attention.k_lin.weight', 'encoder.transformer.layer.3.attention.out_lin.bias', 'encoder.transformer.layer.3.attention.out_lin.weight', 'encoder.transformer.layer.3.attention.q_lin.bias', 'encoder.transformer.layer.3.attention.q_lin.weight', 'encoder.transformer.layer.3.attention.v_lin.bias', 'encoder.transformer.layer.3.attention.v_lin.weight', 'encoder.transformer.layer.3.ffn.lin1.bias', 'encoder.transformer.layer.3.ffn.lin1.weight', 'encoder.transformer.layer.3.ffn.lin2.bias', 'encoder.transformer.layer.3.ffn.lin2.weight', 'encoder.transformer.layer.3.output_layer_norm.bias', 'encoder.transformer.layer.3.output_layer_norm.weight', 'encoder.transformer.layer.3.sa_layer_norm.bias', 'encoder.transformer.layer.3.sa_layer_norm.weight', 'encoder.transformer.layer.4.attention.k_lin.bias', 'encoder.transformer.layer.4.attention.k_lin.weight', 'encoder.transformer.layer.4.attention.out_lin.bias', 'encoder.transformer.layer.4.attention.out_lin.weight', 'encoder.transformer.layer.4.attention.q_lin.bias', 'encoder.transformer.layer.4.attention.q_lin.weight', 'encoder.transformer.layer.4.attention.v_lin.bias', 'encoder.transformer.layer.4.attention.v_lin.weight', 'encoder.transformer.layer.4.ffn.lin1.bias', 'encoder.transformer.layer.4.ffn.lin1.weight', 'encoder.transformer.layer.4.ffn.lin2.bias', 'encoder.transformer.layer.4.ffn.lin2.weight', 'encoder.transformer.layer.4.output_layer_norm.bias', 'encoder.transformer.layer.4.output_layer_norm.weight', 'encoder.transformer.layer.4.sa_layer_norm.bias', 'encoder.transformer.layer.4.sa_layer_norm.weight', 'encoder.transformer.layer.5.attention.k_lin.bias', 'encoder.transformer.layer.5.attention.k_lin.weight', 'encoder.transformer.layer.5.attention.out_lin.bias', 'encoder.transformer.layer.5.attention.out_lin.weight', 'encoder.transformer.layer.5.attention.q_lin.bias', 'encoder.transformer.layer.5.attention.q_lin.weight', 'encoder.transformer.layer.5.attention.v_lin.bias', 'encoder.transformer.layer.5.attention.v_lin.weight', 'encoder.transformer.layer.5.ffn.lin1.bias', 'encoder.transformer.layer.5.ffn.lin1.weight', 'encoder.transformer.layer.5.ffn.lin2.bias', 'encoder.transformer.layer.5.ffn.lin2.weight', 'encoder.transformer.layer.5.output_layer_norm.bias', 'encoder.transformer.layer.5.output_layer_norm.weight', 'encoder.transformer.layer.5.sa_layer_norm.bias', 'encoder.transformer.layer.5.sa_layer_norm.weight', 'tasks.0.heads.ner_head.bias', 'tasks.0.heads.ner_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534
  return FileStore(store_uri, store_uri)
Instantiating tasks and model...
Instantiating trainer...
MLflow logger initialized. Experiment: 'JengaAI_NER'
Starting training...
Training:   0%|          | 0/1000 [00:00<?, ?it/s]/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 108, in <module>
    main(args.config)
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 78, in main
    trainer.train()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/training/trainer.py", line 168, in train
    outputs = self.model(
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/core/model.py", line 81, in forward
    task_output = task.get_forward_output(
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/tasks/ner.py", line 44, in get_forward_output
    loss = loss_fct(active_logits, active_labels)
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
 (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:113.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
mlflow logger closed.
Traceback (most recent call last):
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 108, in <module>
    main(args.config)
  File "/home/naynek/Desktop/Jenga-AI/examples/run_experiment.py", line 78, in main
    trainer.train()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/multitask_bert/training/trainer.py", line 176, in train
    loss.backward()
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/naynek/Desktop/Jenga-AI/venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output.
Training:   0%|          | 0/1000 [00:16<?, ?it/s]
